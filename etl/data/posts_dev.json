[
    {
        "id": "1aieu3j",
        "title": "Facts",
        "description": "",
        "score": 1352,
        "upvotes": 1352,
        "downvotes": 0,
        "tag": "Career",
        "num_comments": 40,
        "permalink": "/r/dataengineering/comments/1aieu3j/facts/",
        "created": 1707019173.0,
        "comments": "I saw an engineering job that was looking for 11 years of Databricks. For sure that organization would put that founder level knowledge of Databricks platform to use on Excel files.\nHow do you even get to these odd years of experience even for regular stuff? Is 11y worth of experience really a noteworthy difference to 10? Sure it's 10% more time but this sure doesn't equal to 10% better. Edit: typo\nThanks, I hate it. My previous job, as a consultant, I would get excel jobs. One particular BA would always send them my way because I \"knew data\".\nmore realistically, dashboards\nWhen they asked me if I knew how to use Looker Studio \nAhahahah this was good Better hurry and do 2 years of certificates\n\"Can I export it to Excel?\"\nThen, someone proceed to give you the data in excel table with joined cells and colorful...\nAnd don't forget about VLOOKUPs, HLOOKUPs, pivot + macroses....\nAnd the color is actually part of the data.\nidk what kind of DE jobs y'all are getting. When I was working as a DE, I think the only thing I ever used Excel for was keeping track of my hours.\nMy entire 200 employee company is built on top of a 14 tab excel spreadsheet with so many macros that it's functionally an application, built in a week as a 'temporary solution' by an unhinged senior architect to get data moving.\nEh I wouldnt blame him probably had a random VP yell at him on why cant this be done today and if not done, the VP threatened to do it himself.\nMy team extensively uses Excel for documentation, unfortunately\nAI appears at least 4 times in the job description\nLooks like a general dev meme since the technologies cover everything (web, devops, security, monitoring, cloud services). In reality I doubt you'll get that much Excel jobs lol, you usually really do need to use a good chunk of various technologies depending on which aspect you're working on\nI often have to use it as one of many upstream data sources from different business units...but in addition to a lot of the other pieces mentioned here for the actual pipeline bits. I'm surprised there isn't more focus on specifically how to organise data pipelines that originate with Excel as the source. There's a whole craft to it...the first thing to accept is that people will NOT use ANYTHING else if they're using it as a data entry point.\nI get the joke but this is untrue. The final product is almost always a spreadsheet in an email (or OneDrive). But there is an amount of under appreciated work that goes into making the tools that produced the spreadsheet.\nSometimes not even excel. I just make a table in email and send it in outlook of our daily etl process \nOmg, I laughed so hard.\nTbh, a friend of mine works for a major bank in Canada, gets $120k and his job is to run a few VBA scripts once a month and make sure they work the rest of the time. Easy money.\nI guess in my experience its more like straight up vanilla SQL\nExcel should be on the left as well TBH.\n I had an analyst job that required Azure, Python and working with big data turned out to be data entry and excel.\nThis but the company won't fork out for MS Office licenses, so you have to try and do it all in LibreOffice \nWould've upvoted thousand times if i could\nI've spent more time on confluence/jira/excel in the last couple weeks than I did with any other tools.\nSolve the task with Python. Export df to xlsx. Automate it. Tell them it keeps you busy.\nPlus 5 Rounds Interview!!\nIs this how it generally goes? Data analyst before data engineer? I have a computer science degree trying to get my first SE or DE gig\nLearn Power BI throw in that you know sequel, and youll be as popular as a hot chic at an all boys school.\nAbout half of the data engineers I know started as some type of SWE, and then the other half started as like data analysts, yeah.\nGuys, I would really appreciate it if someone could send me the Talend open Studio software folder on my Gmail. If anyone could help, please let me know !!\nYou own your teammates? What does that mean?\nHuh. I regularly use 11 of the things on the left and I'm not even classified as a \"data engineer.\"\nI wish.\nChatGPT would also fit to the right :D\nTrue\nIs this even true? I've never worked at a job that listed a bunch of things that we don't use\nNo, no, please no",
        "content_hash": "debe46f9a3887409ac66f561b56fa1ae"
    },
    {
        "id": "1arwf4u",
        "title": "Had an onsite interview with one of FAANG, all 6 interviewers were Indian ",
        "description": "7 if I count the person who did phone screen. Had a positive experience with majority of the interviewers but hiring manager and another interviewer appeared very uninterested and seems didn\u2019t even read my resume. Almost 0 coding and majority was behavioral questions despite the fact that this is mid level data eng position. \nWith this much skewed perceived diversity, I can\u2019t help thinking they\u2019re looking for another person from their own culture. \n\n\nEdit:\nSeems like many other also witness this trend: https://www.reddit.com/r/cscareerquestions/s/pnt5Zidl1X \n\n",
        "score": 995,
        "upvotes": 995,
        "downvotes": 0,
        "tag": "Interview",
        "num_comments": 68,
        "permalink": "/r/dataengineering/comments/1arwf4u/had_an_onsite_interview_with_one_of_faang_all_6/",
        "created": 1708047303.0,
        "comments": "Data Engineers, We appreciate your engagement and feedback. It's important to maintain a respectful and inclusive environment here. While sharing interview experiences can be valuable, let's ensure our discussions remain focused on data engineering topics and refrain from making assumptions based on cultural backgrounds. We encourage constructive feedback about interview processes related to data engineering. Let's continue to keep our conversations productive and respectful. We will lock this post from further comments.\nAre you saying all 7 had no coding or only the two you mentioned? First one is definitely super weird situation tho. I work at one of these companies and each interviewer is assigned what to test. Some get assigned technical things like SQL, Modelling and Cloud stuff and some get assigned some behavioural stuff to test like how the person approaches a problem, do they keep scale in mind etc.\nIgnore the people calling you racist. As someone whos interviewed with full Indian panels you never get the job offer theyre 9/10 times going to prefer another Indian candidate unless theres no other candidates. This has not been only my experience I know many people who have this same impression. People calling you racist are trying to gaslight you, Indians will literally admit to doing this and these people will still call you racist.\n10000%. Got one job with an Indian panel and thats because both a white male and the HR recruiter (African American woman) fiercely advocated for me. Otherwise \nTheres literally major lawsuits alleging that caste systems were enforced by Indian management at companies like Cisco and Apple. India had racism carved into the structure of its culture and religion for thousands of years. Its not getting eradicated by a plane ride to Cali and a PPT presentation by HR. https://www.reuters.com/business/sustainable-business/caste-california-tech-giants-confront-ancient-indian-hierarchy-2022-08-15/\nAt one company I worked for, a Westerner in management had to fly to India to fire someone because all the on-site supervisors were in a lower caste. Edit since it's locked: No Zoom call, I think the point was being respectful of their culture and being terminated in person is the respectful thing to do.\nCouldn't do a zoom call?\nAs someone thats been in data engineering for 25 years (a white guy) in tech and non tech as soon as I see an Indian in the hiring process I know Ill get the thumbs down. At first I thought it was me but even seeing Indian managers when I wasnt looking for a job they always hired Indians. Once they lay a beachhead they will funnel more Indians in until they take over whole departments. If you bring it up you get labeled a racist or they say there is no qualified applicants except their Indian friends. Glad Im getting out soon.\nWheres the greener grass?\nYMMV, but as an Indian, I've faced \"racism\" from Indians. White folks are at least polite, even if it's a reject; Indians choose interviews as the place to establish who's the boss.\nI was in a team of 10, I was the only non Indian, 9 others were Indians. I dont need to tell you how it went\nI sometime interview candidates for DE positions (mainly sql coding challenges) and last time we were left with 2 candidates, 1 indian 1 white dude. I would honestly say that both were good, 50/50 chance imo. But hiring manager was indian and chose the indian guy. Nothing wrong from my point of view but most likely a biased decision, half the team is already indians.\nYeah, Indians are doing a lot of empire building inside American companies. Its getting bad.\nIm an Indian and Im very sad to see all these comments about Indians. Sure, some of these statements may be true but I just want to say that not all Indians are bad. Given the population of India, youll encounter people with various mindsets. I would say the main problem with the Indian culture is Seniority triumphs everything. NeetCode talked about it in one of his YouTube videos and I feel its very accurate - https://youtu.be/zjz6RkVWSoA?si=hS0ASn3MaG1xXZG6\nIve worked and interviewed with my fair share of Indians and what you say is true. My closest friends at work were Indians but my experience with Indian interviewers really stand out in a negative way.\nThese days, the \"Silicon Valley\" mindset is the opposite of seniority as been seen as \"Boomer\". I was surprised at my 35s been called a \"Boomer\" as if I had 70 years ...\nIndians want to hire other Indians because they know they will work harder. If I had a nickel for every time my Indian friends called Americans lazy... well, I would be as rich as them \nI believe you because Im black and I have the opposite experience, I see an Indian and I like my chances. Ive actually never had a bad experience interviewing with Indians but I have twice with white people who wouldnt even look me in the eye or engage in conversation. One of them just straight up asserted that I dont know OOP because I took a functional approach with the take home (wasnt stated to use OOP). Most of my interview experiences even with white people have been fair and I feel like I was evaluated on my merits. People often dismiss the idea that theres ever discrimination in hiring but there definitely is and its really interesting to read these comments.\nSimilar experience with a young \"antiboomer\" manager. I applied for a job advertised as a procedural legacy PHP website, and that manager complained why I was not using OOP PHP code...\nOh wow, Indians actually treat you fairly\nPut the race issue aside. Anyone on an H1-B is likely to accept a job for less money, especially if the company offers legal support and is much less likely to leave, regardless of where they are from. Look up green card wait times.\nMy experience: I ( immigrant not Indian) was hiring 2 data scientists ( entry & mid level) literally 2 weeks ago. I selected a local white guy as my first choice for mid level role, and submitted my preference to leadership & HR, they came back to me asking to pick diverse candidate if applicable. I kinda denied it at first but ended up hiring an Indian girl(she was the only POC among short listed) so, its not always what manager wants often time leadership intervene manger to choice POC. Context: F100 company in finance division >98% employee are whites\nIm guessing its Apple?\nMy guess was actually Amazon or Facebook..\nMostly behavioral interviews makes me think its Amazon.\nIm an Indian living abroad and I can confirm this.\nYoure spot on. Its one of the main problems in tech tbh. Massive discrimination by economic migrants toward residents \nI am an Indian asking my peers and manager to add diversity ( I strongly believe in different thoughts processes). But, we get very few non brown applicants! Maybe my company is not sought after in other ethnic groups.\nyeah indians literally only hire other indians. i was in a company where 70% were indian and they brought their entire families over to the company\nThings like this discourage me every now and then, I am Indian and hate the shit these people pull, I bet we'll reach a point where genuine and deserving engineers aren't hired because of how our reputation has been going down the dump. It was the hacktoberfest, express.js events and now this.\nAs an Indian, everytime I have a white male as my interviewer- I never end up getting the role despite getting strong positive feedback. My best friend is a white male but the sad reality is people in general tend to support/help people who look like them or are from the same culture\nIt goes both ways sadly. Im latino and its even worse for me lol, i need to be extremely better than anyone else, which is basically impossible\nI regularly interview Indians and most of them dont get the role because theyre fucking frauds or just unskilled, not because theyre Indian. In fact, the only people (and I interview dozens of people per month) that I ever caught cheating on interview were Indian as well. I remember the utter shock of one candidate when I asked him to share his whole screen and close all the Zoom sessions, since the interview is on Teams. He even tried to push the bullshit that if he turns off Zoom, hell get disconnected - and for reference that was an interview for a senior software engineer role (and I am an architect with FAANG experience). That was just disrespectful. Dont blame the player, blame the game. If someones good and matches the team in terms of friendliness/cultural fit - I dont give the shit if theyre white, black, brown, yellow, pink, blue - they will get the job. Period.\nMy Indian friend hates interviewing Indians, he says most of them are frauds and fake their experience. We recently interviewed a guy for SRE team and he indicated 3 years of experience with Google Cloud. Homie couldn't answer most basic questions to save his life. AND he passed 3 interview rounds with Indian recruiter, Indian manager and Indian Solution Architect. What a fucking joke. As a counter point, I've personally witnessed white interviewers/managers be racist af. I never understood how race plays into job function but sadly it does.\nWell, this is definitely going to become a shit show \nOh lordy. Anecdote wars, go! I'm not Indian. Indians _love_ me. Hire me. Promote me. Invite me to their weddings, their birthdays. They share their food, their homes, their culture and their holidays. Just like, wait for it -- non Indians. One of my Indian counterparts grills the shit out of people and is frankly more suspect of other Indians because of more internal biases within their culture than being hell bent on hiring Indians wholesale. But still he won't hire *anyone*. He wouldn't hire his damn self frankly. That isn't something I now project on every other Indian tho since ya know, that's a nationality with a highly diverse culture in their own right so generalizing about having a whole panel like they'd be in lock step is already a bit out there. As another poster put it -- try being a woman who gets men every single interview and denied 9/10 times. Hence all men only want to hire men. Q.E.D. right? At the very least its more complicated than that no? No one _reads_ resumes, skim at best because (a) they're usually terrible indicators of anything but baseline skill sets/exposure/keywords -- no way you'll find others just as disinterested in yours that aren't Indian :eyeroll: (b) most of what I want to know has already been filtered for this person to get to me in a panel interview and I'm going to ask what I want to know rather than let you yammer on about some past project that I dig into only to find you're basically taking credit for a project you were around but can't actually articulate worthwhile skills from. Hiring sucks for everyone. No one wants to interview anyone. And lots of people have high (unreasonable) standards. It's that first the rest 2nd. Get used to it.\nGiven that India is now the most populous country, I agree. We can't generalized over a billion people like this. Some people are racists, across any race.\nMajority of the people who interviewed me were nice and pretty much average DE interview. But I live in diverse area and rarely saw 100% of the interview panel being only one ethnicity (in my area 15% are indian). Like never encountered all white/asian/latino interview panel before. The hiring manager and the oldest interviewer were the ones who gave strange vibe. All body language speaks they were not engaged or interested. Asked me a few questions neither related to my skillset nor job description. They tried to cut off the interview short and ended up not using provided codepad for SQL/Python coding at all. Did I fail to impress him with the couple of questions he asked me? Maybe. But that was the first hour of the 6 hours interview and wish they were more engaged\nIm Japanese and lazy typer. Sorry for broken English\nMy friend, you are going to get roasted for this.\nWait Im not saying this isnt happening. But why do you guys recognize the issue when its Indian interviewees showing a favorable bias toward Indian candidates and against whites- which doesnt work in your favor  but when URM and in particular Black folks voice these same concerns around all white institutions/panels/interviewers displaying favorable biases toward other white candidates and negative biases against URM you guys suddenly cant comprehend? Suddenly you can see the importance of DEI and other regulation to ensure these sorts of things dont happen.\nNothing but fucking entitlement\nI imagine many of us would recognize it to be a red flag if 6 out of 6 interviewers/people in the hiring pipeline were white men. I've never had a job where there was that much homogeneity among the staff.\nIve absolutely experienced all white panels. Dont change the goal post. We are discussing racially homogeneous interview panels not gender as well. Yes, Ive experienced this. More than once. Actually this is what I experience more often than not. I think white folks live in such a bubble and are so used to be at the center and forefront of every institution that you guys literally just dont notice or want to acknowledge prejudice of any kind and are now only voicing concerns bc youre not benefiting in this particular scenario. Bc the fact you honestly are shocked that most URM are routinely presented with all white interviewers is surprising and confusing to me. Why are you shocked by that? Should not be a major revelation given the lack of diversity in high-paying fields like tech, finance and consulting.\nLmao, youre really just prejudice\nNo intelligent response.. not surprised. Youre not ready to have an intelligent conversation with real dialogue so you have to resort to youre just prejudice. Im raising the same concerns OP is, except my concerns are legitimized by historical events, data and research but you dont like the scenario where youre the suspect and not the victim.\nSo you didnt even need coding questions and were thrown out in behavioral ? Now that Saved both your Time.\nIT guy here. I've had a lot of job interviews, non Hindu interviewers, that were too much behavioral based and almost ignore previous experience or skills, or use psychological tests to evaluate for programming skills like too many math tests. After I talked with other people of the same field, we realized, that method does not work well, and leave a lot of experienced people out ... Like when a job recruiter hires a too social, too popular, not too much technical person with a suit and tie, from an Ivy League school, instead of an introverted, rocker guy por gal, with strong experience in the area, from a public or lesser known private school. In most of the long term jobs I got, the job recruiters did not got too focused in those behavioral tests. Anyway, this is not about been racist, but been cheap or having favoritism. There's competent people in India, but if a company wants to outsource everything to a cheaper country, either India, China or elsewhere, you will just find a lot of cheap people looking to hire another cheap people, becasue someone on the top wants to maximize profits...\nWould you be posting this if all of them were White?\nThis is a fact. I had this happen to me. My former manager referred me for a job for his company. I made it through to all rounds: technical assessment, gmat assessment, oral presentation, and finally meeting with their one-ups. At the very end of it, the hiring manager ended up hiring one of his kind, another south East Asian. I guess my referral from my former manager was not even enough to seal the deal. Even in my current organization, I would say 8/10 of them are Indians.\nThis is crazy I always recommend the best for the job and that means indian Chinese white black. As long as they all did their job I would have to do less work. I know amazing workers of all types and I look for competence and hard work. What I find amazing these days is the person not dedicated to learning or working hard to finish the job. Nowadays people just don't want to be honest with themselves. Unfortunately that is again all types of people white black Indian Chinese.\nAnd this is what needs to be called out. Currently we accept the available not just the exceptional. In an age of AI and job loss, were going to have to have serious discussions about immigration policy, particularly regarding the average seeking economic betterment. Exceptional should always be welcome\nI am an indian female. I agree there are situations like yours which happen in small companies. But in FAANGS they usually have panels so there is more scrutiny in selection process\nWhat about when panel is all Indian lol\nAs I said more scrutiny never said it doesnt happen \nOof\nI live in India and so I only had few all white panels. I had a good experience most of the time. My worst experiences have been interviewing with Indians working for Indian companies.\nIts not Amazon\nits amazon\nMaybe Im stupid, but I took this as someone realizing how important diversity is. Usually this story is a black person/woman confronted with a panel of white males and when said people tell their story theyre shouted down by people saying its no big deal. This person got a taste of what its like job hunting when the people with power dont look like you. Hopefully they carry this experience to their next role and and use it in a positive way.\nI agree with your point, but Id like to also point out the fact that OP never said they were white, you assumed it.\nWhere did I assume they were white? I said that when people talk about similar experiences _with_ white people they get shouted down. If op is East Asian/mexican/russian and was confronted with a panel consisting of only south Asians the same lesson applies. IMO the moral of the story is diverse != not white and if youre running interviews you should try to mix up the panel instead of assuming we sent a bunch of non-white people, its fine. Edit: Im dumb and just realized youre responding to > got a taste of what its like job hunting when the people with power dont look like you Fair point, but i didnt really think about ops race, more about them getting the experience of being the odd one out. I _did_ assume this is the first time theyve been confronted with a panel of people that didnt look like them which does imply theyre white, but I didnt think it through to that extent.\nyou rascal u\nhttps://www.reuters.com/business/sustainable-business/caste-california-tech-giants-confront-ancient-indian-hierarchy-2022-08-15/\nwhy are you spamming this\nWhat is your ethnicity? Curious..\nnot indian probably.",
        "content_hash": "815ac0cee1dc7f3d36798cdbd662c12e"
    },
    {
        "id": "1bd5wv8",
        "title": "It\u2019s happening guys",
        "description": "",
        "score": 826,
        "upvotes": 826,
        "downvotes": 0,
        "tag": "Discussion",
        "num_comments": 200,
        "permalink": "/r/dataengineering/comments/1bd5wv8/its_happening_guys/",
        "created": 1710271207.0,
        "comments": "Devin the intern simulator\nAt least I can talk about how cool dune 2 was with interns..\nGlad to know I have some value \nI LOL'ed\nThe spice harvester battle sequences?? Omg. Also the opening shots of the harkonenns floating up the mountain. Amazin.\nSaddling shai-hulud was pure cinema\nImagine losing your job by an AI named fucking Devin\nFuck Devin and everything it stands for\nUsing an actual human's name is not a good idea, i.e Karen and Andy.\nwait... what don't I know about Andy?\nOr Chucky!\nDevin is not a human\nLet's see if Devin can figure out what the heck the client wants for the actual project I'm involved in. \nThat shouldnt be too hard. Client tells Devin something. Devin builds something. Client tells Devin its not exactly what it wants. Devin says those were not the specs you gave me, but we can iterate. Devin builds v2. Rinse and repeat. 22 versions later. Drag out the project for 4x longer than expected. Consulting company makes way more money than a compete human engineer wouldve created by doing it correctly. Tell the client, our ai swe saved you money. When i was doing grad school, that is exactly what my prof told me, build some bs Al consulting company. Clients eat they sh*t up thinking it will save them money.\n> build some bs Al consulting company. Clients eat they sh*t up thinking it will save them money. I mean, the client interacting directly with the engineer who can iterate it immediately and do it 20x as fast as a human engineer will 100% save money. Now there are definitely other issues that could come up, but being able to quickly iterate through minor changes would absolutely change the game\nMaybe my joke sucked. But in the business of consulting, unless a client is paying a massive premium for speed, it is not in the interest of the consulting company to do things right or fast. Regardless of the cost of the engineering\nThe trick is to make the client happy, make them think they're \"saving money\", but at the same time bill them through the roof and they are happy to pay. Actually sifting through the shit that's in their specs, and directly solving their business problems doesn't benefit the consulting company nearly as much as #1.\nDepends. Some projects are charged hourly, some are only charged at the completion of certain milestones. In the former, sure. In the latter no.\nI work in consulting, this is far from accurate. Doing things slowly or with poor quality does not win additional business. Consulting companies have every incentive to do things quickly and w/ high quality, that's the only way to maintain a credible reputation and get follow on business.\nMaybe my joke sucked. But in the business of consulting, unless a client is paying a massive premium for speed, it is not in the interest of the consulting company to do things perfect or fast. Regardless of the cost of the engineering.\nPlus AI starts to kind of suck after a few revisions of code (at least chatgpt 4). My last project I got lazy and kept having it revise functions and parts of code. Eventually it would just output either something that didnt work or the original code. And when projects get anywhere near complex it doesnt great if you dont know how to fix mistakes it makes.\n\"I apologise you are correct, here is the working code: **\" ***facedesk***\nHaha exactly. Also I dont know why it bothers me but I get annoyed with it casually apologizing like that all the time. Just give me updated code, stop apologizing.\nPrecisely the reason I stopped paying for GPT4 and went back to 3.5 for free. Between it and me, I get what I need faster than googling.\nThis. I'm not sure what Devin's using, but if it's anywhere near ChatGPT v4, it will cause me more headache than figuring it out myself.\nIve had some luck but I swear a good chunk of the time, especially with a language I know, it is easier to just to it myself with either docs or stack overflow. One thing I like about Reddit or SO is you can see different implantations and discussions why people did what they did. Which for as good as AI is, itll never have that feature. More like just here is how to do x. Which one day may be the best option. Or I guess you can spend time discussion why it chose the method it did. That works as well I guess\nBut the algos will get better. We are in the early days.\nThe best part of this is that some folks will see V2, V3, V4...V22 as refinement, instead of the reality which is, IME, that each iteration solves one thing they were focused on but creates new issues and also very likely \"unsolves\" things you solved in previous iterations.\nWho will be asking the questions and what questions will they ask? That's the issue, not the tech. People are always the issue.\nMy AI, will be in contact with your AI.\nHow does this extrapolate out?\nMeanwhile I was imagine that Devin will keep adding code and not removing the old code\nDude. You used AI. Billing rate = old rate x 3. Client is happy because they use AI.\nI do contract and consulting work. While it sounds nice to stretch out the project. It doesnt always work that way. The client isnt going to pay for more hours. So its important to lay down really clear expectations and requirements beforehand so you can point back to them when somebody asks for whats essentially a 20 hour change request. But as you know, thats easier said than done. I wish I had an open ended contract to just screw around for a year or two though and still get paid lol. Although Id be too ashamed to actually do it. Honestly Im bad about under-billing if anything. Should probably be more ruthless. Everybody else is.\nWhat you're looking for is the \"enterprise\" level contracts that sometimes do have an open-ended length. However, there are limits to everything and they will eventually come down on you if progression isn't happening.\nYeah, I've worked on teams that had that type of contractor. They tend to be targets for budget cuts - unless you're a golden god level resource. So far I've been making my niche contracting with a handful of smaller local firms who already have clients and connections. Down-side is it's a lot to coordinate, and it's almost like you have 5 bosses. Up-side, I'm ultimately independent and can turn down work as needed if I'm too busy. And, I'm getting to touch lots of different things and it's helping me skill up.\nCan confirm the eating part.\nFor now, what happens in a few years after the tools have been trained better\nIt is the users that need to be trained better not the tools\nAs long as there are external data vendors, like Nielsen, Im not concerned. Garbage data, unannounced changes to what they deliver, incompetence. Actually, I hope Devin takes my job. \nYeah! Good luck Devin talking to the business!\nIn all honesty it'll be 24/7 available, obedient, and have endless patience for stupidity so if we're fucked in one way, it's stakeholder and business communications.\nI have seen the future and it's a red faced CEO yelling at a computer at 3AM about not having the accounting debentures processed correctly for the last 3 years.\nThat's a positive for AI in my book. (making CEOs mad.) Though we will probably end up in the cycle where we \"save money migrating to AI\" and then \"bring back human programmers to fix the AI code\". Rinse and repeat. Just like outsourcing.\nAnd calendar management!\n\"I'm a people person!!!\"\nWhat if the client also uses an AI?\nThat's cheating, the client's intelligence was always artificial\nThat's not really the issue. What used to take a team of four or more data professionals could potentially be done by one person that knows how to use this tool well.\nRequirements?! What requirements?!?\n\nSoon another AI will be the client, so no worries for devin.\nCan Devin figure out where the Google doc is? With the env file that no one knows about?\nNope\nFor those you need project managers, business analysts not programmers \nNot when the client's needs include more than the barest minimum of technical specificity, and even less so when the clients don't have all of the necessary details consolidated into just one or two peoples' brains. Most BAs and PMs are lost and just taking notes to pass along to tech staff at that point, and the problem is that an average BA/PM is going to lose details and just won't know what salient questions to ask. Have you never been in a situation when you needed someone technical to speak to the client team to help them clarify requirements?\nIt will only progress further and yes there will be exceptions but its here and its not going away.\nnah, you're talking about science fiction. Current AI is still pretty bad, have you used GPT4 for any type of coding? It's dog shite\ni cant remember the last time a business analyst got a tech spec right   \nAwesome, when shit hits the fan, we can just blame Devin.\nLol Devin is on-call\nDevin said he'd do it yaay\nYes but will Devin cry when the PM yells at him on the level 1 response call?\nNext level Turing test\nI love how they \"interviewed\" at AI companies. How about interviews at companies that don't have a stake in the ai game.\nI wanna see Devin clear a recruiter call, ATS, and three screening calls, and then wait a month for a response. Devin got preferential treatment.\nDevin isnt a protected class, so companies want to hire and abuse it.\nWas 100% a joke, but yes.\nWas 100% a joke, but yes.\nWas 100% a joke, but yes.\nWas 100% a joke, but yes.\nWas 100% a joke, but yes.\nLet's see Devin taking a Jira ticket and solving it, in an existing code base. Passing interviews has almost no indication of real work...\nyeah what a joke, memorizing of LeetCode solutions is not really a sign of AI\nThe jira/kubectl/gitlab/grafana/etc plugins to LLMs will come, and those will be the killers that enable it to fully change and maintain products.\nIts been about a year and a half since GPT-3.5-turbo dropped and Ive heard just wait until X just about every month during that period. Its a model architecture problem, not a tool problem. LLMs are probably going to shape up to almost fully automate online customer service type positions, create efficiencies for other positions including SWE, and thats about it. Stuffing more parameters into models has proven to hit diminishing returns. For the next while you will hear look, we doubled our context window several times, but thats about it. There is a significant and unforeseeable way to go in model capability for it to actually assume a role as an autonomous agent.\nAt least with electrical engineering, you will have a solid plan B (I assume youre here because you want to go into data engineering as your career). To be quite honest, and I know it is off-topic, everyone is panicking about losing their jobs when really the only threat we should worry about right now is that the oceans are warming at a rate that no model was even able to predict.\nMy company has plenty of variation between how projects are implemented to the point that for example some use a patchwork of 'legacy' pipelines whereas the newer projects use a really convoluted multi-pipeline that has a billion hidden gotchas. Until companies stop having \"good ideas\" like our super-unfriendly pipeline setup, I don't believe AI is close to even being able to get the code to deploy in our custom setup because if it trains on all of the repos and doesn't understand the context of which ones use which tooling and why, it's already fucked from step 1.\nLol. Are you me? I'm so tired of \"good ideas\"\nIt literally does this in the demo. You are in denial.\nAre you sure it takes a jira ticket?\nthat was literally what the demo was\nThere's a demo of Devin doing just that (fixing a bug on a pre-existing project/codebase): [https://twitter.com/cognition\\_labs/status/1767548765924114881](https://twitter.com/cognition_labs/status/1767548765924114881) I've been generally skeptical of the ocean of LLM products but this one seems pretty impressive.\nExcept the demo isnt of Devin doing that. We dont see Devin do anything. Were just taking the presenters word that Devin did it. Which we probably shouldnt since as u/jalopagosisland points out. This is clearly a scam.\nA little digging into their website suggests they cant even build a website properly, but Im sure they built a state of the art AI SWE that is going to put us all out of the job. Smells like vaporware to me\nIts going to happen.\nSure, one day, but not today or near future.\nIt'll happen, until the senior rejects the PR because it is \"too long\" and complex.\nThen just go watch their vids. I mean theres been another company, sweep, doing exactly that with GitHub PRs for months now too.\nHow many engineer hours are required to fix the 86% of tasks that Devon screws up?\nIdk but I think billable rates are about to creep up for the rest of us ;)\nThat poor cto having to sift through thousands of lines of auto generated code each day...\nPutting two ai's in a row does not result in an increase of success. It will just create a more efficient idiot.\nWithout proper orchestration yes, I agree. But one should not throw away entire coordinated multi-agent learning/planning work out of the window. Even some of the single agent learning approaches have actor-critic type of architectures to simulate two AIs interacting (actor network does, critic network gives feedback) to achieve better result. Most of these are research level these days, but once monetization path is open, we will see some mind blowing products, and likely our grandkids will see more compounded impact.\nSorry Marcus is the AI that fixes those and he isn't out yet\nBold claims ! Well, 13% of a bold claim.\nThe solutions to those original issues are probably a part of the training data now too.\nHe solves 13.86% of issues. Enjoy fixing the code Devin produces in the other 86% . Business people will buy it anyway and fire some engineers, just to realize nothing is working anymore within two weeks later.\nLet the MBAs eat the shit they make\nwE aRe a DaTa dRiVeN cOmPaNy, lEaDeRs iN iNnOvAtIoN, pOwErEd bY cUtTiNg eDgE aI God. I swear. MBAs must have dedicated courses about how to say innovation, area of opportunity and mention where they did their MBA. So many innovators out there and yet Devin barely can work on its own about 1 out of every 10 days you leave it with work to do. SMH.\nLooking forward to seeing Devin at the backlog refinement!\nWho let Devin into production env? Were down again! Fgs\nGDI Devin\nYeah okay. Just read through this gold comment about this SWE AI Company on the CS career questions sub. [Devin is a scam](https://www.reddit.com/r/cscareerquestions/s/kaB6kUImmR)\nPost the entire thing so that everyone can see... \"This feels like a scam like wtf? Look at their website....can't they use Devin to make a better one??? lol [https://www.cognition-labs.com/](https://www.cognition-labs.com/) Also if you go to the \"preview\" url it looks NOTHING like the video [https://preview.devin.ai/](https://preview.devin.ai/) YOU CAN UPLOAD FILES WITHOUT LOGGING IN AND THEY DONT HAVE A LIMIT You can test it yourself, just press the paperclip and you can upload anything, they don't have a filesize limit OR even a filetype limit. I just uploaded a 5gb file to their server and yes i checked, the POST req does get sent Heres a copy of the settlers 3 soundtrack if anywant want's it from their servers: (idk closest thing i had on my pc at the time lol) \"https://usacognition--serve-s3-files.modal.run/attachments/84c86aee-3a5f-4b85-8b62-22ebe23ed262/settlers\\_3\\_uc\\_soundtrack.zip\" Here is the entire collection of the NES library of all north american ROMS uploaded to their servers: [\"https://usacognition--serve-s3-files.modal.run/attachments/8c9b6ae2-3950-47c9-af3f-3231580afabb/GET REKT.7z\"](https://usacognition--serve-s3-files.modal.run/attachments/8c9b6ae2-3950-47c9-af3f-3231580afabb/GET) Nintendo.....do your thing I also took the liberty of uploading some intresting images F off \"Cognitive EDIT: Are they running [https://preview.devin.ai/](https://preview.devin.ai/) in dev mode? Not a react dev myself but i can see all their react components in the chrome debugger... EDIT Why are they using [https://clerk.com/user-authentication](https://clerk.com/user-authentication) to handle logins? If Devin is as amazing as they say im pretty sure building a simple login functionality should be trivial for it.... Hell it should even salt and hash the passwords right? EDIT Ok maybe im reaching for straws here but if you inspect the DOM in the react debugger they have a prop called \"afterSignInUrl\", take one guess what the value of that prop is? \"\" EDIT Ok i need to stop but it's just fascinating They actually dont do ANYTHING themselfs Analytics: Hotjar Website: NextJS Login: Clerk Jobs: Ashby Waitlist: Google docs (ROFL) Learn more about their funding: A link to twitter Their so called \"Blog\" isnt even an actual blog, it's literally a static page with hardcoded dates and entries.... Who are these people? EDIT Aaaaaand i went to Linkedin and checked... Yeaaaa i'm getting heavy vibes of: \"We were laid off and now we try to scam some investors for money while we think of a better plan\" \" ~ minegen88 The fear mongering thats happening all around reddit on all technical subs shows that there are really not many good engineers around. Further shows how no one decided to dig into the issue even after being fooled by google a couple of months ago. AI atleast in its present state isn't replacing any good or even decent engineer. Yes , the guys who write repetitive code on their jobs are at risk. No need to sh*t your pants chaps.\nEvery day this shit feeling more like crypto\nUnlike crypto, there are legitimate use cases for AI. Unfortunately the crypto scammers do seem to be making the jump pretty effectively sh who knows if we'll ever see them become a reality.\nvery interesting research, thx! had couple of laughs too\nThis is the most absurd and cringey display of copium Ive ever seen, god damn. Not a single thing in this rant gives any actual indication that Devin is a scam. Literally not a single relevant point was made. If were going to lose our jobs because of AI lets at least try to keep our sanity.\n>If were going to lose our jobs because of AI lets at least try to keep our sanity. The only person whose going to lose their jobs is probably you , as a person who has worked in IT , there's a lot more to developement & programming than whatever the hell this so called AI is doing (or rather this LLM is doing)\nIts not the fact that you evidently cant even spell development that gives you away, or your clearly-tenuous handle on basic language syntax, but the fact that you call it working in IT. Next time you decide to go LARPing around you should say you were a software engineer, otherwise youre going to come off like the hello fellow kids meme again.\nIf I solve 13% issues correctly, I will be fired next day. Glad I have few more years of my job, and sad this AI improvement is not here to save a dev hell any time soon.\nCan Devin write an AI system administrator that just reboots the server and fixes 13% of the problems?\nThe name \"Devin\" after a few years will have a very bad reputation in the software world if this picks up.\nLet's wait: Me: Devin, clone this repo and lets start working Devin: I am sorry, I cannot do that because the default branch name of this particular repo is deemed to be a reference to slavery and can be seen as offensive by some people.\n\nDevin correctly resolves 13% of issues autonomously? I bet at least 26% of issues are trivial.\nWhat I want to know is: how are they using the number comparatively? Do they have a historical database of known GitHub issues to test against? Are they just picking random issues and somehow coming up with a percentage?\nDevon is actually a JR\nGood luck getting the security team of any competent company to let this thing in. Sleep well coders, jobs aint going anywhere.\nLooking at stats, Devin should be fired\nHmmm...I think I will start a consulting company specializing in cleaning up companies that went the Devin route.\nFire Devin! Our AI (Actual Intelligence) will solve 100% of the problems given them.\nThats a really great idea\nCan these people just fuck off already with this dystopian bullshit.\nIncorrect to speak about this \"AI\" model as a sole contributor when it's work is \"13.86% unassisted\". That's terrible stats, it only does what you want one time out of ten? lmao\nWill Devin answer the phone at 2AM when the offshore team calls?\nHello Dave. You're looking well today.\nIt is too early to comment. The data on which these models were trained and not optimized for scalabiity , custom architecture and other stuff. We already have an AI chatbot in Databricks which no one uses at my company. There will be some usecases where this will work. I see this tool to be a productivity enhancement for a SWE but not a replacement.\nThey should have called it Dennis the menace. Please run it in production and you will see beautiful fireworks that you will miss while asleep.\nI was born in the wrong generation\nI think there's a significant aspect being overlooked in this thread and people are being overly critical. While it's true that Devin may not fully grasp client needs or seamlessly navigate complex problems and legacy systems, we should not underestimate the potential of such technology. Devin is a step towards enhancing productivity, not diminishing the value of human engineers. Its about quickly constructing initial solutions that professionals can refine and evolve. This is about support, not replacement, enabling faster iterations and freeing up time for more intricate tasks that require human insight/creativity. Edit: I want to be clear that I am not suggesting that Devin is THE tool. I am saying that in few years we will see a more refined tool like Devin that will prove itself to be a massive productivity tool on actually building software that goes beyond solving trivial leetcode problems\nI can't wait for Devin to talk to Linda who has no idea what a zip file is\nBut can Devin do the needful?\nThis is neat, but solving 14% is *really* bad. I know it's going to get better with time (probably), but still.\nScam bots assemble!\nThis is cool, but you know what would be even better is an AI that could go to meetings instead of me wasting half my day\nI mean eventually this will take over. Everyone is laughing and making jokes, but Im terrified.\nCool. Now we can finally address that backlog of seven years.\nAm I supposed to like it because it has a cute name?\nSo Devin can successfully pass interviews but fix only 13% of issues correctly? Wow.\nSo, it has a success rate of 13.86%? That's it?\n13.86%. Must be upwork\n14% correct? I think I'll be ok for a little while longer\nCant wait it to send it to my coworker , Devin\nThis us precious for those retarded hackerrank and leetcode test that the lazy tech manager usually sends.\nLmfao. 13%.\nI feel like this thing should have been named Tibor\nI just saw other threads where a bunch of dudes literally uploaded large files to their server as it didnt need any login or file limit . I hope Devin can help optimise their S3 costs \nWhat's the date of the test? When will this be at an acceptable level to assist in our daily activities? What additional skills will need to be learned to take full advantage of this tool? We're here to continuously up skill. Tools like this are a reminder that we're in the unique position of actively working on projects to make our current skill set obsolete.\nIf I required an assistant to resolve 85% of the problems tasked to me, I would be fired in a day. Devin should be put on a PIP.\nAi going to get possessed by something when it passes the test maybe? Dur\nPotential scam... https://www.reddit.com/r/cscareerquestions/s/L8jeOEz5Zw\n[https://www.reddit.com/r/cscareerquestions/comments/1bd12gc/comment/kujyidr/?utm\\_source=share&utm\\_medium=web3x&utm\\_name=web3xcss&utm\\_term=1&utm\\_content=share\\_button](https://www.reddit.com/r/cscareerquestions/comments/1bd12gc/comment/kujyidr/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button) u might want to read this\nGood luck with that.\nwho made it? lets kill them.\n13.86%\nwhy do I feel like software related jobs would be the first one's to be replaced by AI.\nYea I hope it works. It is so hard to find folks interested in data engineering or science. Hoping I can use Devin as a O&M dude\nDevin can go suck a fat dick\nI wonder if it's clear to the agent/Devin, which 13% was done correctly. If so and it can triage (pass on the 80% of tasks it can't do well, but does attempt/solve the other %), then that's great... if its attempting everything haphazardly and someone else has to determine what % is done well, then...\nDevin sounds like the kid on the playground that picked on everyone in 4th and 5th grade because it lacked the intelligence of the other children in its cluster not buying it\nBut can Devin get me the credentials that I need from the DBA that has ignored me for weeks??\nAdding to my resume now: \"Proficient at fixing bad Devin code...\"\nIt must suck so much to have a run of the mill name like Devin or Alexa and then have to share it with some lame but ubiquitous product\nAn AI that can solve 13.86% of issues!! Wow! Where do I throw my millions of dollars?\n[Do you want to develop an app?](https://giphy.com/gifs/rickandmorty-season-4-episode-2-rick-and-morty-L3bj6t3opdeNddYCyl)\nWheres Pete, the AI QA tester?\nIve been seeing an uptick of shitty pull requests in the wild and they make navigating open source issues a nightmare. I get the hype cognition is trying to generate but holy shit would I not be bragging about this\nDoes Devin have access to prod?\nHope not\nWhy is no one sending these viruses to kill them? We are at ground zero. Fucking do something smarties\nBS. Don't buy the hype\nIts sad that Devin got hit by a bus\nDevin steals leftovers from the fridge\nState-of-the-art - we all know what the state of the art is right now.\nGood luck to the managers using that if the engineers are gone.\nThis is so hilariously dumb. This is like when MS introduced Clippy\nHow hard is it to pass an interview if you're allowed to use Google? I don't think this is the flex they think it is\nDevin, get me a coffee {crickets} pfft. Cant even do that.\nWell someone has to program Devin.\n13.8%? I need 100% accuracy .. 100% if the time.\nRelax Guys, Its just a RAG built on top of LLM with interactive GUI. Its not over until AGI will come and fuck us all.\ncorrectly resolves 13.86% of the issues unassisted Okay. But how did it do in meetings? Is it a good mentor? Or does it sit around and wait until someone asks a question or assigns work to it? I doubt Devin will ever get promoted beyond level 1 if it fails to show growth or increased business knowledge. (Above is mostly sarcasm)\nImagine being the software devs developing the AI that will phase out your job.\nAre these people actively trying to automate their own jobs away??\nSeeing this Devin stuff in quite a few subs, seems like they are trying to drum up hype about nothing new or exciting at all.\nDiscover the game-changing SWE AGENT, an advanced open-source software engineering agent that outperforms all others. This article covers its features, benchmarks, design, limitations, and more. This \"Open Source DEVIN\" has remarkable accuracy, speed, and open-source nature making it a tool to watch out for! [https://ai-techreport.com/swe-agent-new-open-source-devin-outperforms-all-others](https://ai-techreport.com/swe-agent-new-open-source-devin-outperforms-all-others)\nSo apparently they faked/overstated the things Devin could do in their demo video ?\nlol when are software people going to get their heads out of the sand. Were not special people, reading a jira ticket and fixing some bug in a system capable of internalizing the codebase is not a stretch\nTbh this gives me shivers. Thinking of job security for myself as a data engineer, what skills should I focus on ? Preferably to not get replaced by a smart AI bot.\nDevon just won me $1. Had a bet with a PM that AI will replace devs.\nLaugh now, but its totally gonna happen eventually. It wont just be human software engineers using AI assistance. Companies arent going to keep 100% human software engineer teams. The AI will totally replace us. Most software engineers I know already feed ChatGPT prompts and get code out of it. Pretty soon clients will be able to do that, and there will be all sorts of plugins and libraries the AI will be able to use.\nRemember no-code tools? \"Use our drag-and-drop UI and get a working app ready to be published\". I wonder what happened to those ones, since mobile devs are still in-demand.\nThere's a gargantuan gap between fixing issues in an existing code base with clear prompts and architecting + building said code base from scratch\nSeems like real scam by most of new grads telling it by no means in discouragements but looking little tells they already broke up ai ish startups money may drown out also presentations are like highschool grade, just is. Dont suggest to get in hypetrain whereas it is hard to do for you people i know that you need.",
        "content_hash": "5b2855736dd34595413f0212409180b3"
    },
    {
        "id": "1brqa92",
        "title": "Is this chart accurate?",
        "description": "",
        "score": 770,
        "upvotes": 770,
        "downvotes": 0,
        "tag": "Discussion",
        "num_comments": 67,
        "permalink": "/r/dataengineering/comments/1brqa92/is_this_chart_accurate/",
        "created": 1711827453.0,
        "comments": "I don't understand your question. Is this an accurate list of Python packages? Is the claim that things are quicker and easier if you use Python? Is life short? If it's one of those: 1) Yes, though incomplete. 2) It depends. 3) Yes.\nYeah, sorry I didn't elaborate, but thank you, I got the answer from you. My main question was, is this list correct and complete. >1) Yes, though incomplete. Understood\nTo elaborate my answers a little further then -- I think, for the domains listed in the charts, you can accomplish 95% of the tasks you need to do with the packages listed. You will always need to reach for additional packages to supplement specific needs for your use cases. On the other side, there is redundancy, for example Polars and Pandas are both Dataframe libraries targeting very similar usecases, so it's not like you need proficiency in every package under a domain to be able to get work done. Edit: Learning how to read docs and pick up a new tool is more important than knowing any specific tool.\n>Polars and Pandas are both Dataframe libraries targeting very similar usecases, so it's not like you need proficiency in every package under a domain to be able to get work done. Spot on! Thank you so much for these details.\nI think the worst thing about the last is that it doesn't tell you which packages are complementary and which are substitutes. For example pandas uses numpy so they're complementary but polars is a newer wholesale substitute for pandas.\nIs your thought that you don't want to learn another language? I tried learning JS and indeed life is too short for that. I'm open to learning but it's got to have a purpose and it's got to some how be valuable.\nMy #2 says \"It depends.\" There are cases where you are doing bog standard data wrangling and stats. Python is usually the path of least resistance. But then you want to do a custom algorithm, and you should probably reach for Julia. Or you need maximum performance for a very specific, predictable use case, probably reach for Polars in Rust. Or you need to do it client side, JS. Etc. Etc. It depends  Edit: I thought you were responding to me -- my bad!\nHold on hold on are you saying there are data stacks out there, in production, that run Python without some kind of containerization, or some kind of virtual machine running with at least headless Ubuntu, alongside some kind of Linux based automation scheme to run and QC the Python pipeline??? Or an AWS/Azure process to take the need for a Linux box off your hands??\nThere are companies orchestrating their entire operation with elaborate excel spreadsheets. There are companies that have devops teams to abstract all the infrastructure away so developers just write Python. And everything in between. There are certainly developers who work in only Python day to day!\nOnly a Sith deals in absolutes\nI wrote my entire ETL framework in JavaScript. I assume I'm Saw Guerrera then...\nI wrote my entire ETL repo in pure Python. Fuck pandas and dataframes.\nSith's are awesome man!\nPySpark has very little to do with database operations. It's an API for Spark, which is an engine for distributed scale-out in-memory computation (summary to the best of my abilities). Whatever Hadoop has to with Python is a bit of a mystery to me. Same goes for kafka. Koalas is just the Pandas API over Spark. So, either the name is incorrect of the \"database operations\" group (do you perhaps mean at-scale computation or something?), or the contents are vastly misunderstood. So... be careful with overlap with the 'desktop data manipulation' group top left.\ndatabase operations category is the most egregious for sure.\nAgreed on each of your points. Koalas goes with Polars/Pandas, Spark, Kafka, Hadoop aren't really database operations. Meanwhile PyODBC and SQLAlchemy are missing there. I saw the creator works at Meta so I started wondering if I was crazy lol EDIT: Wrong alexwang, the person who actually made the infographic hasn't used many of the modules there in any depth (LinkedIn influencer who's tagline is learning by sharing).\nI am novice but shouldnt sqlalchemy or (shudder) pyodbc be on there?\nYeah that'd make sense. Or psycopg2 or any Python-based SQL client/ORM.\nLife is short, that's why I like to choose between 17 different options when I want to perform a GROUP BY in Pandas\nHahahahaa\nNo, you also need set based languages like SQL.\nBased on the set of dependencies they have chosen I would assume pandas is their SQL driver of choice.\nGood point, as long as there's a gateway drug into the wonderful world of SQL.. pandasql will do !\nPandas is great for SQL, until you try to write a huge file. It will take the entire output into a dataframe, so it'll eat up ram. I had to switch some code to SQLAlchemy so I could stream the output to file.\nWhat other set based languages are even used than SQL?\nThe prequel..\nThank you for the info!\nSQL compliments python really well though- I use both (i.e. in snowflake) or in different cells of a notebook.\nThat's nice, in fact I have just started to learn SQL and have some Python some experience.\nYoull find it easy after a few weeks of practice. SQL is pretty straight forward. If you want to practice both in concert, I recommend a free account on hex.tech (this is not an ad, Im unaffiliated with the company other than using them at work)\nTo add on Omni's suggestion, Mode dot com also has a free tier with SQL, Python, and R.\nOr you can use Django like a sociopath.\n\nNo, there are quite some questionable placements & missing major ones. Also, never met a person with enough domain knowledge to use such a wide scope (other then in the most superficial manner), especially not those who stick to only Python. SA, ML, NLP & TSA ... Its more like \"i know there exists fancy stuff\".\nWhat are the missing major ones you can think of off the top of your head/\nRe, networkX, xarray, sqlalchemy, leafmap, geopandas, graphviz\nDon't forget OpenpyXL. All output has to be in Excel according to my users.\nYeah, went there once. Though wouldn't go there for a second time.\nI wouldn't call any libraries in the database operations category database operations libraries.\nI could say almost the same, but: Life is too short. I use Scala and SQL for last 20 years.\nWhat do you think of pyspark?\nPySpark is just a facade for a Spark. Spark is written in Scala. Nothing else. If it works for you  just okay. However, my focus is language expressiveness and safety while writing my code. That's why Scala.\nI respect that.\nPymc3 is now just called pymc (theyre on v5.X), and you wouldnt learn both that and pystan unless youre all in on Bayesian inference. (And probably dont use either unless you are doing Bayesian inference)\nFairly accurate to start with. To be honest, there are many in this list I have not even heard of, let alone using them, let alone being proficient. But absence of huggingface is a bit glaring, especially in the NLP category. I am sure many others will raise the absence of their favourite libraries too. For example, I love celery for asynchronous task processing, airflow for pipeline orchestration, fastapi for web backend, sql alchemy ORM for database operations etc. Regardless, you cannot know everything before jumping in. So, just get started. Along the way, you will discover your own toolchain and other libraries too, and add them to your repertoire.\nPlaywright > Selenium and Puppeteer for webscraping.\noctoparse is not a scraping library as far as i know. its a no code solution for web scraping\nLove it lol!! Yep I use most of these packages\nhaha, Linkedin influencer. I follow her too\nThanks for this, there are a lot here I haven't heard of and will check out. Not seeing anything miscategorized but I would add duckdb under data manipulation, playwright under web scrapers, and add a section for web servers.\nI need to get to work on those NLP packages for my job. Thanks for this graphic\nGood luck!\nSure, lots of packages for Python. Sort of the multi function printer of languages at this point. With all that that implies......\nYes , incomplete even\nI still miss a package to replace Simulink and save some money\nSaved and thank you\nI don't see how this is useful at all, plus spark and Kafka in same category but Polars in separate? Wtf\nI dont know what Genism is but Ive used a library called Gensim in the past for LDA topic modelling\nLooks comprehensive!\nGenism = Gensim?\nHeard most of it. Haven't used any of it. \nThe packages I recognize are categorized correctly. This is of course not an exhaustive list.\nYes. But it's aged now. Long story short: Python is king.\nwhere is huggingface for NLP ?\n*R users step into the conversation Uhhhhh.....\nI dont think database operations is right name for what pyspark, dask and ray do.\nThe short answer is no. Data engineering exist before any of these packages or languages. And it will exist after Knowing one language or set of tools is never enough because the field and everything changes constantly. So you need to learn and update your skills as well.",
        "content_hash": "ce75c0519289cb070d3349a3bf30843a"
    },
    {
        "id": "1h6iwx2",
        "title": "How Stripe Processed $1 Trillion in Payments with Zero Downtime",
        "description": "FULL DISCLAIMER: This is an article I wrote that I wanted to share with others. I know it's not as detailed as it could be but I wanted to keep it short. Under 5 mins. Would be great to get your thoughts.  \n\\---\n\n**Stripe**\u00a0is a platform that\u00a0**allows businesses to accept payments online**\u00a0and in person.\n\nYes, there are lots of other payment platforms like PayPal and Square. But what makes Stripe so popular is its developer-friendly approach.\n\nIt can be set up with just a few lines of code, has excellent documentation and support for lots of programming languages.\n\nStripe is now\u00a0**used on 2.84 million sites**\u00a0and processed over\u00a0**$1 trillion in total payments in 2023**. Wow.\n\nBut what makes this more impressive is they were\u00a0**able to process all these payments with virtually no downtime**.\n\nHere's how they did it.\n\n# The Resilient Database\n\nWhen Stripe was starting out, they chose\u00a0**MongoDB**\u00a0because they found it\u00a0**easier to use than a relational database**.\n\nBut as Stripe began to process large amounts of payments. They\u00a0**needed a solution that could scale with zero downtime during migrations**.\n\nMongoDB already has a solution for data at scale which involves sharding. But this wasn't enough for Stripe's needs.\n\n\\---\n\n# Sidenote: MongoDB Sharding\n\n[*Sharding*](https://newsletter.betterstack.com/i/147520006/sidenote-sharding)\u00a0*is the process of*\u00a0***splitting a large database into smaller ones***\\*. This means all the demand is spread across smaller databases.\\*\n\n*Let's explain how MongoDB does sharding. Imagine we have a database or collection for users.*\n\n*Each document has fields like userID, name, email, and transactions.*\n\n*Before sharding takes place, a*\u00a0***developer must choose a shard key***\\*. This is a field that MongoDB uses to figure out how the data will be split up. In this case,\\*\u00a0***userID is a good shard key***\\*.\\*\n\n*If userID is sequential, we could say*\u00a0***users 1-100 will be divided into a chunk***\\*. Then, 101-200 will be divided into another chunk, and so on. The max chunk size is 128MB.\\*\n\n*From there,*\u00a0***chunks are distributed into shards***\\*, a small piece of a larger collection.\\*\n\nhttps://preview.redd.it/1ikmvqtjju4e1.png?width=1322&format=png&auto=webp&s=b7b52793b3ec51ee17f1d5f84a31ed8917c56cab\n\n*MongoDB creates a*\u00a0***replication set for each shard***\\*. This means each shard is duplicated at least once in case one fails. So, there will be a primary shard and at least one secondary shard.\\*\n\n*It also creates something called a*\u00a0***Mongos instance***\\*, which is a\\*\u00a0***query router***\\*. So, if an application wants to read or write data, the instance will route the query to the correct shard.\\*\n\n*A Mongos instance works with a*\u00a0***config server***\\*, which\\*\u00a0***keeps all the metadata about the shards***\\*. Metadata includes how many shards there are, which chunks are in which shard, and other data.\\*\n\nhttps://preview.redd.it/vwskco26pu4e1.png?width=1313&format=png&auto=webp&s=d62a63e3e250d937c824456864b09f26d5f7c3f7\n\n*Stripe wanted more control over all this data movement or migrations. They also wanted to focus on the reliability of their APIs.*\n\n\\---\n\nSo, the team\u00a0**built their own database infrastructure called DocDB**\u00a0on top of MongoDB.\n\nMongoDB managed how data was stored, retrieved, and organized. While DocDB handled sharding, data distribution, and data migrations.\n\nHere is a high-level overview of how it works.\n\nhttps://preview.redd.it/qyf2kd7bpu4e1.png?width=1137&format=png&auto=webp&s=5e13978d8d7a7d6b7b72fff600255d0a14ca2579\n\nAside from a few things the process is similar to MongoDB's. One difference is that all the services are\u00a0**written in Go to help with reliability and scalability**.\n\nAnother difference is the addition of a CDC. We'll talk about that in the next section.\n\n# The Data Movement Platform\n\nThe Data Movement Platform is what Stripe calls the '*heart*' of DocDB. It's\u00a0**the system that enables zero downtime**\u00a0when chunks are moved between shards.\n\nBut why is Stripe moving so much data around?\n\nDocDB tries to keep a defined data range in one shard, like userIDs between 1-100. Each chunk has a max size limit, which is unknown but likely 128MB.\n\nSo\u00a0**if data grows in size, new chunks need to be created**, and the extra data needs to be moved into them.\n\nNot to mention, if someone wants to change the shard key for a more even data distribution. Then, a lot of data would need to be moved.\n\nThis\u00a0**gets really complex**\u00a0if you take into account that\u00a0**data in a specific shard might depend on data from other shards**.\n\nFor example, if user data contains transaction IDs. And these IDs link to data in another collection.\n\nhttps://preview.redd.it/dz05y86hpu4e1.png?width=1437&format=png&auto=webp&s=985fc057f3ad9ea9ba8d4241083125b12f49a72a\n\nIf a transaction gets deleted or moved, then chunks in different shards need to change.\n\nThese are the kinds of things the Data Movement Platform was created for.\n\nHere is\u00a0**how a chunk would be moved**\u00a0from Shard A to Shard B.\n\nhttps://preview.redd.it/o9yr2z6mpu4e1.png?width=1391&format=png&auto=webp&s=00d8eae6244339fc00e49c1b15ff2e601cbfd75b\n\n1.\u00a0**Register the intent.**\u00a0Tell Shard B that it's getting a chunk of data from Shard A.\n\n2.\u00a0**Build indexes on Shard B**\u00a0based on the data that will be imported. An index is a small amount of data that acts as a reference. Like the contents page in a book. This helps the data move quickly.\n\n3.\u00a0**Take a snapshot.**\u00a0A copy or snapshot of the data is taken\u00a0**at a specific time**, we'll call this T.\n\n4.\u00a0**Import snapshot data**. The data is transferred from the snapshot to Shard B. But during the transfer, the chunk on Shard A can accept new data. Remember, this is a zero-downtime migration.\n\n5.\u00a0**Async replication**. After data has been transferred from the snapshot, all the new or changed data on Shard A after T is written to Shard B.\n\nBut how does the system know what changes have taken place? This is where the CDC comes in.\n\n\\---\n\n# Sidenote: CDC\n\n***Change Data Capture***\\*, or CDC, is a technique that is used to\\*\u00a0***capture changes made to data***\\*. It's especially useful for updating different systems in real-time.\\*\n\n*So when data changes, a*\u00a0***message***\u00a0*containing before and after the change is*\u00a0***sent to an event streaming platform***\\*, like\\*\u00a0[*Apache Kafka*](https://kafka.apache.org/)*. Anything subscribed to that message will be updated.*\n\nhttps://preview.redd.it/djf7ipoqpu4e1.png?width=1439&format=png&auto=webp&s=b8eeecd66ded2ea0792d4e74f49775d53e965e2a\n\n*In the case of MongoDB, changes made to a shard are*\u00a0***stored in a special collection called the Operation Log***\u00a0*or Oplog. So when something changes, the*\u00a0***Oplog sends that record to the CDC***\\*.\\*\n\n*Different*\u00a0***shards can subscribe to a piece of data***\u00a0*and get notified when it's updated. This means they can*\u00a0***update their data accordingly***\\*.\\*\n\n*Stripe went the extra mile and stored all CDC messages in Amazon S3 for long term storage.*\n\n\\---\n\n6.\u00a0**Point-in-time snapshots.**\u00a0These are taken throughout the async replication step. They compare updates on Shard A with the ones on Shard B to check they are correct.\n\nYes, writes are still being made to Shard A so Shard B will always be behind.\n\n7.\u00a0**The traffic switch**. Shard A stops being updated while the final changes are transferred. Then, traffic is switched, so new reads and writes are made on Shard B.\n\nThis\u00a0**process takes less than two seconds**. So, new writes made to Shard A will fail initially, but will always work after a retry.\n\n8.\u00a0**Delete moved chunk.**\u00a0After migration is complete, the chunk from Shard A is deleted, and metadata is updated.\n\n# Wrapping Things Up\n\nThis has to be the most complicated database system I have ever seen.\n\nIt took a lot of research to fully understand it myself. Although I'm sure I'm missing out some juicy details.\n\nIf you're interested in what I missed, please feel free to run through the\u00a0[original article](https://stripe.com/blog/how-stripes-document-databases-supported-99.999-uptime-with-zero-downtime-data-migrations).\n\nAnd as usual, if you enjoy reading about how big tech companies solve big issues, go ahead and\u00a0[subscribe](https://newsletter.betterstack.com/).",
        "score": 623,
        "upvotes": 623,
        "downvotes": 0,
        "tag": "Blog",
        "num_comments": 40,
        "permalink": "/r/dataengineering/comments/1h6iwx2/how_stripe_processed_1_trillion_in_payments_with/",
        "created": 1733326569.0,
        "comments": "Wow actual good DE content on this sub, props\nWhat? You don't want to have a discussion about how Pandas isn't very good for data engineering?\nPandas is for data analysis and not for data engineering\nWhile working last week I found some pandas dataframes in a pipeline which ingests toooons of data - at least dozen of jobs running once per week and some every day, some of them running almost 2 days. We are getting data from API via Python and then uploading it to Snowflake. What would be a good replacement for Pandas? Cutting the time in half would be a good argument for a pay rise :)\nyeah, but do you consider yourself an swe? im a da wanting to transition to either de or mtf: help me decide. what is appropriate salary in europe?\nLmao people don't see the /s dripping from this post\nhttps://youtu.be/b2F-DItXtZs?si=bbXCceAlm_iprdP6\nLol why have I never seen this before. This is great \nyeah man, great job btw, I am just joking (good sense of humour on your end). Sick engineering.\nSo good lol. A bit too strongly worded to share with my PC colleagues but thank you for reminding us of this gem :)\nPosts like this are why I joined this subreddit, nice work!\nI read this while sharding on the toilet! Great read\nwhat did you use to create the flowcharts/diagrams?\n[https://excalidraw.com/](https://excalidraw.com/)\nI miss excalidraw.. My work moved to lucid and it is more, but worse. I love how fast draw helped me explain ideas\nWell kudos to Stripe. Building a DbaaS using MongoDb is hard (heck building any DbaaS that is scalable is hard). I really only know Google Spanner that does this, and the effort behind the scenes to keep it rebalancing and reliable is absolutely non trivial. Question I have is do they address the CAP theorem.\nYugabyte and cockroach do this too\nOkay I need to read this again. But looks aersome\nI dont want to be that guy but i work at a legacy fintech that competes with Stripe (and is much larger, think 8x volume). We have this exact same architecture, we use Snowflake/Astronomer. From my understanding its pretty standard to use this architecture across the industry. The only difference is we're ingesting data from legacy mainframe core.\nHow are you serving latency sensitive transactions with snowflake and astronomer/airflow? Doesn't that defy the point of the on demand querying if it's running 24/7 or is there some sort of hot cache or something else?\nI will go a bit more in-depth. Essentially what im getting at is that Stripes approach is redundant. We use a mainframe to accomplish the same thing Stripe does. Mainframes are used by Visa and other payments giants because of their reliability. Bandwidth is the smallest part of what you are concerned with in these kinds of processing loads. Database access, automated fraud detection, business rules and maintaining the order of operations consume far more than just shoveling transactions around. There are multiple elements to a transaction (preauth, auth, fraud, etc.) Mainframes do have a huge edge in stability - it's the biggest marketing point in their favor. So long as you keep your maintenance contracts up to date and your applications can support it, their uptime can be measured in years. Commodity servers' computation abilities have grown, but they don't have the kind of silent failover / disaster recovery that come natively in mainframes (Kubernetes is a great example.) We batch process files daily into snowflake from our mainframe via mainframe files and control tables. The ETL is handled by Astronomer DAGs. Data lands in abstract layer before being cleaned and row level encryption and protocols are applied before being brought into views or shared via snowflake data share to various environments for different products. Everything is handled by DAGs and Stored Procedures. The downside is we dont have real time transaction data, as ETL is daily. The upside is we have the robustness to process credit, debit, prepaid, ach etc. For BI applications we've built specific models and the data is cached server side after one time load.\nI think, like you said, there are tradeoffs between Stripe's real-time model and your company's daily ETL model. Both have failover protection. I'd venture to guess that Stripe has active-active configuration that takes advantage of the sharding while the RDBMS model more likely has active-passive. This can affect availability. The RDBMS model seems to favor strong consistency while I'd imagine Stripe favors eventual consistency that has its order of operations managed by its CDC. Not to meme, but the NoSql model does scale better from its partitioning. Whether that scale actually matters, well, judging from what you've said it doesn't. OP didn't mention Stripe's availability, but looking it up their SLA guarantees 5 nines (99.999% or 1/100k drops) which beats out MSSQL Server's various configs (Azure SQL DB, Azure VM, Azure Managed Instance) that only at best guarantee 99.99% (1 in 10k drops). At a huge scale, that high availability does make a difference. All that said, Stripe's configuration strikes me as likely to be extraordinarily expensive, which makes me reconsider why they're still a private company. Perhaps if their financials were public, it'd be made clear what their margins are and investors would balk, similar to Snowflake's issues.\nIt is very expensive what they're doing. They essentially built custom database functionality on top of MongoDB. There aren't a ton of companies that have the resources to do that and maintain it going forward.\nAzure SQL db is 99.995%. I am 100% sure stripe is not 99.999% as they claim since everyone (including Azure sql) cheats in these numbers a little bit. They most likely do not count the duration during planned maintenance upgrades.\nMany banks and payments and card processors do move transactions in real-time from mainframe, nonstop, oracle, postgres etc. (into their olap/lakes/Kafka etc)... Several big CDC tools have been doing this for decades. Eg; goldengate started as CDC tool for tandem non-stop based cash machine backends back in the 90s . Pretty much all the top banks do multi active dbms and real-time to analysis...\nIm not saying it is not possible. Its just not something that we do because usually the juice isnt worth the squeeze. There is no real purpose for us to have real time data.\nHey u/spock2018 A newbie here, is your architecture build on top of relation databases? I was wondering if relational database offer more advantages for these situations?\nYes, at the volume we're processing using a nosql approach is not really viable, and generally if you went that route you will end up wishing you had built a relational solution very quickly. Concept at every payments company is the same. You have your daily snapshotted aggregated transactions table where each record is a transaction and then you have your daily snapshot account fact where every record is an account. Those two are joined by some unique hashed value (DPaaS). From there you break out the transaction and account dimensions. (Mcc codes, meta data for trans; Household info for accounts).\nThank you for shedding some light on it, when you want to do joins, how do you make sure it stays performant? Is it a combination of right database and sharding?\nUse the correct joins, use performant CTE's, use date drivers, where clause and window functions.\nNice read, thx!\nVery Informative , thanks a bunch.\nNice recap post of the stripe blog post. I think that your blog post could be taken to the next level if you spend a bit more time explaining what MongoDB's out-of-the-box solution lacks before going into what Stripe built for their custom needs. Right now, you just jump right in, leaving the reader wondering what the situation would have been without all the extra infrastructure that Stripe built. Right now idk how many 9s of availability you get with out-of-the-box MongoDB vs what Stripe gets with their in-house system.\nLearned something today, thanks!\nWell written ,we need more posts like this in the sub\n.\nThis reminds me of the Tablets architecture used by ScyllaDB and YugabyteDB. Small, easily migratable chunks of data - yep, checks out.\nsmall correction: shards are not cloned in mongodb. primary shard holds all the unsharded collections as well as the sharded collection. there is no secondary shard. if a node in the replication set goes down, then another node in that set will take its place. if a whole shard (all 3 nodes that make up a replication set) goes down, you lose access to all the collection data in that shard. no other shard will come up to take its place. cool article otherwise!\nCatchy title with no substance. 1T/year is not that much compared to many other companies and it doesn't really say anything about the infrastructure's performance. From all I know it processed a single transaction worth 1T. This would make it extremely underwhelming. Realistically, based on some very poor information I quickly got from Google, Stripe processes about 400 transactions per second. This is still not impressive. Peaks are very important tho... What's important is volume. Now, getting back to the technical side of your article. Yeah, it's cool stuff, but what's the value it brings and why is it better than other strategies? I like it when I see articles like this, but I like it a lot more when it's wrapped in a relevant business context that doesn't try to hustle me.\nok cool. you know an asterisk is essentially like a 'look here!' or like a pointer to a reference or footnote. tf are there like 80 \\*s in your post, psycho? that said, interesting enough and thanks for the share\\*.\\*",
        "content_hash": "54b626b28a66dd7203f8800430d22448"
    },
    {
        "id": "oyju56",
        "title": "DataEngineering 2021 in one pic",
        "description": "",
        "score": 607,
        "upvotes": 607,
        "downvotes": 0,
        "tag": "Career",
        "num_comments": 53,
        "permalink": "/r/dataengineering/comments/oyju56/dataengineering_2021_in_one_pic/",
        "created": 1628174775.0,
        "comments": "OP: Appreciate the effort and think you did a really great job of providing a pretty comprehensive mapping of the things we might touch in our day to day. Entry-Level/Aspiring DE's: Please do not take this to mean that you need to know everything on this diagram. IMO, you need to be familiar with the main yellow cards here, but you by no means need to have a large depth of knowledge in *all* of them. I don't know of a program in the US with a curriculum that covers all of these things (Other nationalities, please feel free to disagree if there are comprehensive DE programs where you're from). A company worth its salt will bring you on if you have a general programming language, SQL skills, can wrap your head around a pipeline, and at least some idea of how to test what you're implementing. TBH, if anyone was a master, or even remarkably proficient across the board in the technologies and concepts above, I would not hesitate to worship them as a DE deity. A lot of the technologies above are still relatively new, and I would imagine that most of us in the field are still learning many of them (May be projecting my experience on others though). However, this diagram is a great tool to direct you to concepts you would want to learn more about if you are interested in this career path. Just *please, please, please* do not feel like an imposter for not knowing these things or overwhelmed by the idea that you need to know them all Current DE's, please feel free to disagree with my sentiment above.\nI would agree with the above - there's no point in knowing \\*all\\* of these, especially to a great degree. Understanding what happens at each level is way more important (as a DE) than knowing all technologies in an area (or even in multiple areas). Concepts > Technology - if you understand what's going on, you can usually sort out why it's different. If I mention using an RDBMS, it's more useful to understand that there's a relational system in place, how to query it in general, etc. The syntax of the commands may change in each database but you'd know enough to look it up. On the data processing layer, understanding the variance between batching and streaming and why you'd use them means more than knowing Kafka, Spark Streaming, and Storm. Side note to this is that it can be really tricky to understand a concept without having implemented a use case within a specific technology. For example, if you start with Postgresql and have never used a database before, you've got a fair amount of learning due to SQL, relational setup, etc. But if your next project requires MSSQL, you'll be able to sort out what's different much more easily than if you just tried to learn both at the same time. Not so hot take... learn SQL - by far the best bang for the buck of anything in the data (analyst / engineer / science) space.\nI totally agree. Most of the tools listed are good to know to become \"DE DEITY\" as u said  , and not must needed skill to become a DE. Thank you for clarifying to everyone. Note: This is not done by me ( though I wish ). I stumbled upon this from source:http://datastack.tv/ FOR BEGINNERS: Imo the order will be ... Sql-Oltp-olap-Data warehouse concepts-dimension modeling-scd types -shell scripting-python -pandas(dataframe) -map reduce concepts-spark(pyspark/sparksql).\nGreat resource, please include due credits to the original creator: - https://github.com/datastacktv/data-engineer-roadmap\nSorry new to reddit this is my first post. Not able to edit the post so I did that in my first comment. source: http://datastack.tv. they do have good number of online courses for DE and more coming soon. Everyone please check them out.\nContext for aspiring DEs. I'm a senior with a career that is going pretty well. I'd be comfortable putting a tick next to maybe a quarter of these and can talk with reasonably intelligently about a third beyond that. Still loads to learn.\nThis career is a marathon. Knowing a portion of these techs makes you extremely valuable at a lot of companies. I wouldn't rush it.\nYes ...And SQL \nI was about to say, this is an *amazing* map of the general DE curriculum; the lack of Databricks was pretty much the only question I had about it.\nYeah, I was actually pleasantly surprised at this diagram. I've been in this game for a long while, and while I would swap out Pig and MapReduce for Spark/Databricks in 2021, its a pretty good map.\nSQL, SSIS, and a loooooot of .csv and .xlsx that people say \"add this to the model\".\nHonest question, how are you using Databricks for pipelines? Do you mean notebook code?\nWe use Airflow to orchestrate our pipelines but it does have a built in job scheduler.\nAt previous company where i introduced databricks to point of being production tool. We'd upload UberJar files to dbfs then execute runs using the databricks Api passing in the dbfs location of the jar file + application arguments. Then the API call was scheduled using Python in airflow just to get which directories needed processing/dates etc. Worked very well for scheduled jobs and saved money only using notebooks for dev/tinkering.\nActual reality: Data \"jails\" in ERP systems and Excels You guys have useable data?.gif\nWhy you gotta call me out like this\nOh, yes, the unending joys of vendor tool hell.\nI see Java is a general recommendation but Python is only a personal recommendation. Is Java really that common in the data engineering world? I really haven't come across it all. Also just for fun, I typed in \"data engineer java\" and \"data engineer python\" in indeed for my city (Los Angeles) and got twice the results for python (and actually \"python engineer scala\" got more hits than java)\nI'm also suspicious of that. However, back in the days Java was the heavily used in big data projects.\nJava is very much present in the DE space, many ETL tools are java first or include java API. Apache Beam, Samza, Hazelcast Jet, many ETL proprietary vendors.. I'd take them anyday over most of the python mess I have to deal with.\nAs much as people love to hate on Java, all of Hadoop and Spark and the million other Apache products in the diagram are written in Java(and Scala). If you don't know how to read a Java stacktrace you're gonna be in for a suprise.\nDepends on what you do. Java is way more common in streaming world with Flink.\nA lot of big data stuff is in Java. The Hadoop ecosystem (hdfs, hive, zookeeper, etc) is all JVM based and a lot of early big data engineering was writing mapreduce jobs in Java. Kafka is also written in scala, which is a jvm language. The industry is definitely moving towards python, but jvm languages will always give you that advantage with speed when you really need it.\nI work with Scala as the main thing I write software in, and I'm in a team of Python users so I support them too. There are definitely more roles with Python out there as it covers a wider range of use cases in a businesses growth stages. Anything where you really NEED to know Java and/or Scala you're looking at a pretty well established business or more technical use cases that can't be covered with existing tools out the box. There are a shitload of roles out there that require basic python and a SQL technology. less that require Spark and even less that require some sort of custom real time applications plus Spark plus Cassandra et al.\nNo Microsoft SQL Server?\nThis pic and others, seem to mostly ignore the Microsoft ecosystem I've noticed. They usually include Azure Blob storage but ignore almost everything else. I'm actually surprised this one has Synapse on it.\nI feel like most Azure companies are in .NET world using and that's like a different planet for some reason.\nWhy use it over postgres?\nHow to be a Data Engineer for the rest of your career life:\n\"Learn how to write clean and extensible code. Spend some time understanding programming paradigms and best practices. Get familiar with an IDE or code editor like VSCode\" Wish this was my day to day reality. Teammates' shitty notebook code lolz at this so hard.\nThis diagram made me feel satisfied to an extent. cause my work revolves around Denodo and solving customer issues ranging from Data extraction to consumption and other integrated components of the platform. I'm glad that Ive been able to get atleast a tinge of knowledge/ aware of most of the tools mentioned here and this chart makes total sense now as to what role a DE exactly plays.\nDude, thanks a lot! Some hours ago I was looking for something like this.\n\nData Engineering is not all that. There are probably 1000 ways to classify tools and skills, but still... There are skills: * \"must to have\" (something like 70% from the list): SQL, Different RDBMS, Hadoop, Columnar DBs, (other DBs), Messaging Queues, data processing tools, storage format * \"nice to have\": programming knowledge, orchestration tool/s, experience working in Cloud, OLAP,... * \"bonus\": all other There is no real rule. One probably may have 100% of \"bonus\" knowledge and one skill from must/nice to have, but it may be enough to enter the field. If somebody studied CS, he has enough knowledge to enter the field. He won't probably be productive at first, but it is like in any other area\nI believe Jenkins doesn't deserves the tick any more, while Apache Arrow does.\nLove this map. What has been amazing is that my company started with a couple of these and has gone from 1999 tech to 2020's tech in the past 2 years and I've gotten to see all these concepts in practice. So I've been doing interviews and they're going very well at the moment. Any thoughts on salary for what someone who does know how to use at least one tech in every box on this path?\nI have a long way to go... \nThat diagram is a lifelong curriculum in reality, you don't need to know everything on that list. Plus, companies use DIFFERENT stacks.\nAs a newly-hired entry-level data engineer, I can't tell you how much I appreciate this! Thanks!\nGuys, where do you put databricks on this Pic? I am a little confused :\\\\\nData lake section?\nCluster computing technologies. AWS EMR is a managed version of Spark and competes with databricks.\nWhy not ?\nAh I see\nS3 is the storage technology of AWS. When you need to store something on AWS, you store it in a S3 bucket\nVery nice. Appreciate the effort on this!\nThis complements the previous: https://a16z.com/2020/10/15/the-emerging-architectures-for-modern-data-infrastructure/\nI am studying data science right now. Can anyone mentor me on becoming a DE?\nHi r/dataengineering , i am a PHP Developer i want to ask you some questions i want to be a data engineer i already know a lot of the technologies listed from the chart do i need to learn python and go into data engineering stuff directly or make web apps with python first then go to the data engineering route ?\nNow the main question is how to find the best resources to learn these topics?",
        "content_hash": "acad4ca03e2ac53ceb9b3764bf9c9add"
    },
    {
        "id": "o210i3",
        "title": "I wrote a children's book / illustrated guide to Apache Kafka",
        "description": "hi fellow Data Engineers, just wanted to share a beginner-friendly resource that I've been working on for Apache Kafka. I spent 3 months illustrating this digital book, which I'm calling Gently Down the Stream: [http://www.gentlydownthe.stream/](http://www.gentlydownthe.stream/)\n\nI know that normal tech books that span hundreds of pages are often intimidating for beginners (I wrote one of those too, btw: [http://kafka-streams-book.com/](http://kafka-streams-book.com/)). So I just wanted to create something a little simpler, more colorful, playful, and fun. Hope you enjoy it!\n\n&#x200B;\n\nedit: wow, thanks for the kind words everyone!! I'll be creating more books like this in the future, and will be announcing them on my new Twitter account if you're interested in following: [https://twitter.com/\\_round\\_robin/](https://twitter.com/_round_robin/)",
        "score": 611,
        "upvotes": 611,
        "downvotes": 0,
        "tag": "Career",
        "num_comments": 45,
        "permalink": "/r/dataengineering/comments/o210i3/i_wrote_a_childrens_book_illustrated_guide_to/",
        "created": 1623946550.0,
        "comments": "From now on, I want every technical concept explained to me in children's book form.\nNext, quantum computing!\nNext, funeral home embalming.\nNext, JavaScript\nThat was absolutely perfect OP! Im a beginner and was struggling to decide what I want to start learning but I guess now I want to learn more about the otter's amazing system. In all seriousness, the intimidating technology was for the first time something I could conquer! Please more content OP!\nThis is on an'otter level! Really cool, thanks for sharing it!\nThis is the highest quality content I've ever seen on this sub. Thank you.\nI love this. Thanks for sharing!\nAmazing OP. Great work!\nLove your drawings! That's a perfect way of explaining Kafka. Such a friendly approach to bring dry CS concepts to everyone! Have you considered doing illustrations for more concepts?\nAs a parent of a 1 year old, and someone learning Kafka this was a god send. Please do more!\nAwesome job, man! I love this. You put in a lot of work, and it shows.\nThats the coolest guide ever.. Period\nDude, what a awesome job.\nWell .thats Amazing...every fucking concept should be explained through Children books..\nI love this!\nLove it\nThis is amazing\nbrilliant! How would you represent zookeeper and the metadata into that?\nThis was amazing. Thank you!\nVery nice work, thank you for sharing it with us.\nThis was delightful\nThis is so awesome! Interesting work\nFantastic. The amusing little details are very cool (the ant stealing the cherry)\nAwww!\nThank you for really explaining it to me like I'm 5\nMe gusta\nNow I need to work with this person on one for entity resolution.\nThis was all the rage on Slack at work today. Nailed it!\nWow, that's great\nWhat a fantastic job! I'm in love with your drawings and the way you explained the concepts. So clear and wholesome! I'm looking forward to your next guide!\nAwesome! You managed to get the gist of what Kafka is (and does) in a really fun and intuitive way. Looking forward to what comes next :D\nJust read it, and its awesome \nI love this. Really nice work.\nThis is amazing.\nBrilliant\nAwesome\nThis was getting passed around in my work slack yesterday lol. Awesome job!\nThis is so brilliant. I feel short of words to describe how good this visualisation really is \nReally awesome thanks for sharing.\nThis is incredible. Such a fun way to learn about a technical topic.\nGreat job, :)\nNothing to say except congrats !!\nI loved it !",
        "content_hash": "5b5cc7559abbcd1f6b8c90a9dc0244ea"
    },
    {
        "id": "1bc0bkv",
        "title": "ELI5: what is \"Self-service Analytics\" (comic)",
        "description": "",
        "score": 580,
        "upvotes": 580,
        "downvotes": 0,
        "tag": "Blog",
        "num_comments": 105,
        "permalink": "/r/dataengineering/comments/1bc0bkv/eli5_what_is_selfservice_analytics_comic/",
        "created": 1710154042.0,
        "comments": "Missing from comic: * Customer refuses to make own pizza, insists that chef do it. Chef falls behind on ingredient prep. Owner gets mad at chef for helping and falling behind. Owner gets mad if chef refuses to make pizza. Owner refuses to hire more chefs. * Customer puts raw meat on pizza after cooking instead of before. Customer refuses to listen to chef that meat must be cooked. Customer shares raw meat pizza with other customers. Suggestion to remove raw meat from ingredients entirely is rejected by customer and owner. All customers get violently ill. * Customer doesn't like putting on pepperoni, so makes pepperoni putting machine out of legos and duct tape. Other customers start using pepperoni machine. Pepperoni machine breaks, all customers start yelling at chef. Customer who built pepperoni machine stopped coming to pizza shop 6 months ago. Pepperoni machine turns out to have been putting out ham the whole time.\n* Customer puts every topping on the bar onto one single pizza dough. * Crams ungodly mess into oven using brute force and prayer. * Complains repeatedly that the dough breaks. * Uses shovel to scrape entire ungodly mess into a slop bucket. * Puts bow onto bucket. Names it \"AI\". * Presents AI to management * Promoted to senior vice president * Engineering team gets order to automate all future AI production in the exact same way. God I hate that comic.\nI hate that comic but love this thread so much, I'm both laughing and crying.\nCustomer makes bizarre pizza attempting to recreate a chefs custom pizza. Complains to the entire restaurant that the pizzas are broken because they dont taste the same.\nCustomer doesn't know shit about veggis but remembers he once heard that olives are green or black and puts cucumber and charcol on pizza. Then he serves it to the elders convincing them that olives are wierd and everyone agrees - we will never invest in olives. All of the above isn't statistically verified ofc...\nAnd while you take care of the \"problem users\" you turn your eye away from the \"good users\" that aren't complaining. When you eventually get a chance to check on them, they're licking sauce off the wall and saying, \"It keeps me from starving, but it doesn't taste very good.\"\nThe heart shaped pepperoni cutting machine took 100 hours to make and not even the first customer actually cares to use it.\n* Customer takes the raw pizza home to cook in their own machine which somehow ends up duplicating the pizza. Customer can't figure out why they have 10 of the same pizza and it took twice as long to cook. Blame pizza shop because they don't know how... left joins work (idk i can't think of a metaphor)?\nDon't worry if your real-life scenario doesn't fit. It's a stupid metaphor.\nI feel seen.\n* customer tries to make a calzone, and when it doesnt fit into the oven, tries to jam it through. complains about the messed up 'pizza' and now you have to clean the oven\nWow I think you nailed it with this one.\nFrom customer angle: * The chef uses ingredients just like what I want for the VIPs custom pizzas, but those ingredients dont exist in my ingredient bar. * The ingredients in the bar seem a little more out of a can than the fresh ones the chef uses. * I tried cutting pepperoni into heart shapes but the pepperoni was so hard my scissors broke.\nLiterally just had to deal with a pepperoni/ham machine  Finance wanted a report they had apparently been getting for years. Guy who made the report left 6 months ago, left zero notes. I spent days trying to reverse engineer the process only to find out that the original report was basically made up and the guy that made it didn't even query from the database  but finance still wanted the report, so I had to speedrun building a semi-functional one that did query from the database. I think rather than \"self-service\" analytics for all users, each department has a dedicated analyst and the engineering focus is on enabling those analysts with the data they need to provide department specific reports. Then those analysts can collaborate with end users in their department. Data analysis should not be centralized, it needs to be democratized. But there are also plenty of people in every organization that are not equipped to \"self-serve\".\nBig time agree, and makes me think of one of my favorite quotes I heard in an interview Thinking is a solitary skill, but analysis is a team sport.  CIA Director of Analysis, Linda Weissgold\nThe hefty chuckle this gave me while reading this was, *chefs kiss*\nAlso apparently storing all the different sausages is too expensive, so the chefs provide animal fat, animal meat and different spices, except they are all labeled according to some esoteric logic that was used in the company 5 years ago.\nI thought this was gonna end with customers complaining that their pizzas taste like shit and they don't have time to make their own pizzas anyway. I've never seen self service analytics work at scale. Sure, you can have a few users who manage to build a couple of dashboards, but that's it.\nHave you ever been to a place that you're thinking \"yes, I love feeling empowered to do this\". WhoTF makes this shitty comic like strips??\nIt all sounds GREAT until you try to teach Excel monkeys the finer points of wood fired pizza making. That's where it falls down.\nThe moment you get an email from a departement head that your data must be incorrect, as it should show 98 lines , but she only gets 50. And in the screenshot you see at the Bottom: \"limit to 50. Likes 50/98 showing\". At this point it all seems pointless.\nIn the screenshot the line 'Showing 50/98 lines\" is manually circled to prove that there are only 50 lines.\nWow this destroyed me, such accuracy\nIt would absolutely not be beyond this person tot do this. I need a shower now\nLiterary today. \"Could you make an export of table x? Customer tried it himself, but got only 1000 rows\".\nExactly, show me one organization where this worked like the comic. Some users will just refuse to touch the ingredients.\nIt worked at a company where I implemented it. It was awesome. Not sure how I \"show\" that to you, tho.\nI mean there are different factors you can share. How old is the company, which sector does it work? How many employees? How long was it between your implementation and you leaving? Would definitely paint a more complete picture.\nThe company was between 10 and 15 years old when I was there. It was in the gaming industry, about 350 employees. The implementation was there for there years before I left. Hope that helps.\nThat and the ingredients are poorly labeled.\nAnd dont underestimate good old fashioned fucking laziness\nExcel is great until you have to work with others. Thats true whether theres people who know SQL in the org or not.\nIt's even better when you realize the Excel monkeys don't even know Excel in the first place and management thinks you can teach them coding lol.\nI have legit been approached by business people to highlight discrepancies in my dashboard data when in reality they were using the filters wrongly.\nThis happens almost daily for me\nFast forward six months and every department has their own pizza that they claim is the absolute version of truth, except all the pizzas taste slightly different and no-one is willing to work out why. I've seen some silliness on Reddit in my time but this is probably the worst. Either that or we've been expertly trolled in which case bravo!\n^This is the company I have worked at for the last 13 years.\nSame for 11. Its endemic I guess.\n\"And then the horrified chef noticed that a customer was feeding another customer with a strawberry-ham pizza that he called 'Pizza Hawaii' \". The end\nThe pizza dough used also contained gluten, to which the second customer was intolerant. But the customers never read the labels and manuals the cooking staff carefully made for the products.\nNearly all the pizzas end up only 10% eaten and left in the shop, but you're not allowed to throw them away. Also, it's your fault that the place is a mess.\nDamn, this reminds me of the dashboards that are built for \"self-service analytics\" then ended up being used a few times, signed off, and then never used again after three months.\nHad a C-suite client tell me that a dashboard I made for her wasn't useful enough. She had no idea we could see that she had never logged in.\nWithout paying\nI cant think of a worse idea than letting non data people do their own analytics. Hell, even the data scientists I work with do things sometimes that makes me question how they have a job\nAt my place someone relatively new but with a very sophisticate title asked: Why dont we simply make a LLM to help stakeholders figure out their needs? I almost choked on the salad I was eating that day. My manager asked Yeah, great idea, can you write down a high level plan on how to do it with our current setup for the next meeting? Consider we wont allocate budget for it asides of your team's? He didnt said shit next meeting. I love when people want to jump right into the playground before finish building it, because very early you realise youre playing in a swamp of technicalities that must be solved if you wanna run book-written automagical tools. Not that it doesnt happen elsewhere, but an effort was completed before that. I think self-service is only possible with formally trained / proactive / committed stakeholders. Weve conducted endless boot camps teaching sql, given preset tools and other handy tools and they just keep playing the Im too savvy to propose but too lazy / dumb to explain how, so come figure it out for me because youre the tech dude.\nTotally agree. We have a few people who do their own thing and it's almost always wrong. Sometimes they defend it, saying that they have done it this way for N years, so we should keep doing it wrong for consistency. The worst is when they get the right answer, but they've only happened to get it right because none of the things that could have been wrong happened. Then it's hard to even explain to them why what they did was wrong (e.g., it only worked out because they treated a relationship like it was 1:1 and it worked out that the nulls made up for the times when it wasn't 1:1, and they got the count right.) See, their way works!\nI was waiting for a punchline that never came.\nAnd now you have the CEO wondering why he has 7 different shaped pizzas as \"The pizza\" depending on which day he opens his email, they all taste different and oh fuck it with the analogy bs. This is all awesome and great until yoyu try to put it in practice. Now you hve 4 teams with a dashboard that shows revenue as different and the CFO has no idea which one is right. Then they call you to now debug 4 completely ETL/ELT/Dashboard/Analytics/you nameit to find which one screwed up. And then when you find that someone did a round somewhere and screwed the entire process, they get offended on how dare you say they are wrong and that they know how to do their jobs. Oh, and dont forget that inescapable ticket of \"My ETL/ELT/Dahsboard/blah is taking too long to finish, it's been 18 hours\" and you check and they made a Cross Join. and when you limit your queries to 4 hours, then 20 people come that their processes take longer than 4 hours and can range from 2 to 18 hours depending on the month.\nIt is so reassuring to me that so many of these experiences are universal. But really though we're working on a new pizza that's going to solve all our problems. We're rolling Canadian bacon up into regular bacon but I don't foresee that being an issue. (It's definitely going to be an issue)\nYour topping_type handler now needs to handle the possibility of topping_type[\"subtype\"] even though there are 300 toppings and only one has a subtype Also the subtypes now have subtypes but calling it subtype too was too confusing, so it's topping_type[\"subtype\"][\"type\"]\nI've been using the list of toppings that we agreed on based on the Google Sheet that Ops sent in November, and it only has 217, so I'm not sure where these other ones are coming from. EDIT: Apparently we brought in a contractor and they assembled a separate list that Ops is now using instead.\nWhat's even more hilarious is that if you check the poster's history, you'll see it's just an ad account from a BI platform. And it posted it in different subreddit and the Looker sub is like the complete oposite of the reaction here.\nAnd then the pepperoni on top of that comedy pizza, is that the post on the looker sub has 11 comments, half of them are them responding. Then on this post there are 80+ comments, and i dont believe they have responded to any on this version of the post.\nYeah this was a definitely \"oh shit\" moment.\nOh sounds cool and dandy but can we export it to excel, pdf and PowerPoint?\nHow this usually works out: 1. Business user buys a click, drag&drop tool that is so easy anybody can use it. 2. Business user spends months clicking, dragging and dropping, without getting any useful results. Gets annoyed. 3. Data analyst is called in to do the job, but \"it must be done with this tool!\" Expectation is: get it done by tomorrow! We have been waiting for this for far too long now. 4. DA checks data and specs (usually none). Finds there is a ton of problems with the data and suggests to write down the specs so we can all agree on what is needed. Says it takes maybe 4 weeks with this tool; but she can do it in 2 weeks using her tools (say Python, SQL, whatever). 5. BU gets angry and claims DA doesn't get it. The tool is easy to use and there is no need to add bells and whistles. Just do it! 6. DA spends time clicking, dragging and dropping, going slightly mad due to what amounts to the most inefficient way of working ever. Quits shortly thereafter.\nThe second to last panel is the important one. It's about raising Data Maturity so people can handle their own queries. If I've learnt anything from consulting about data across various different industries and company size, it's that there is one singular consistency: Even the most basic level of data maturity is extremely hard to reach.\nI can tell that you are a real consultant by the kindness of your euphemism.\nIt is missing the part where someone cooks their pizza partially and molds it into a sphere. Then they bring it home several hours later and leave it outside on the curb. Then they put it into the toaster oven. Then they feed it to someone who gets food poisoning. Then they make an angry support ticket.\nHonest question: How does the customer on slide 12 order a heart shaped pizza? If you tease out that part of the analogy, it becomes clear why self service is so hard. (This is not reddit-trolling, I'm 100% serious)\nUntil the restaurant gets burnt to the fucking ground from inputting an ingredient incompatible with the oven.\nCustomer asks where the bathroom is...\nWhat is \"Self-service Analytics\"? &#x200B; A: Microsoft Excel.\n\"Why are we paying the chefs so much if we have to make the pizza ourselves?\" Nice comic and I think some self service options are good. but issues do arise as noted\nHuh. i was fully expecting customer to either refuse/unable to learn or the chefs get a big downsize.\nThis is what would happen no joke. They buy this tool, the vendor gives them some Kpis like \"Now you need 50% less engineers\". Business love sit and cuts the data engineering team by 50% and gets this tool. Then it goes into prod. Users have no idea what they're doing. They know basically a select with a where and that's it. Suddenly users can't make their own transformations, dashboards etc and they think. \"Wait, why the fuck do I have to struggle with this when we have a data team??\" So now all these dashboard ideas, get queued back to the data team. Which is now 50% of what it was in term of headcount. So now it takes double time to resolver the users requests. Everyone is angry. Leadership is asking\" why is it taking so long? When it used to take half the time\". And before you can say you have half the people as well. They immediately switch gears or come with some BS like \"let's not focus on the past, let's focus on solutions.\" Bonus points if someone in leadership was the sponsor of this tool. So obviously they can't say \"Yeah we fucked up, kill the contract and let's figure it out\". Because they would lose face, then need to come up with some BS Sooo they tell the data team they have the solution!! The self service BI tool should make everything faster, so the data team needs to use thus tool instead of their common stack to be twice as productive. At this point the chefs go \"fuck this\" and start looking for a job\nLets just say the only organisation where I had self serve analytics was a clusterfuck of toxic culture, passing the blame, moving in circles and mistaking movement for progress. Never again.\nNot sure why this is posted here. I'm going to assume it is for a reaction. Its brutal, but here's mine. Its an ad in the style of The Oatmeal that isn't funny, doesn't come off authentic and feels like I'm being spoken down to. My take is that you are trying hard, aren't very creative, and if I were to hire you, I shouldn't be surprised to learn you are copy/pasting 80% of my analysis from prior work. &#x200B; Good luck, seems like you had fun doing it.\nKey difference between pizza and analytics: With a pizza, it only needs to taste nice to that customer. With analytics, the answers need to be correct, and the non-analysts self-serving their data have no idea how to tell if the answers are correct. All analogies break down somewhere, but this one unravels immediately.\nIve never seen or heard of self serve analytics actually working lol\nCustomer is C suite and demands pizza in CSV format in email, attempts to guide them to the pizza bar result in pay cuts.\nOh, look, a commercial disguised as a post\nUnless data analytics is taught in high school, I dont see how this would work considering how some managers dont even know how to work a pivot table!\nCan you take my pizza and put it in Excel for me?\nThis is actually genuis, because do-it-yourself pizzas would be terrible. If you want good pizza you get a pizza guy to make it for you.\nngl I was expecting some insane custom pizzas\nCustomer: I want my pizza in Excel!\nUnfortunately this assumes that customers aren't babies, which they somehow always are. Highly paid, old, dumbass babies.\nThe impetus to create something like this is good, but this analogy does not work at all. In fact, it misunderstands both restaurants AND analytics.\nAnd in misunderstanding both somehow elegantly makes the point opposite to what it aimed to make haha\n- Customers keep grabbing all the dough and ingredients possible and kicking and punching the giant mess into the oven. It takes 12 hours for that pizza to bake. Customers yell at chef that its their fault and the oven must be broken. Owner yells at chef that he should have somehow made an intelligent oven that would have prevented this. - One customer just throws an oily rag into the oven. Oven explodes causing much damage. Customer says again this is the chefs fault because he should have read his mind that he was going to do that. Owner yells at chef too. Chef goes out to cry in his car.\nSir, this is a Wendys.\nSelf-service analytics is a lie. It's an unreachable asymptotic state. Cute story, though.\nThis is a great analogy! Self-service pizza always sucks compared to something made by a chef. Topping choices matter and the assembly line style leads to low pride in production.\nOh fuck me, the sales bros have started making chocolate and pineapple pie charts to illustrate child slavery and putting our branding all over them.\nlol\ntl;dr\njust another of poppy's hare-brained ideas!\nGuys I bought 6m useless widgets! But dont worry, I did the analysis myself!\nthe problem is that my customers have a combined iq of 6\nMan, if I showed up to a restaurant and they tried to tell me I had to make my own pizza, I would be out the door and looking for somewhere else to eat before they even finished their sentence.\nWasn't this a Kramer scheme lol https://imgur.com/a/XkDRoKO\nThis feels more like a diss at analysts. Even gpt requires data marts to work and can only function on a single table. Power bi exists and yet i am still creating data marts for power bi users. Now, i throw llama index or lang chain on top of a flat domain-level table composed of multiple sources and reduce work for myself and the analysts. Semantic layers are more dangerous to us but require warehouses and tuning through datamarts to be effective. The data mart is essentially our RAG now.\nInstructions unclear, customer poisoned themselves, now they're dead.\nDo people try to cook a pile of tomatoes at pizza bars? Im not saying its not a good metaphor, Im just curious. Ive never been to a pizza bar.\nHead chef now has a smaller team of chefs, but several lower paid pizza technicians interacting with customers. Customers now not drinking at the bar as much. Potential for food wastage increased and lowered pizza quality impression with customers.\nIME the hardest part with self-service analytics is training the customers. Usually they have actual responsibilities so they don't have a lot of time to learn how to use new tools. And they certainly don't have the time to learn how to do confirmatory data analysis properly, it takes some people years to learn to do that. To fit the metaphor \"we will teach the customer how to put together their own pizza\" doesn't make a lot of sense if your workplace is the equivalent of a drive-through window.\nYeah and that's why it won't ever work.\nAnd lo! A manager asked cant you just\nDamn I have been waiting for a meme punchline till the end. Like the customer saying he doesn't know PizzaQL to make pizzas, so you still have to do it for them. Was disappointed that it's just a soulless corporate sales brochure\nThis comic is fucking stupid\nSelf-service is really the way to go. Most companies that try it do it in a half-assed way, where it's doomed to fail. The most important thing is to give them a read-only replica of the data, or a spare server, or whatever -- just so that whatever they do doesn't impact production. Companies [decide that] they can't afford it and instead spend people resources on having the monks developing reports and dashboards to be handed, occasionally, to the commoners. The belief that data can be democratized is amazingly entrenched. I suggested it at a conference a while ago, and the speaker mocked me from the stage. Fuck people like that.",
        "content_hash": "6abe87c85334ecaae8612ecd6bba29b1"
    },
    {
        "id": "udboyq",
        "title": "I've been a big data engineer since 2015. I've worked at FAANG for 6 years and grew from L3 to L6. AMA",
        "description": "See title.\n\nFollow me on YouTube here. I talk a lot about data engineering in much more depth and detail!  [https://www.youtube.com/c/datawithzach](https://www.youtube.com/c/datawithzach)  \n\n\nFollow me on Twitter here [https://www.twitter.com/EcZachly](https://www.twitter.com/EcZachly)  \n\n\nFollow me on LinkedIn here [https://www.linkedin.com/in/eczachly](https://www.linkedin.com/in/eczachly)\n\n&#x200B;",
        "score": 580,
        "upvotes": 580,
        "downvotes": 0,
        "tag": "Discussion",
        "num_comments": 463,
        "permalink": "/r/dataengineering/comments/udboyq/ive_been_a_big_data_engineer_since_2015_ive/",
        "created": 1651087787.0,
        "comments": "Not question but wanted to say to OP that your replies have been insightful and I appreciate you taking the time to do this!\nThank you for your kind words! I just started here on Reddit today so I wanted to give back to this wonderful community!\nIf y'all like this content. Please follow me at eczachly on whatever other platforms y'all are on. I'm eczachly everywhere.\nZach is the GOAT. If you don't follow him on LinkedIn ([https://www.linkedin.com/in/eczachly/](https://www.linkedin.com/in/eczachly/)) you're missing out!\nLooks like it's time to follow\nRumor has it that Netflix has by far the most rigorous DE interviewing of the FAANGs. What's your opinion on this? How does interviewing vary across the orgs that you've worked for? Can you walk through what a DE interview at Netflix might look like?\nSure. My interview at Netflix was broken into two four-hour interviews. In the first four hours: I had an hour interview on Spark fundamentals. I was asked a lot of questions about how to troubleshoot OutOfMemory exceptions, TaskNotSerializable exceptions, etc. I had an hour on data architecture. Discussing the tradeoffs between lambda and kappa architectures. When would I pick streaming vs batch? How would I architect a real-time version of Netflix's recommendation system? I had an hour on data modeling. When would I choose a graph database vs Hive vs a relational database? How would I model my tables for efficient querying? I had an hour on software engineering fundamentals. This was a more leetcode style interview and I was asked 2 LC mediums that I destroyed and had 15 minutes left at the end to bullshit with the interview. In the second four hours: I had a one-hour project deep dive. What was the biggest impact I had in my career? Project deep dive. I talked a lot about work at Facebook here. I had a one-hour behavioral interview. How do I give and receive feedback? How do I deal with failure? I had a one-hour leadership interview. How do I lead teams? How do I prioritize and compromise? I had a one-hour culture fit interview. This was mostly a quiz on the Netflix culture deck.\nJesus that sounds tough\nI failed in the first two hours\nI already failed reading through all of that.\nSurprised how much deeper it goes than traditional \"Data Structures & Algorithms interview questions with some [SQL interview questions](https://datalemur.com/questions) mixed in\". Someone needs to write a \"Ace the Data Engineering Interview\" \\*hint hint\\*\nHow can I learn more about the first three points?\n\"Designing Data-Intensive Applications\" by Martin Kleppmann is a great (theoretical) resource. Debugging knowledge usually comes with hands-on experience and familiarity with the JVM.\nI really enjoyed that book. It's most insightful.\nI learned these things through hard-fought experiences at Facebook. I wish I had some good resources to recommend.\nBrilliant - this is all excellent information. Thank you so much for the reply!\nPlease tell me you are describing the L6 interview and not the L3\nThis was for L5 at Netflix actually\nAs someone who regularly deals with spark OutOfMemory and TaskNotSerializable errors how do you answer that lol. My approach is to google it and try whatever shows up lmao.\nThe whole point of these questions is to pick out how much fundamental understanding you have of the Spark framework\nHow much understanding do you need for OOM? Either use less memory or get more memory.\nWas it the most rigorous interview you've had would you say? And if yes is it partially due to you being further along career wise?\nRigorous is a hard word to define. I've failed 3 data engineering interviews at Google. So I'd guess those would be more rigorous?\nFair, Netflix only hires senior talent though so I kinda see both sides. Based on your explanations above they view DE as a separate field and have very comprehensive interview process geared around it. I never really thought much of them but for their TC numbers but I am really impressed with both you and them. Thanks again!\nBruh\nJeeee what a fkin pain.\nMay I ask what the salary range and location were? Hope you got the job and hope they paid enough\nI got offered $365k in Los Gatos, CA\nIs that total comp or just the base?\nWoW impressive. I would love to read your answers on every question  That would help everyone understand/learn some amazing skills\nhow would you answer the leadership one?\nAnd this is for a L3 role?\nFor L5 role. Netflix doesnt hire juniors\nGotcha. Had me worried for a moment.\nI dont think I have reached the level to answer all of these questions. But I have this goal in my mind to reach that level sometime soon. Can you walk us through your learnings from your career and how did you grow? Was it just with the projects you worked on at the companies you have worked for or it involve personal pet projects, online courses/certifications etc? Thanks!\nGood lord, this sounds incredibly tough. I feel like I'd be way out of my depth there, despite being a relatively successful data engineer.\nSecond this\nIs it true that data engineering at FAANG is more heavily focused on analytics than software engineering?\nReally depends on the FAANG. That was true at Facebook for sure. Not so much at Netflix. Netflix really has their data engineers build pipelines with a strong software engineering mindset which was something that really attracted me to that company.\nSo if I'm more of a data analyst profile, apply at Meta basically? :P Also: do you know where Spotify lands in this analytics  SWE spectrum?\nHow stressful is Netflix DE compared to other FAANG?\nI think that the companies have different forms of stress. The stress at Facebook was sourced mostly from bad work boundaries. People pinging you late at night. There was also just a high expectation of outputting a lot of code. The move fast mentality caused a lot of engineers to take shortcuts in order to have more \"lines of code\" written for their performance reviews. This naturally created a lot of tech debt. The stress at Netflix was sourced mostly from unclear expectations. Netflix doesn't have performance reviews. Their expectations are \"be a stunning colleague\" which is very vague. And if you aren't stunning and fail your manager's \"keeper test\" you get fired. I found Netflix to be less stressful than Facebook in a lot of ways since I had a really supportive manager for most of my time when I worked there. But your mileage may vary.\nThis keepers test stuff sounds a lot like how Microsoft lost a decade.\nlines of code is a kpi in Meta?\nMileage definitely varies as I've had a totally different experience, but have only been at Facebook, not Netflix. Also was in London and think culturally there's a lot of difference when compared to the US. The one part I'd completely agree with is that move fast generates a lot of tech debt and people aren't incentivised well enough to reduce it. But I'm trying man, I'm trying.\nNot sure if you're still answering questions here, i came late, but i'm currently a DE at a fairly small analytics company and we definitely focus more on SWE principles while building our data pipelines and i'd like to continue with that. Can you recommend any companies, other than Netflix, where Data Engineers are more Software Engineers focused on data? &#x200B; Thanks for an amazing AMA.\nHow's your stock options holding up?\nIt's been a rough ride since November\nYoure god damn right\n\nWhat's the difference in duties between L3 and L6? What does big data entail you doing in that position? 6years is a good long time at FAANG any problem with burnout? How did you grow?\nGreat question! What's the difference in duties between L3 and L6? L3s are going to be focusing narrowly on probably 1 piece of a pipeline or building simple pipelines. L6s lead teams. I lead a team of 7 now and prioritize the work for them. I'm responsible for the data quality of a large organization of people. What does big data entail you doing in that position? So, this has changed since I worked at three different big tech companies in that time. Big data could be large event data that needs to be processed efficiently. It can also mean complex data that needs to be modeled in a scalable way. 6years is a good long time at FAANG any problem with burnout? Yeah. I actually did burn out in early 2020. I took most of 2020 off work and started back up again in early 2021. I was too hyperfocused on growing my total compensation and not taking care of my mental health enough. How did I grow? I focused beyond just data engineering. I focused a lot on getting better at writing and understanding people's emotions. This helped me tons in communication. I also focused on building my software engineering skillset. Strong software engineering fundamentals will make you a much better data engineer.\nThanks so much for the reply! I've been in data engineering for about 7 years myself though not at FAANG or anything I would call \"Big Data\" ,still in the Terabyte and under sizes. Towards the back half of experience here with teams and mentoring have definitely been good soft skill improvements for me. What do you see as next level for someone that works on mostly smaller NoSql / sql dbs?\nThere's been a huge shift over the last 2 years or so in data engineering where quality is really becoming in the forefront. I recommend learning dbt, Great Expectations, and Google BigQuery because I think they are the future of data engineering in a lot of ways. If you already have a pretty solid data quality skillset, maybe dabbling a bit with Apache Flink / Apache Spark would be a good idea!\nWhy BQ? Totally agree with your tech stack gimme that dbt and GE\n>I recommend learning dbt, Great Expectations, and Google BigQuerybecause I think they are the future of data engineering in a lot ofways. It's really interesting to see an experienced engineer give this take. This sub is very focused on SWE, and the analytics-focused DE roles are frequently dismissed as \"not real data engineering\"; there's a very strong bias for data platform work, with data modeling and data warehouse management being viewed as easier and less valuable. I'm curious if you think that tracks with your experience in the industry. In my recent job search, I definitely felt like a second-class citizen coming from a BI / analytics background; until I found the right fit, every place felt like they just wanted a Python / JVM engineer who knew the difference between INNER and LEFT JOIN.\nI'm surprised about the love for GE in this thread. I've found it difficult to work and I know I'm not the only one. What are we doing wrong, is it a case of just persevering with it and getting over the learning curve?\n> Yeah. I actually did burn out in early 2020. I took most of 2020 off work and started back up again in early 2021. I'm not in FAANG nor am I a real data engineer but I feel this so much. I quit my (fairly good) job at the start of the year to focus on myself and I'm now in a better place mentally, but I'm REALLY insecure about the job gap I have now. What do I say if people ask? Do I omit the gap? Was that ever a worry for you on your 2020 sabbatical?\nDefinitely was a worry for me when I started applying for jobs. After talking with recruiters, my worries were relieved though. A lot of people got laid off during COVID. I feel like you get a COVID-related exception and you shouldn't worry too much since so many people have had gaps over the last two years.\nAgain, another amazing answer. Any good reading you would recommend that would help understand peoples emotion or learn communication skills ?\nHow to win friends and influence people\n\nCan you elaborate on some of the said \"strong Software Engineering fundamentals\" for a beginner? You mean like Data Structures and Algorithms, or far beyond that??\nWAAAY more than DSA. Unit testing, CI/CD, proper documentation, integration testing, readable code, good system design\nI figured it went wayy deeper, unit testing and integration testing im not familair with, good system design im also not sure about I buid basic linux systems for a startup via VMWare, nothing Data Enginerring related thoughh, my good system design is Memory, HD, CPU and bootable  I'm familair with CI /CD and kinda understand Ansible, and we use Salt Stack somewhat, but I dont get into the code deployment side of things since I am just starting and dont have that CS Degree /Coding background... using Jenkins, using Terraform etc etc I know some SQL / Python, but I'm definately in that imposter /online video/tutorial phase.... I need to start picking up some projects, I do have the initial AWS Cert, and fairly decent Linux knowledge, and Networking Just need to bridge the gaps and create some projects I think I'm currently applying for some Jr Python Dev Roles, in hopes they will take a Systems /Networking guy who has the passion/drive just not the classical 4 year CS training\nJust wanted to say. Google \"EcZachly\" to follow me on whatever other platforms y'all are on. I'm EcZachly everywhere!\nHey, couldnt see you on LinkedIn with that handle\nThe first hit when you Google EcZachly is my LinkedIn\nYou can look for Zach Wilson and you'll find him.\nHi, would first like to express my gratitude for answering these questions! Ive been a data engineer myself in Europe for the past 7 yeas. Ive got 2 questions for you: - With the rise of Snowflake/big query and DBT, do you think that a shift will happen/is happening towards doing ETL flows more and more in DBT SQL and less in scala spark? Scala spark/flink for complex use cases, but dbt for 95% of the generic transformation use cases? - my company is heavily investing in data mesh, using snowflake as the 1 stop shop for data products that business teams can create,maintain and expose using DBT (if they have the necessary skills and have done the necessary trainings). Do you also see some talks about Data Mesh in FAANG or are there a lot of risks/concerns introduced with this paradigm shift? Thanks in advance for your answer!\n- for sure. I talk about this a lot on LinkedIn. - Uber is very invested in data mesh. Havent seen it much at FAANG though.\nDid you sleep at night while working with AWS glue? :)\nI've actually not really worked with AWS glue. I've heard good things about it though. I mostly use Apache Spark, Apache Flink, S3, dbt, Great Expectations, and Airflow.\nThanks for your reply. It's a pain sometimes. It's not mature like other aws services. Follow up: before L6, how did you manage data quality/integrity vs development speed? How far would you have to go to \"serve\" the consumers, was it enough just to leave it raw and let others model the data? I see that you mentioned dbt.\nThis is mostly dictated by company culture. At Facebook, the tradeoff would always be to prioritize getting data to consumers as fast as possible. At Netflix, they really focus more on quality and realizing it's more important to move slower so we can move faster longer term Personally, I like Netflix's approach to data pipelines more.\nI had an interview were I was asked about data integrity and a solution for solving it. I suggested using Great Expectations. The interviewer had said that Great Expectations does not really do what it advertises it does, is this true? What is your experience with it.\nGreat Expectations is narrowly scoped to single data set quality. It doesnt solve all quality issues for sure.\nCould you please recommend to start learning Spark.\nAny reason you havent worked with Apache Kafka? Ive noticed its super popular in DE for streaming pipelines.\nI like Glue. I just hate that its like 7 products in one. Catalog should be different from ETL. Shouldnt be under the Glue umbrella\nOne of my fav glue features is that you can now automatically scale up and down the number of workers on a spark job\nFor someone switching fields and looking for a junior data engineer role, what is you advice for them to get their foot in the door? Edited to add what is the most impressive thing on a juniors resume that you have seen?\nLearn Python and SQL really well. Build a really solid portfolio project that you're passionate about. I once had a junior DE show me this crazy pokemon portfolio project that scraped Twitter to determine who the most \"popular\" Pokemon were. That was probably the most impressive portfolio piece I've seen from a junior DE.\nAMA still ongoing? Any other language apart from Python that you would recommend?\nHow important is to have a good hold on oops, clean code, concepts for DE.? Which one do you prefer Java, Scala or Python for Pipelines?\nI've been coding exclusively Scala Spark pipelines since 2018. I use Python and Airflow for orchestration though. So both Python and Scala. I think it's more important to have a grasp on functional programming than OOP for data engineering.\nThank you Im stuck between Python and Scala! Beginner in both ( i write spark data pipelines in scala but) Im from Java background, any thoughts, on which one to focus right now?\nWell, 90% of data engineering roles are Python. So probably Python.\nThank you :) you are doing great, I joined your zoom meetings on sql, they are great! \nLate. But any thoughts on Haskell/\"pure\" FP btw?\nI've heard that FAANG ecosystems can be insular and that the skills they develop may be less applicable outside of each company. Did you find that to be the case?\nYou just highlighted one of the primary reasons why I quit working at Facebook. I didn't want to learn just Facebook data engineering. I wanted to learn data engineering in the commercial cloud (either AWS, GCP, or Azure). I jumped to Netflix because I was really attracted to the fact that they ran everything on AWS because that seems more \"real world\" to me.\n+1 this is why I quit Facebook too :) Huge feeling that, outside of the growth in product sense and in general self-confidence thanks to a flat structure that empowers everyone to try thinks and make their own mistakes, from a technical standpoint I was only really learning to be a better Facebook DE.\nI'm currently at Facebook and I fully value the growth in product sense and autonomy as being more valuable than the specific technical knowledge in the long run. And it feels that conceptually most of the stuff translates, and the specifics don't matter as much. That makes me think it wouldn't be hard to translate. How wrong is that view?\nHonestly? Youre totally on the money. But its about what you yourself enjoy - life shouldnt be about min/maxing. For me, I realised I missed the nuts and bolts of Data Engineering so moved over to a smaller company with a less mature data stack. Im building a lot from scratch and technically, its a lot more challenging but I love it. I did take a slight paycut, but at least for me its worth it for the increased enjoyment.\nThanks, I appreciate the insight!\nI'm curious what you would suggest and how you would approach tech choices if you had to accomplish similar tasks outside of those 3 commercial cloud environments. For example, say you had to support a high security use case that has to run workloads on VMs in an internal vmware private cloud. How would you approach that?\nHow involved were you in recruiting and hiring? If applicable I have some questions regarding those topics: * Does a CS background matter for you? * As a follow up, does the average SWE interview knowledge matter (like data structures/algos and stuff)? If I fail that stuff am I fucked? * Do you ever hire based on potential? i.e. candidate has a more SQL/DWH/Data Viz focused background rather than Python/Scala/Java, but he seems promising - what do you do? * Do prestigious companies on the candidate's resume matter? * Do job gaps matter? (say, 3 months) -- this is mostly a general question, but would be good to get a data engineering perspective on this. Asking for a friend ;) Thanks\n\\- CS background doesn't matter. I've hired people with no degree. Psychology degrees. And other engineering degrees. \\- DSA is important. If you fail the coding interview, you're out. \\- I haven't hired many junior candidates so I don't have an opinion here. \\- I would say prestigious companies make it MUCH easier to get interviews. Recruiters reach out to me pretty all the time. \\- I had almost a 1 year career gap in 2020 and still managed to land a very solid role in 2021.\nI mentioned this on another reply to you but I'm really insecure about my 3-4 mo gap even though I think my resume is serviceable. I guess having a stacked resume like yours makes the gap easier to overlook for recruiters lmao Again, thank you a lot for this. Might not seem like you're doing much but I've been going through a huge career slump / impostor syndrome / feeling like I don't have proper skills. This is helping me a lot, even if it confirms that I'm nowhere near a FAANG tier engineer l\nYou got this! I believe in you! Imposter syndrome is challenging to overcome but not impossible!\nThanks a lot buddy. I swear these are my final questions (I really feel like I owe you a coffee or something), feel free to get to this whenever you're available. If you were in my shoes: > Strong SQL, traditional ETL and data warehousing, i.e. very structured and relational - and strong data viz (I have mastered the Gartner BI quadrant at this point). > > I have very basic SWE skills (stuff like knowing my way around a terminal) basic Python, VERY basic Scala+Spark, basic to average knowledge in AWS/Azure data services and no real experience with the ~modern data stack~ 1 - Would you try to get into FAANG as a data analyst / analytics engineer type role first and then hopefully pivot? Can a FAANG data analyst be truly happy in the shadow of the DEs/DSs? 2 - In which order would you rank the following, for learning and development? (feel free to add or delete stuff): * Computer science fundamentals * \"Being an engineer\" fundamentals (i.e. the terminal shit I know nothing about, I don't even know how to call this) * Getting comfy with OOP with Python * Getting comfy with FP with Scala * Leetcode style DS&A * Learning Spark (with Scala) * Reading Kleppmann's book * Put learning on hold and switch to hands-on with a pipeline project, go all out with shallow-ish implementations of Airflow, dbt, Spark, Great Expectations and whatever is trendy. Hopefully learn stuff on the way and use it for a portfolio. Again, let me know if I can get you a coffee :)\nIm no hiring manager, but as long as you can prove youve used that time off developing new skill sets, or pursuing projects for your portfolio applicable to where you want to be, they wont really care as much.\nThanks, this makes sense. I'm starting to develop my skills from now on, but to be completely frank, I used my gap to sort some personal life shit, start exercising and no life Old School Runescape, No regrets - if the job gap subject ever comes up on an interview I'll just go with the flow and follow my heart.\nHow important is spark in your pipelines?\nExtremely important. I use Spark every single day. I've been able to scale Spark to pipelines that are 150 TBs per hour.\nWill be adding scala to my to-learn list. Thats really exciting man 150TB per hour, I didnt even know that was a scale of measurement.\nDo you use Pyspark or you try hard with Scala?\nI really don't like PySpark since it's not native and has problems with UDAFs. I learned Scala in 2018 and I've only written Scala Spark pipelines since.\nWould you be able to recommend any scala learning resources ?\nThis matches my experience. I've had to use pyspark when we needed to parallelize python models (mostly tensorflow, but stuff like FAISS). Seems like the Spark and Databricks teams have put a ton of work into PySpark but it still feels incredibly rough compared to Scala. Especially when debugging and tuning performance.\n[removed]\n\\- Tools like dbt and Great Expectations make enforcing data quality easier. I recommend not null, non-empty, and duplication checks since they have a very low probability of being a false positve. \\- Non-data engineers writing pipelines creates tech debt. I've seen some very obscene SQL queries in my time. These queries are often not very performant and bring the warehouse down. \\- Datadog is a pretty great tool for observing your pipelines. At FAANG they have a team that focuses entirely on this observability problem so I don't have any other really solid suggestions here. \\- Post mortems are so important. Make sure they are BLAMELESS. You don't want to demoralize people. You want to learn from mistakes and move on!\n**- Non-data engineers writing pipelines creates tech debt. I've seen some very obscene SQL queries in my time. These queries are often not very performant and bring the warehouse down.** Never, ever, have truer words been spoken.\nI find it's more about version control. The non engineers can write the pipeline. But the engineers have to *review and approve* the pull request for the pipeline. Creating this workflow is absolutely key.\nI'd add Pydantic to that short list of tools that have had a significant impact on maintaining data quality/integrity.\nHow are you?\nExperiencing some burnout right now. On PTO rn and taking a step back from posting on YouTube and LinkedIn while I reevaluate what I need to do with my life.\nI know you, are you Zach Wilson? I follow all your content on LinkedIn and medium! Love your work\nYep thats me!\nOH wow it's you!! I follow your LinkedIn posts and on YouTube eventually, and recently saw the one of you taking a break. Enjoy it, just wanna thank you for sharing your knowledge and educating people\nYoull see my medium and LinkedIn URLs are also eczachly\nSpoken like a true data engineer split between management and coding!\nExperiencing burnout too and on PTO right now\nThe business world pushes too hard typically. I read the biography of Eisenhower and he never pushed hard for more than 11 weeks or something oddly specific like that. Dont be too hard on yourself. The supreme allied commander in WW2 and President of the United States golfed for weeks at a time.\nWhat is your opinion of MLops roles?How much knowledege of ML do u have?\nI think it's a role that will grow VERY quickly over the next few years. I'm very bullish on MLOps. Personally, I'm okay at ML. I know precision vs recall. I know feature engineering. But if you start asking me about when to use random forest vs logistic regression and stuff like that, I won't have an answer since that's outside my skillset.\nAs a fellow engineer but at a non-FAANG (or MAANG) company, this is a great ama! Thanks for your responses and insights into your work, experiences, and ideas!\nFor sure. Thank you for your kind words!\nFor sure thanks for the AMA. I follow you on LinkedIn as well. If I'm ever in San Francisco I'll for sure try and do a coffee walk.\n* Now that you've moved into more of a leadership role, how do you see your career progressing in the next few years? * I think there is a general assumption that as a FAANG engineer, there is a high chance of adopting more stress and a worse work-life balance while getting paid more, and hopefully learning a lot from working for a top tier tech company. Do you feel like this has generally been true for you? Is this a path you would recommend for most DEs? I find myself happy with my current job but I can't help but feel like I'm doing my future self and family a disservice by not moving to a larger company. At the same time, I do have growing responsibilities at home, so I do worry about work-life balance more now than I did when I was younger.\n\\- I actually don't really want to get promoted to L7 because I do enjoy coding and one more level up on the ladder and you rarely code. I don't find that appealing. I like the balance I have now. I'm also focusing more on building out a side business and teaching people about data engineering. So I think instead of climbing up the ladder more, my next step will be hopping off the ladder. \\- I would agree with the stress and bad WLB tradeoff for more pay. Although if you have a good manager and mentor, you can actually have exceptionally good WLB. I know some people who have worked at Netflix for 10+ years because they've just gotten really good at what they do and at saying no and sticking up for themselves.\nAppreciate you taking the time to reply, I feel like posts like these are great for the community. Good luck on the side business!\nIf you had similar offers from Meta vs a funded start up that was looking for all rounder DE (building API, streaming data from Kafka, and ML ops) which would you pick?\nI'm never working for Meta ever again. So startup.\nIs it a culture issue or more related to the insular non transferrable tech stack?\nI'd say both. L6 expectations at Facebook are also extremely high\nAs an IC6, how much time do you spend for meeting and actual coding? Thanks for doing this btw!!\nMy manager and I try to get the split to be 50/50 but some weeks it's 80/20 and others it's 20/80. I code the most complex and risky pipelines that power revenue-impact machine learning.\nIt's so nice that you take the time to answer to everyone that I couldn't let my chance go. Right now I'm doing an internship as a software engineer in a tech company that it's 100% sure that will ask me to continue working with them while doing my master in computer science focused on Big Data, it's kind of their culture to hire interns to become apprentice. The thing is, as I'm aiming to become a data engineer and doing a master in big data, would you advise me to look for a job in data science/analysis while doing my master in order to introduce me into the data professional world, or should I keep doing my SWE job and try to do side projects in data? Ps: I'm saying a job in data science/analysis because it's very rare to find an entry job in DE. Thank you in advance!\nBoth options are viable. Depends on if you want to be more analysis heavy or more building heavy. I personally was an Android developer very early in my career and transitioned into data engineering that way.\nThank you for your response! Actually I have no professional experience in data analysis so I wouldn't know how it really is. So in that way I feel more comfortable in the building area, but I won't know until I try it! But thank you, your background shows that there are no really wrong choices :)\nI'm getting out and job hunting after being at starts up for 4-5 years now. I know that I've developed bad practices because start ups operate on the idea that you should be resourceful. In short I got good at everything except for hard core technical work. I'm doing hackerrank and leetcode among other things to prep for interviews. My gameplan is also to start interviewing with companies that I don't necessarily even want to work for at first, for practice. Does it sound like I'm doing the right things? I know some people at FAANGs that offered to recommend me in but I don't even want to put in an application until I feel that I have a good chance (knowledge wise).\nI personally only interview at companies that I would actually see myself working for since I view interviewing for companies that I wouldn't want to work for as a waste of time. If you think that interviewing for these shitty companies and getting offers would boost your confidence though, more power to you. The interviews will be very similar to leetcode most likely. So buckling down and practicing there before FAANG is probably the more effective use of your time.\nYeah interviewing at places that I don't particularly want at first is for both practice and a confidence boost. Contrary to a lot of what I read online, I've had it pretty easy in the past. Probably through sheer luck, I've never really experienced extra difficult interviews and yet I've gotten offers. Thanks for the response.\nHow common is it to see data engineers transition into more of a business oriented or CIO-esque role instead of the typical L3-L6 path? At some point, Id personally want to be more strategic and less in the trenches. Hope youre still enjoying your work.\nDefinitely. For me, the next role would one of: \\- Principal engineer I'd be working on how to design processes for other engineers to follow that reduce tech debt and increase engineering quality. \\- Engineering manager I'd work on prioritizing my teams work to be as efficient as possible. Both of these roles would be very strategic. My role right now is like... 50/50 strategic and in the trenches.\nWhat is your best piece of advice for someone coming in as an L3?\nAsk all the dumb questions that you want. Slurp up the knowledge of those who are more senior to you.\nAfter all this time, what do you enjoy most about your job? What trends in big data excite you most? What differentiates a beginner, expert and master in this craft?\nI enjoy solving business problems the most. Actually addressing pain points and unlocking insights. The trends Im excited about are the push for more real-time pipelines and streaming. I also really like the data privacy pushes that make data engineering more complex than just moving data from A to B. Beginners: know how to build pipelines Experts: know how to build optimized and complex pipelines Master: knows when a pipeline should be created at all\nHi Zach, thank you for this. I follow you on Linkedin, great content btw! As a data engineer (recent FAANG) interested in creating content DE/Analytics, any advice on how do I go about it ?\nConsistency is important. I posted for 450 days in a row on LinkedIn to reach 150,000 followers.\nJFC.. I respect it but I could never\nComing up with that much fresh content is a talent for sure\nDo I interpret you right that they look at lines of code on Facebook? As in, someone outputting 500 lines is more productive than someone outputting 15 lines? What if those 500 lines are redundant and just a lack of abstraction?\nIt's one of many indicators they look at for promotions from L3 to L4. They also look at things like business impact and what you did to make things better. They don't myopically look just at lines of code.\nOk, but lets say I wrote a library that would have a good business impact. Another person, person A, also wrote a library that has a good business impact. I dont do abstractions and do redundant code, but still manages to pass code reviews to get my code accepted. Person A however has a very clear, beautifully written, abstracted codebase that can accomplish in 15 lines what I might need 100 for. If both of us had about the same impact on the business with our libraries, will I still get a better performance review?\nBusiness impact is the most important thing they look at\nNo one is myopically looking at # of lines/diffs. One of the reasons it exists is that at the low levels it shows some basic indication of delivery and familiarity with the infra. No one is looking at that without nuance, or weighting it with any importance beyond the trivial statistic that it is. The value of what they delivered is what managers are trying to ascertain.\nThis maybe asked already. But I will ask anyway. How does one go from Novice to Junior ? Any steps, resources and projects you might think is helpful to do and follow through? Bootcamps, online tutorial ? I know there are tons of resources but I would like to know from your point of view ? What are the most common issues you find in Junior in terms of technical and other aspects ? What do you normally suggest for a Junior to do to rectify those common insufficiency whether its technical or otherwise ? Is there helpful blog, or other resources that you recommend to keep up with in the industry and what your experience has been like with said resources ? Thanks\nNot to be overly self-promotional but I started a YouTube channel to help DEs learn the skills needed. You should check it out! [https://www.youtube.com/c/datawithzach](https://www.youtube.com/c/datawithzach)\nOh my God it's you! I had seen you from an interview somewhere. Thank you for your answers\nIm kind of famous on r/LinkedInLunatics\nHere's a sneak peek of /r/LinkedInLunatics using the [top posts](https://np.reddit.com/r/LinkedInLunatics/top/?sort=top&t=all) of all time! \\#1: [Work from Home is old news, Work from ICU is the new trend](https://i.redd.it/ocwmwdi5l3s61.png) | [42 comments](https://np.reddit.com/r/LinkedInLunatics/comments/mnbsp2/work_from_home_is_old_news_work_from_icu_is_the/) \\#2: [BILL GATES](https://i.redd.it/etvibrxl8by61.jpg) | [43 comments](https://np.reddit.com/r/LinkedInLunatics/comments/n97d4f/bill_gates/) \\#3: [Finally, some honesty.](https://i.redd.it/ytd7anzhcrv61.png) | [11 comments](https://np.reddit.com/r/LinkedInLunatics/comments/mzuwax/finally_some_honesty/) ---- ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| ^^[Contact](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| ^^[Info](https://np.reddit.com/r/sneakpeekbot/) ^^| ^^[Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/o8wk1r/blacklist_ix/) ^^| ^^[GitHub](https://github.com/ghnr/sneakpeekbot)\nSubbed \nThank you\nyes! please post more!\nbtw, sorry to ping you. You seem like a bit of a sql wiz. I'm very basic in my knowledge and tend to solve my problems very inefficiently. What books/online courses can you recommend to get better? I feel like solving leetcode sql exercises in sort of a 'brute force' way (many unnecessary passes/temp tables etc. etc.) isn't helping me get better. Thanks for reading :)\nHow much of the work you do with dbt can be categorized as data engineering, and where does analytics engineering start? In my current position as analytics engineer i focus mostly on data modeling, housekeeping of the dbt project, developing macros for internal purposes like extending a dbt-labs authored package to better suit our external table usecase in Snowflake and make the ingestion of data into Snowflake through them more seemless and dynamic, but i haven't thought of myself as a DE. What are logical next steps to try and break completely into the DE role? I'm not really tasked with airflow or our cloud applications as this falls to the \"real\" DE's.\nProbably focus on software engineering fundamentals and getting good at handling scale. The lines are very blurry between AE and DE though.\nIf you focus and upskill, then I promise you can make it. Ive seen it happen\nWhat is your base and total comp?\nAt Facebook my base was $140 and TC was $245k At Netflix, I started at $365k base + 5% as options. When I left Netflix I was making $550k. At Airbnb, my original offer was base of $250k base $250k stock and $75k bonus. The stock component is a bit lower now since I got hired in Feb 2021 when the stock was at all time highs.\n\nha RSUs is such a double edge sword\nWhat was your total compensation each year? (Love your LI content, good to have representation in this craft especially in the SWE side of data)\nI answered this in more detail below. Started at FB at $190k TC. Left FB at $245k TC. Started at Netflix at $365k TC. left Netflix at $550k TC. Started Airbnb at $575k TC.\nSorry missed that, thank you for actually answering this. Didn't think you would, respect\nYou can easily look this up on https://www.levels.fyi so I think sharing isnt a big deal.\nIt's a little hard to filter for just data engineers tbf\nDo DE responsibilities vary across teams a great deal or mostly across orgs? For example - Is every DE at NFLX doing a variation of the same thing with same technologies (Streaming, ETL, Analytics) or is it very different depending on team or is it called a different title if responsibilities vary? Sorry having a hard time formulating my thoughts hopefully you understand my meaning.\nMore across orgs than across teams for sure. Some data engineers are focused on creating master data that has a lot of downstream consumption. Other data engineers are more \"vertical\" and work on building downstream datasets and dashboards. I think one that that is happening in data engineering is it's getting more defined. The master data DEs are the \"true\" DEs in my mind. The downstream dataset builders are more analytics engineers and FAANG is starting to hire more analytics engineers.\nTIL I learned I am a master DE and used to be a vertical DE. Thanks for summing this up so aptly!\nAny advice for DEs stuck in Sr (L4) roles? Non fang.\nFocus on communication skills. Learn how to influence people without authority. Be in the prioritization conversations. Also, keep building your technical skillset. There's so much to learn. Never quit learning.\nThis has been a great read! Thanks so much for doing this. One quick question from me. I have 3 years of experience in the pharma industry (oncology lab and project management), a biology bachelors and masters. I recently switched over to a data focused role mostly debugging and maintaining our data pipeline. I wouldnt know if I would really consider myself to be a data engineer because the pipeline is fairly mature at this stage and there isnt a lot of designing/building going on but anyway! I currently work entirely with MS SQL server and Python with a bit of Azure DevOps/Git for working with our repo - where do you think would be a good place to focus my learning in order to make that leap into a more modern tech stack (potentially at a FAANG)? Dbt? Learn Scala for Spark jobs? Snowflake? Airflow? Maybe do a full end2end DE project and grind leetcode? Thank you!\nI dont recommend Scala to most people since Python is the overwhelming favorite for most DE jobs. Python, dbt, Snowflake/BigQuery, SQL, and Airflow would be the stack I think is most universally applicable\nThank you for the response!\nNot many of FAANG do this, but how do you see geospatial big data in the coming years?\nI do love map data. I think there will always be a strong need for this. Google Maps is a primary example. Im sure theyll be some strong SaaS providers that make it much easier to work with just like everything else in DE :)\nHello! Thanks for doing this. Its been incredibly insightful. 2 questions for you. I nearly overnight went from a developer to a leadership role with a department of 4 (soon to be 5). Do you have any go to resources or advice that really helped you in that transition? Also, Im in a spot where Im working with almost exclusively all SQL (DBT and Snowflake). Looking to expand my knowledge/skill set to include python. Whats the best way to start there? Thanks for your time!\nRemember that people aren't like code. They're emotional and have their own goals. Work with your reports to align on what they want and they'll give you what you want. Learning Python has a lot of options. I would recommend trying out Pandas to do some of the analyses you're currently doing in SQL.\nAsk for formal management training from your company to learn better communication skills! This is so important and many people in your position dont know what they dont know.\nWhat role do you suggest for a person having 15 years plus experience worked mostly on ETL stuff like Informatica, Hive, Python and SQL? Working currently as TPM\nProbably data engineer or analytics engineer. AE is probably a better fit since you have strong soft skills from being a TPM\nThank you so much! Is there any Level that I should aim for considering my YOE?\nAt least L5. Probably L5 or L6.\nAwesome! Thanks so much!!!\nHello, Can you give practical tips and advice on how to become a better software engineer. Tips for junior devs and practical advice on how to become a senior engineer, also if u are free, in practical terms, what makes a senior engineer ? Thank you so much!\nHey Zach, I have been following your content on various platforms for a while now. I came across a question in one of my FAANG interviews and haven't yet found a satisfactory answer to it yet. How do you optimise a daily incremental merge(inserts and updates) when we have a huge target table about 10 TB and we have smaller incremental data coming in daily of 1 GB ? (A usual merge statement would require a scan of the entire 10 TB table. The panel was looking for a solution which didn't need to scan the entire target table)\nIf I say to you that I am building a new distributed data framework (like Spark but 5x faster), which i am trying to do, what boxes do you need to tick before trying it?\nSpark is actually getting more optimized too. I'm very skeptical of a framework like that since almost always the way they pull it off is by caching and essentially trading storage for compute speed. Your pipeline may be 5x faster but how much extra storage are you racking up to get these amazing results? Also, when I see \"benchmarks\" that show these results, I'm very skeptical because they usually pick a very contrived example and across more generalized workloads the performance gains are either not there or much worse.\nAre you using both specialized storage and compute? I don't like vendor lock-in. So I'd want a general purpose compute that can write to whatever storage I want. If you can pull that off and make it 5x faster than Spark, I'd be amazed.\nI am not using anything specialized. Read from S3 (CSV/Parquet), do queries, write back to S3 or convert to Pandas. Everything is open source: [https://github.com/marsupialtail/quokka](https://github.com/marsupialtail/quokka) (will have to write more docs for it to be useful), but I will let you know when I have a dataframe level API ready. I don't cache anything. Your note about generalized workloads is very helpful. I strive to be general, but it's hard for a new framework to get there since you have to implement a lot to match the API of existing frameworks... Main thing is changing Spark's pull-based execution to all push-based like modern database engines.\nAre you using concepts like associativity, commutativity, Semigroups, Monoids,  in your pipeline design work or think in terms of them?\nNot really. The only concept from FRP that I use a lot of is immutability.\nThank you so much for taking the time to do this AMA and being so willing to share your knowledge and help others!!\nThanks for your kind words!\nAs a graduate student in masters in data analytics with an year of experience(unrelated field), I just want to what skills do I need to break into this field and how do I upskill myself? I have got huge student debt so I am aiming for FAANG. (My bachelors are in CS) Thank you in advance!\nSounds like you probably already have alot of the needed skills. Have you tried interviewing at FAANG? If you fail, youll know what you need to learn\nHow I wish, my resume never got pick up ( at least for internship) for full times - I only applied for Amazon but it was never even reviewed!\nThanks for doing this. Had you ever thought about moving to a Customer Engineer/Solutions Architect type role, keeping the focus on data engineering/analytics? Im about to start as an AWS SA and am not sure where that will lead; Im worried Im going to lose some technical skills if I ever want to switch back to a role focused more solely on the engineering/coding side.\nI wouldnt worry too much. This position sounds like a great opportunity to learn about architecture, building complex systems, and about the ever-changing data space, meaning youll probably be more up-to-date on whats going on then most data engineers being siloed at one company. I would just do some coding exercises every day or have a personal project going to keep yourself fresh if youre worried.\nNice, those are great points and I appreciate the advice!\nI dont have enough experience here to give you good advice. Sorry!\nI am a junior level data analyst. My work mainly consist of writing SQL code or sometimes I write python scripts to automate certain task (reading multiple Google Sheets and writing on BigQuery). I have been working with dbt for the past couple of weeks. I love this data engineering field. What should I learn next? Any skills? Any languages? Any concepts should I be well versed on? Is it possible to learn by myself? Thank you.\nData modeling and getting better at Python would be the two things I'd recommend\nHi OP, Finding it difficult to get interviews for DE roles. Situation: Im a sql developer and have 5 years of experience with SQL. Can write complex stored procedures, etc. Have data modeling experience. I also worked with Informatica ETL which is a off-the-shelf product. I recently applied to few DE roles. One of the recruiters asked me if I had experience with Redshift and Airflow. I never heard back from them. Basically all the recruiters are asking for experience in tools like Snowflake, Airflow, Spark streaming, Kafka, etc. Without a DE job how can I get experience on these tools? What skills do I know: Apache Spark, Python, SQL, bash, AWS. I can create a ETL pipleline from scratch using these tools. Currently I am learning dbt and airflow. What I am doing now: I am trying to create one complex ETL pipeline in AWS cloud where Incan showcase dbt, python, etc. What should I do? Am I on the right track? Should I focus on data analyst role and then make a transition? Or keep on applying for DE roles when my portfolio project is ready? Also suggest me a good resource to get data sets for the portfolio projects?\nI think you should keep applying. Youll find a fit. You seem to be on the right track\nWhats the best approach to prepare for leetcode style DE interviews for SQL and Python if say you have just 7-8 days for the interview. Just want to be well prepared in advance in case I get any interviews. Personally I dont do well in these type of coding rounds. I just get nervous :(\nFocus on leetcode easy and medium. They definitely won't be asking a leetcode hard question in a DS interview. Do like... 5 per day (should take you 1-2ish hours). Don't overdo it because you'll psych yourself out.\nSorry I meant DE interviews. Would it be same?\nI've never been asked LC Hard in DE interviews except when I interviewed at Robinhood last year. So yeah, the advice is mostly the same but don't hate me if you get asked one.\nI have a couple of questions if you don't mind. I am the manager of a BI team (more data team than anything but it's my title) and the architect of our warehousing and pipelines (very small business) 1. Have you used or tried Prefect? If so, what would be your preference? 2. Are you running GE validations pre and post processing? 3. How efficient is GE in handling the volume of data you are processing? I have convinced them to swap to Python for ETL work and we are implementing the very things you use, but have always had in the back of mind about the drag from GE. At my old job we used Informatica MetaData Manager it was insanely taxing. Thanks in advance!\nIve heard that faangs purposely try to push you out within a year or 2, is this true in your experience\nI mean. I did 2 years at FB and 2 years at Netflix. I dont know if its on purpose though\ndo you use adderall or amphetamines? do your co-workers use it?\nIm prescribed Adderall by a physician to treat my ADHD, yes.\nSo, you spent 8 hours on interviews in only 1 company? To have interviews at 5 companies it's going to be 30-40h? Where do you get that time? xD Seriously, did you need to do them during your vacations and day-offs or you sneakily had them during your working hours (if you work remotely or something else)?\nI took PTO at the prior startup to interview at FB and I took PTO at FB to interview for Netflix. I wasnt employed when interviewed for Airbnb.\ngive me a referral and share me your tech stack and how to go about learning it\nDM me your LinkedIn and Ill think about it\nThank you so much for this AMA!!! How to learn and be good at data modeling? Do you recommend any books/courses? What kind of development should I focus: TDD, BDD, DDD or EDD? Do certifications matter? Learning Hadoop, hbase and hive still needed?\nS3 and hive metastore are still very used. Hadoop is on its way out although it'll probably stick around for the 5-10 years give how many people have adopted it. Certs only matter in that they give you exposure to valuable technologies. The actual piece of paper is trash. I like TDD the most. Unit testing is so important.\n>Hadoop is on its way out What are people storing their data in if not HDFS?\nI'm a L6 data engineer in FAANG - two main questions: \\- how do you think about comp vs WLB ? Any guidelines for how to think about money now happiness later mindset ? \\- any advice on growing your sphere of influence ? Mainly what i'm asking is if you have any learnings and advice from growing your linkedin network ?\n- Airbnb has amazing WLB. I rarely work more than 35 hours per week. So I feel like I get to have it all working for them. - offer as much free value as you can. Dont worry about monetization. I still havent monetized my LinkedIn even after spending 10-15 hours per week on it for 1 1/2 years. Give as much value as you away for free. It makes people trust you because they recognize that your intentions are pure.\n\\- I'm actually similar on the WLB front now - but worried about how that will change as I progress from L5 -> L6. I took a side job consulting when my hours were around 20/week. \\- Thanks - that makes sense. I'll see you on Linkedin :)\nWhats your opinion on Analytics Engineering role?\nI hope this gets answered too, since I will start in this role with June 1st.\nLets go ZACH!\nThanks Nick! Glad you found me :) This is my very very first post ever on Reddit!\nWow, what a pleasant surprise to see you here Zach! In your lovely podcast with Maxime Beauchemin, you guys discuss how DE's are being squeezed in by `Analytic Engineers` (AE) on one side, and `FiveTran-like` extraction software on the other. To stay relevant, you mentioned one possibility is getting more into the Software Engineering side, i.e `Software Engineer - Data` roles. Could you go into more detail on the best way to pivot towards the SE side, especially if they're currently more on the AE side?\nI personally really like project-based learning. Like, for example, [https://www.zachwilson.tech](https://www.zachwilson.tech) is my personal website. I coded 100% from scratch. Building your own personal website from scratch is a nice way to learn all the skills like servers, REST APIs, etc. To get to Software Engineer - Data, you really need to know backend development fundamentals around concurrency, race conditions, database indices, and stuff like that. So building a project that leverages your AE skills to scrape and ingest a bunch of data into a database and then putting a REST API on top of it is probably a pretty solid way to learn the skills necessary to pivot into this role. That's what I did at least to transition from DE to Software Engineer - data back in 2018.\nThanks for providing a concrete example, that's very helpful.\nYeah. check out https://www.zachwilson.tech/graphs There I have an example of a stock market network based on cooccurences in the news.\nMind to mention the name of the podcast?\nAnalytics Everywhere https://podcasts.apple.com/us/podcast/episode-3-building-data-products-with-zach-wilson/id1612532253?i=1000558179577\nthanks a lot\nWhat does great documentation look like for a master DE and how might that change across orgs? Any examples to point us to? How much does documentation help in the long run?\nGood documentation should be split into a couple layers: \\- Table-level docs. You should document what every column means and does \\- Model-level docs. You should document how the tables interact with one another \\- Pipeline-level docs. You should document the data flow for your pipeline\nI study data science and because I love hardware I want to be consulting other firms how to get an IT infrastructe to gather data. What do you think?\nThat sounds like a really awesome career path. Follow your passions and you won't go wrong!\nWell. How difficult do you find to archieve that? Any tipps? Why are people so focused on FAANG when doing it on your own can you get even more money?\nThe top of the ladder at FAANG is hard to beat. L7 engineers at most FAANG companies make $800k to $1m/year and they dont even manage a team.\nYou think that with their skills they could also create an own business?\nPython, SQL, CRON, and probably either Snowflake or BigQuery\n[removed]\nI don't know how to answer this question in a way that makes sense. It really depends on the context of the problem and one \"unified algorithm\" for this doesn't make sense.\nThanks for this AMA and it is very insightful\nNo problem. I'm glad I could help!\nI started my very first DE job and it's been close to 6 months now. My day-to-day is pretty much build pipelines to get data from point A to point B (Snowflake tables, where analysts use our in-house BI tool to connect to those tables and create dashboards). Someone mentioned this in another comment, but can you explain the difference between analytics vs. software engineering focused data engineering? I'm assuming my work is analytics focused, but I'd like to know what my job responsibilities could look like as I move up the ladder in an analytics DE role. I do enjoy my work, but I'm not sure if I want to keep building pipelines for dashboards in the long run.\nOne of my favorite ways to distinguish the two is: Do you pipelines yell when something breaks in a place that isnt production? If so, youre doing software engineering. If not, its probably analytics engineering.\nI'm going to offer a different perspective on the difference between these roles. What is the thing you care about most? If it's building a system that works well and is consumed by others, then you're probably a software engineer. If it's understanding and influencing something in the product/real world, then you're probably an analytics engineer.\nThis is more of a question for your team leader hat. In your teams, how long of a runway do you give new hires for getting up to speed and be productive?\nThey should ship their first bit of code in their first 2 weeks. They should deliver their first major project in the first 6 months.\nThanks!\nI'm currently working in SEA companies with a job position that allows me to work on business development, product development and exposure to data engineering projects. I wonder if that would help me stand out in FAANG companies as I slowly build up my portfolios when I migrate out of SEA in the long run? I also hold a masters in big data and I'm contemplating if a PhD is necessary to keep up with the FAANG companies. I do however want to catch a break from all this studying haha.\nI only have a bachelors degree but most of my colleagues have masters. Not as many PhDs in data eng as in data science though. I think your strategy definitely could work. Building a portfolio is something I highly recommend\nThank you so much for the pat on the back. I'll try my best!\ndid you ever support any machine learning initiatives? were they business critical? did they actually deliver value?\nI've supported machine learning initiatives in every single role I've been in since 2016. They were business-critical in almost all the cases except when I was working on some experimental ML at Netflix. They delivered massive value to the tune of millions of dollars per year.\nawesome. do you think it worked well due to the large amounts of data these companies generate? ive seen it used in the non-tech corporate world and the results are more or less dismal at best.\nI think its more the abundance of skills than the abundance of data. FAANG knows when and how to apply ML to get an ROI. its much less about data volume\nAt FB, it was ML related to optimizing notifications. At Netflix, it was ML related to detecting security threats.\nYou mentioned growing into a team lead/manager role. I have a few management oriented questions. What technology stacks do you think provide the best return for time invested / effort to implement and maintain? What internal dev team processes or norms do you find to help your team be the most productive? What would you focus on and invest in if you were at a small start-up with limited resources and had to be responsible for everything related to data engineering rather than a large organization where you are responsible for just a small piece of the system?\nIs it possible to break into faang as a career changer?\nPotentially. Itll be harder but Ive seen it done. You need a stellar portfolio and skills though\nI've been following you for quite some time on LinkedIn - surprised it has taken you this long to get onto Reddit! :D If you don't mind me asking, how old are you/age bracket? Your creds are aspiring.\nIm 28 :)\nGood lord, you're just a child. Flipping heck mate, some career you've made, well done.\nYou say \"big data\". How big are we talking? I've seen people use that to refer to terabytes, petabytes and exabytes.\nExabyte is a lot. Like. All of FBs data together is like.. 2 EB\nI managed a 20+ PB warehouse at FB and a 10+ PB warehouse at Netflix.\nnice! I should have also said that I've heard people use big data to refer to a few gigabytes, but I guess it's a constantly shifting label based on current storage capacities and the compute ability of a desktop PC.\nHi u/eczachly, thank you for the AMA. I have found very few resources beyond the the main books (kimball's) that go over star schema examples (and how to actually create them in code). **unlucky / lucky for me, my first star schema, is a weird one. I am wondering if you can assist?** I am trying to model zendesk data. I did find participial answer online: [https://rivery.io/kits/zendesk/](https://rivery.io/kits/zendesk/) First, question. **Why would they have (User details Email & name) in the fact table?** and not in the user table. Second question, Info: there are two tables missing from the example above which is where I am having my main problem. [Here](https://imgur.com/ofJ6EbW) (Power BI basic model) is what I get out of their API. Ticket table works almost fine as a fact table.. except for Ticket Comments and Ticket Audits. These have the opposite relationship. **What would the fact table be? or is there more then one..** My current thoughts would be to have a \"user action table\" as a the fact table. A user can create create a ticket, an audit or a comment. Each would have a creation date.. but the ids don't make sense. &#x200B; Thank you so much if you have the time to assit\nI'm not answering specific work-related questions like this. Sorry! I'm not your data modeling monkey!\nFare enough, thanks for replying.\nIf my current role doesn't lend well to catching the eye of recruiters for stepping up to a DE role, would you recommend focusing my time on building out my GitHub portfolio or studying for a cert (e.g. - Microsoft Certified: Azure Data Engineer Associate -- studying available via Pluralsight from my current company) to get my foot in the door? Thanks so much for your time and effort here in sharing your experiences and wisdom!\nHey, awesome work from your side, just subbed and will be following you for content! Questions from me as a DA that has 3 years experience and transitioned into a DE role the past year 1) what was your most stand out personal project like? 2) is it worth making a blog this early in my career and just sharing interesting thing I find in my own way (with credits given of course!) 3) Whats one piece of random advice you would give someone at my stage? Thanks so much!\n1. You should do a personal project that you are actually passionate about. If I see someone's eyes light up when they're talking about their project, that's a HUGE plus. 2. Yes. Writing early and often will improve your communication skills and your personal brand. I know people on LinkedIn who are in HIGH SCHOOL who have 20,000 followers. Your ideas are valuable. Your perspective is valuable! 3. Remember to breathe. Career transitions are really goddamn exciting! Remember to stay open-minded and keep learning. It's so so important!\nthank you for this well thought out response!\nIs Spark used a lot by Data Engineers in the product analytics team at Facebook?\nFacebook is heavily invested in Presto/Trino. Spark is only used for the heaviest pipelines.\nDid most of your spark knowledge come from studying in your own time or at a different company?\njust started as DE and saved this post, looking forward to reading it :)\nThis may have already been asked sorry but Ive seen youve only been using Scala since 2018. I wanted to ask your thoughts on using Scala over Python for Data Engineering. What was the reason for transitioning over to Scala? Was this for job opportunities?\nNetflix and Airbnb use Scala A LOT. Airbnb uses it exclusively and Netflix uses it for 65ish% of their pipelines\n[removed]\nPySpark\nIs it possible for anyone to go from L3 to L6? Ive heard that in Facebook its incredibly tough to cross the E5 mark and very few do it\nLess than 3% of engineers get above L5 youre 100% right\nWoah thats crazy. Just a quick follow up is this statistic specific to netkifx or all FAANG?\nThats a pretty solid stat across FAANG. Theres a solid reason why they call L5 a terminal level.\nDamn, crazy. Appreciate the info!\nIt's worth also saying that L5 at FAANG is a very good living. Here in the UK, it'll put you in the top 1% of earners by income in the country (as total compensation).\nSame here in the US! You wont be too 1% but veryy close\nWhat does the pay bump look like from L3 to L4? I'm starting as IC3 in a few weeks (I assume that is similar to L3) And how long does it take to level up to IC4/L4? My IC3 offer is for about 165k TC fully remote in MCOL area. 1 YoE. Curious what I can expect at IC4 and when that will be.\nI posted my comp journey in this thread. I went from like $190 to $245 at FB when I went from L3 to L4\nOh ok interesting since FB doesn't have L levels. Did you mean to say IC3 to IC4? And how long did 3 to 4 take?\nI use L since thats universal across companies. I went from L3 to L4 in a year. And L4 to L5 at Netflix in a year. And L5 to L6 equivalent (Netflix doesnt have levels but your comp is a good proxy) at Netflix in a year.\nI was incredibly laser focused on this from 2016 to 2019 though so YMMV. the expectations usually are L3 -> L4 in 2 years. L4 -> L5 in 2 years and L5 -> L6 in 3-5 years.\nThank you!!\nWhat's the difference between your job and data scientists\nData scientists focus more on modeling, analysis, and experimentation. Data engineers focus more on delivering high-quality data and building infrastructure to enable quick analysis.\nSo the output of the infrastructure you build is then received by the data scientists? Is data engineering more coding focused while data scientists more code/statistics?\nNailed it!\nThanks for your answer!\nRegarding mentioning skills on a resume: Is it okay to mention a skill like Redshift on our resume if we havent worked on it in our current jobs but have learnt it? If. not then what types of personal projects based on Redshift do you recommend?\nOnly put technologies and languages on your resume you feel comfortable having a short discussion about.\n!remindme 1 day\nI will be messaging you in 1 day on [**2022-04-29 16:33:14 UTC**](http://www.wolframalpha.com/input/?i=2022-04-29%2016:33:14%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/dataengineering/comments/udboyq/ive_been_a_big_data_engineer_since_2015_ive/i6jv8u3/?context=3) [**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fdataengineering%2Fcomments%2Fudboyq%2Five_been_a_big_data_engineer_since_2015_ive%2Fi6jv8u3%2F%5D%0A%0ARemindMe%21%202022-04-29%2016%3A33%3A14%20UTC) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%20udboyq) ***** |[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)| |-|-|-|-|\nWhat is your TC?\n$575k was my original TC at Airbnb. With stock drop its closer to $515k now\nSorry if I missed this earlier, but how much of this 575k is stock and how much is monthly salary in hand? (if you're comfortable sharing)\nSplit 50/50\nHerein lies the answer to everyone's question. No, no one in this industry makes cash-in-hand salary like that. Just pumpin this post. Take everything you hear with a grain of salt.\nWhat is your thought about the data is an afterthought problem for most companies that dont have big data and cant generate enough analytical value from data? For example, at startup, the focus is almost always on building product itself and not on data, unless its a product that involves analytics/ML. Does limit on good DE opportunities to those sizable companies?\nGetting logging right is what startups need to focus on. The pipelines can come later\nWhats the difference between data engineering and analytics engineering?\nDE is focused more on building master data. AE is focused more on building analytics and experimentation.\nI had an hour on data modeling. When would I choose a graph database vs Hive vs a relational database? How would I model my tables for efficient querying? --> Do you have any advice on how to learn this part as a college student? I don't have any real world experience yet and only did some data modeling to 3NF form\nThis was a senior data engineer interview. You'll learn most of it through experience.\nWhat's the project you're most of proud of working on?\nBuilding out Asset Inventory for Netflixs InfoSec org. An hourly updated graph database of their entire infrastructure and how each piece is connected. Stuff like: Apps talking to apps Employees accessing data tables Code bases that build into apps Vulnerabilities in code bases Apps talking to databases Best practices adopted by apps It was the most complex and valuable thing I ever built. It was a beautiful mix of data engineering, backend engineering, and frontend engineering.\nNice, sounds extremely cool mate.\nCan you talk about your L3->L6 journey in terms of game changers that made you better, lessons learned, mistakes that you think wasted time (if any)?\nI didn't waste much time. I did L3 to L4 in 1 year. I did L4 to L5 in 1 year. I did L5 to L6 in 1 year. This was a mistake I feel in some respects because this hyperfocus on career ended up being really stressful. The biggest game changer was finding a good mentor. My first manager at Facebook was a VERY good mentor for me. He was so good that I followed him to Netflix and he was my manager there too!\nThis makes a lot of sense, I am on my 4th job, first time I'm not in a micromanager or survival of the fittest situation. Good mentorship seems to be rare, as I'm only getting it now, thought everything was fine at the time, but now being in a harmonious team I see there were problems.\nIt is so rare. Thats why I had to chase my good mentor haha\nThanks a lot OP for AMA. Very insightful. Im wondering what made you pick Airbnb after NFLX & not chase mentor to MW.\nLiterally the highest praise right there. Totally get that.\nThanks a lot, I really appreciate your work! I joined a startup and there are only SWEs, no data people. The data sourcing runs on JavaScript (I only know Python) and is implemented in the backend repo. I have a really hard time in maintaining the code. Would you consider rewriting the code? Or do you have any other tips on how to behave? I feel pressured because I'm still in my probation time and feel super unproductive.\nCould you learn enough JavaScript to maintain it? JavaScript is a great programming language. Dont fall into the trap of oh I dont know something therefore its bad\nThank you Zach! Currently I'm trying to but the architecture behind it seems super complicated and there is almost no documentation, so it is a very slow process for me. Imposter Syndrome is kicking like never before Would you consider building up new features with Python or stick to JS as we already started with that? Never worked in an environment with multiple programming languages. I just have no strategy on how to continue\nHey, Zach! Thank you for sharing your knowledge and insights. One question: how many hours did you work per day on average during your career so far? (Approximately)\nAt FB, it was close to 10.5. At Netflix, it was close to 10. At Airbnb, it's close to 7.\nHow do you grow from senior to the next level? Team/tech lead / staff? How does one effectively increase the scope of their impact? How do you grow into a more senior mindset and what are the things you should care about as a senior vs the things you should care about at the next level? These questions are more generally applied but if you answer them within the scope of data engineering, I can make my connections between what you do and what I do!\nAs a Junior in college studying Information Systems hoping to enter the data engineering field, how important is grinding LeetCode? Is it something I should dedicate a lot of time to or not necessary for this field?\nHey mate, I might be starting a role in which they have pipelines existing in SQL which I'm worried about. My SQL level feels like a cliff where if I step over it I'm just clueless as to what's going on (I struggle to breakdown SQL queries and SP's unlike python) Any suggestions on what I can do to improve? Thanks!\nNot OP, but the path I took was make a simple couple relational tables like: Orders, Customers and Products. Create the PK's/FK's for each scheme. You should be able to find a good amount of example scheme's online. Focus on understanding inner joins, outer left joins first and then move on to general ORDER BY, GROUP BY, Aggregation statements etc. When you write the SQL you get instant gratification -- this will help build the understanding.\nThanks for the response. I can already do most of what you are explaining. My issue comes to aggregation statements in combination with subquries. What I am worried about with this new role is there ETL jobs being written in SQL. If the code is simple SQL like joins and some grouping I wont have a problem with it. Issue being is some queries just get too complicated to me and I cant break them down (merge statements, multiple subqueries etc). Ill try do some reading of ETL jobs in SQL and go from there. Thanks!\nsubqueries aren't too bad, just remember they are like new tables being generated\nJust came to say you're the shit! Thank you for everything you do for the community\nI don't know shit, have a business degree, and IT internship at a small family-run business, and have been studying python for a while. How do I even start learning to be able to apply at FAANG? Probably don't even get any sort of interview or callback if you aren't a CS major, right? What about for someone who hasn't graduated but they have numerous credentials and experience?\nWhat do you think about Data Mesh? Would you say that FAANG are moving to Data Mesh direction?\nHonestly I think data mesh is mostly hype. But well see what happens I suppose\nIt isnt all hype. although i dont work at FAANG , my company started building data products and using data mesh approach. It helps create value and break down silos.\nUsed to be called data fabric.\nI still haven't grasped the hype term meaning, to me it is just multiple /federated systems of record, sometimes well connected and other times not.\nHow did you prepare for your faangs interviews? Any recommendations?\nI'm a bit weaker at DSA so I usually do 10-20 Leetcode mediums to prepare since data engineers are very rarely asked leetcode hards. Besides that I feel pretty confident in the rest of my skillset to destroy any other question that is asked to me.\nHow do I get started as a Big Data Engineer? Even for a regular DE, the barrier for entry is too high.\nYou mentioned Great Expectations for data quality in one of the comments. Can you shed some light on custom validations and/or other tools used for ensure data quality?\nWhats the best tool for data quality? I see you mentioned great expectations and dbt. Are these used in FAANG?\nEvery FAANG I worked at uses its own data quality tools built internally. GE and DBT are the best solutions I've seen from the open-source side.\nWe are planning to monitor freshness, volume, schema changes, data distribution and data lineage. Can these all be done using dbt or GE?\nOnly volume, distribution, and schema changes. The rest arent data quality, theyre data system quality.\nWorking as a BI developer, best skills/technologies to learn for transitioning into a DS/DE role?\nWhat was your main method of enhancing your sql and coding skills? What skills do you think are most important for your field?\nSide projects! Check out my GitHub for examples. https://www.GitHub.com/EcZachly\nOooooohhh yes exactly what I wanted, I love u\nWhat advice would you give to a full stack developer looking to break into more data engineering roles? I have experience building scalable application backends, apis and frontend UIs.\nGet really good at SQL! Learn about CRON jobs too\nI have decent knowledge of SQL when it comes to fetching data. Ive been using ORMs to code out my models and build my databases in the past so I can definitely improve my SQL for creation of table schemas. Im familiar with airflow and cron jobs. How much API development does your team do?\nIve really enjoyed reading your replies in this thread! Im starting to applying to the tech sector with about 6 years experience in engineering companies, of which Ive always been more of a satellite analyst/DS on teams dedicated to a variety of business needs. Ive had to become a bit savvy with DE and general SWE, but have felt a bit crippled by the inability to leverage other data technical coworkers. Are there any good ways to boost conversational technical communication to others who really know their stuff? I read and take classes almost constantly, but I feel like when I talk Im speaking a different language even if I know what Im doing. Combine that with being more of a data generalist and I feel like I always come across as trust me Im street smart. My only solution so far is to code up a project and put it on GitHub to show my literacy, but I feel like that is going to extend my job search time frame substantially.\nAny resources you recommended for interview prep?\nHi Zach. Do you have any cloud/product specific certifications? Are they actually useful in climbing the corporate ladder or getting that first job?\nI got the Hortonworks Hadoop cert back in 2015. Thats all I ever got though\nAny career advice for a jr DE working with a single senior DE at a series b startup? We use AWS, Databricks and PySpark.\nHey can you guide on preparation in data engineering?\nZach , I have 10 years exp and working in India Tech stack - spark python scala , entire infra on aws , k8s , airflow Doing some POC on kafka and spark streaming I am looking for other role Can you guide me what to focus for 10 yrs exp Can you suggest me preparation plan What to.focus more on ?\nHow would you recommend learning python and sql to a good enough level to build a portfolio and get a junior engineering position? Atm my skills are quite basic.\nWhat's your advice to learn data modelling? are there any good resources? I am a junior data engineer with very little experience with that and I think FAANG requires strong knowledge of data modelling. hope to get some good resources for me to learn.\nWhat is your favourite tool to work with?\nI'm sure you've answered this already, but I'll give it a go. Currently a Data Analyst using PowerBI/Excel, and utilized SQL at a previous company being an Operations Analyst. Took the dbt fundamentals and subsequent courses provided by them, and learning Snowflake. Advice on switching to an Analytics Engineering role or even junior DE?\nAny resources you would recommend for learning data architecture?\nHey! Someone may have asked this already, but how can you upskill in the DE space without incurring tons of costs? I come from a SWE background and am interested in upskilling, but Im a bit overwhelmed by how Id go about simulating data streams, using a bunch of free tiers of products, etc. Im afraid that Id have to pay to learn in the DE space.\nHey Zach, can you explain the differences between lambda and kappa architectures and pros and cons? I am struggling to understand them. \"Designing Cloud Data Platforms\" suggests a 6 layer architecture that is similar to lambda, for most use cases: https://freecontent.manning.com/wp-content/uploads/the-layers-of-a-cloud-data-platform_02.png The book says this is the same as lambda except that in lambda, data isn't split into batch and streaming like this and that the same data actually goes through both pipelines. Is this true?\nwhat code style do you use for spark ?\nDoes sales force etc store data themselves or does the data remain in a database which is simply read by these apps?\nGood question! Salesforce as an app has it's own Database Typically businesses have their own Database, and will pull data from Salesforce API and pushed to their database.\nThankyou sir! Very helpful :)\nHi u/eczachly \\- awesome AMA. Was wondering: How does a typical day look like at Netflix or AirBnb? Related: What distinguishes a mediocre from a great day?\nWhat ur TC?\nI accepted $575k at Airbnb. Its a bit lower since the crash though\nOh i saw a post about u ur famous zach the data engineer cool! So my question to u is, is there a fast track to at least skip the 50 to 100k range and jump into 6 figures ? I graduate BS CS in a few months and have a few software engineering iternships (4) but theres like no one in data engineering compared to software engineering and i love sql and python and im getting AWS/Azure certified this summer. What should i do to get my first awesome role in data engineering? If i dont make it on ur airbnb team first of course ;)\nHey OP, Id really appreciate getting some insight into how one can transition into data engineering? Im currently a trainee accountant by trade and struggling to see it being long term as I would like to get involved into more tech heavy role and came across data engineering as a possible route. Does data engineering require a lot of mathematics in day to day tasks? What does a typical day as an data engineer look like and what are career/earning prospects like? Id really appreciate any input/feedback you may have and guidance on how to get started! :) Thanks\nWould be willing to have a chat ? I'm a data Engineer but I would like to raise my level. A feedback or help to grow from expert would be greatly appreciated.\nHey Zach, thanks for this amazing AMA. Could you give me a guidance for data engineering TC? Compared to median salary in USA ($61,417, [source](https://www.census.gov/library/publications/2021/demo/p60-273.html)), your TC is USD 575k/year when you were working at Airbnb, which is around \\~9.4x higher. What's the good salary range in USA for junior/intermediate/senor data engineer working at Tier 1, 2, and 3 tech/non-tech company? &#x200B; Definition that I use: Tier 1 means a local company, usually serves one or more cities Tier 2 means company with nationwide market Tier 3 means multinational company/do business in several countries",
        "content_hash": "9ecae24104d5592b6e348521df1c934b"
    },
    {
        "id": "1fs80oq",
        "title": "My job hunt journey for remote data engineering roles (Europe)",
        "description": "",
        "score": 582,
        "upvotes": 582,
        "downvotes": 0,
        "tag": "Career",
        "num_comments": 138,
        "permalink": "/r/dataengineering/comments/1fs80oq/my_job_hunt_journey_for_remote_data_engineering/",
        "created": 1727626444.0,
        "comments": "Damn, 4 offers from 135 applications? Is the market in Europe that good?\nI've seen a lot of European job postings lately, ones that likely would have been US-based a few years ago. Some companies (who are equipped to do so) are wising up and realizing they don't need a $200k engineer in NYC when they can have a $60k engineer in Poland.\n>when they can have a $60k engineer in Poland I think the market is fixing that, quickly. There aren't any \"cheap\" countries for quality data engineers any more. You can pay peanuts to get monkeys anywhere though...\nSince Poland was given as an example - $60k in Poland, which would be about 14500PLN net monthly on b2b contract is a comfortable pay, above national average. Its not enough to feel rich in Warsaw, which is a capital, but since most of the companies are present in any other major cities (were not Warsaw centric like UK), its a very nice salary for a comfortable life. Make it 80k, which is still lower than what youd have to pay in the US, and youre gonna choose between most talented folks from universities. Chat GPT-4o leadership contributors include 5 Poles, and if I remember correctly when I checked them, 3 of them studied at University of Warsaw. @edit - clarified that 14500pln was a monthly figure\nI'm in the UK, wish I could get a job for any company registered in the EU :(\nYou can pay peanuts to get monkeys anywhere \nActually, in Italy a senior DE salary is around 50k and the situation isn't going to change anytime soon, moreover I don't see any lack of quality since I've seen many applying (and succeeding) for jobs both in EU and US\nI dunno, sounds more like an issue with that particular coworker instead of a systemic or culture issue.\nI hear this about international coworkers from a lot of friends. A friend of mine literally put in notice due to managing and fixing issues from an all international team that is supposed to save money. It is a company issue from bad business decisions.\nThese are minor issues that can be overcome\nCan I dm you pls?\nIf they are going to take 3 months off including a massive summer break then they still cost 90k before factoring in coverage. Its a 150k employee. Just hire someone in colorado. You can get someone with 2 weeks off per year for 100 to 110k mid-tier 120 to 150k senior which, judging by epam is still cheaper. Then there is the no work after 9 hours thing.\nCurrently working in Europe after working in the US 10 years. Its more like 25 days off versus 15, which is still nice, but nowhere near your estimates\nNobody gets a 3 months summer break lol. Number of vacation days is fixed by national laws and usually somewhere around 25\nIf it's about the same labor cost even after you account for the social benefits, I'd rather hire the guy that is better rested and has better work life balance. He is more likely to stay in company, be more productive and to be more pleasant to work with\ntbh I think Europe doesn't offer as much during the good times. But the conservative approach means that the market is less volatile compared to the US one. Take that with a grain of salt - that's just my personal feeling without data to back that up.\nI am completely foreign to the hundreds of application thing from the USA. Since when I had a few years of experience, either I was poached or I did maximum 6 application processes to get a job (and this one was \"so many\" because I wanted to move to a new continent).\nHonestly, I think its just a fluke.\nbut you really nailed the interviews. I've had a few technicals and they were really picky and had some rejections in that space\nAgreed. I feel like the bar for passing the final round and converting that into an offer has gone up in the past 5-8 years.\n4 offers is definitely not a fluke. You probably killed it during your interviews.\nThanks man!\nWhat is your particular kind of DE? DataOps? Cloud?\nIm desperately looking for local data engineers in Belguim and cant find them so yes\nI'd move to Belgium if I had a job - I am a data engineer in the US\nDid you target big companies or companies of any size? 4/135 is not a bad number!\nMostly start-ups/scale-ups, but definitely some big companies too.\nPlease give some context. Yoe, tech stack, roles you apply for, etc\nSure! Stack is mostly Python, Spark, SQL, AWS, Airflow, Databricks/Snowflake. Almost all \\[Senior\\] Data Engineer roles, remote only (mostly in the Netherlands and Germany). 5+ years experience in data roles (primarily as a data engineer). Only did a 3 year bachelors.\nDid you have many offers rejected because of the language barrier?\nNope. I only applied for companies working in English.\nOkay, thanks \nCould you please share the offers' salary ranges? I have similar background (Microsoft focused), experience, and mainly interested in same countries /English - speaking positions.\nCheck out glassdoor. It provides quite reliable salary ranges per role for data engineers.\nFor Europe Id recommend https://techpays.eu as well as glassdoor.\nThe \"remote only\" filter might be one of the major causes of having such a low offer/apply ratio, as well as company working in English. I'm in a different EU country and a completely different zone, but looking for such requisites excludes something on the order of 90% of the hiring companies (at least in my country).\nYou're from India?\nNope. South Africa baby.\nWow, nice job man! Were the offers with visa sponsorship or you stay in South Africa?\nI love the visual, reminds me of the Napoleon's March chart.\nIts a Sankey plot and its use for job searches became all the rage on Reddit since a few years ago, especially on the data science sub (which is largely trash now). I like seeing them, gives us a sense of the market and how difficult the job search process is (luckily until now havent had to deal with it too much). However, fairly different from the napoleon march plot, which is a work of pure genius. The author cleverly compressed multiple dimensions onto a 2D surface (time, space, attrition, etc). Fivethirtyeight did one with Star Wars, you might find it interesting.\nAny info on the sorts of technical assessments? Leetcode, coding projects?\nMix between take home assignments, live coding, and data modelling exercises/scenarios.\nCan you share the resources you used for the data modeling, live coding and assignments?\nThe modeling and live coding was both just screen-share, no fancy tools. The assignments were also company resources (not using an online tool), so nothing to really share, sorry. If you want to practice/prep, do a few side projects where you experiment with different data tools.\nMan seeing your numbers, I havent been applying to enough places apparently. This gave me a bit of a perspective. Also, congratulations! Edit: grammar\nThanks! I think this is specifically a challenge for remote roles. So many people applying, it takes a while to get through.\nWhat job listing site are you using? From Italy if I search alla the data engineering remote roles there are only a bunch of proposition (like less than 10)\nGreat question! Mostly these: - Glassdoor - Wellfound - Y-Combinator - Otta - LinkedIn\nthis is the question I wanted to ask. Thank you for sharing\nYou may not have a job, but have my upvote in exchange for your lovely Sankey.\nI mean I currently have a job, and I have a few offers, but glad you enjoy the Sankey! \nwhere did you keep the data used to generate the Sankey?\nI just tracked my applications in a spreadsheet. Then used an online tool to generate it.\nIs that a decomposition tree?\nsankey chart\nDid you get withdrawal confirmations or did they just ghost you for a while until the auto \"job has been filled\"?\nI got confirmation from all four. Nice people..\nMaybe Im misunderstanding what Withdrew means, but it sounds like a rejection, not you withdrawing aka canceling your application. Whats the difference between rejection and withdrawal?\nSure! Rejection is when they dont want to move forward, withdrew is when I dont want to move forward.\nOkay, that is what I was thinking. If you can share, what were the reasons for withdrawing after the first technical?\nI would like to know more about the process of interviews. Specifically Im interested in Data modeling and leetcode. Also, out of 135 how many were complete remote?\nIn my case none were leetcode, literally just screen-share and live coding, or take home assessments. Data modeling exercises can be business cases, e.g. we want to build a logistics management platform, identify the key entities, do some entity mapping etc... Or they show a faulty data model and ask you to fix it. They just want to see if you have knowledge of data concepts. All were remote jobs.\nThats cool. Im planning to start hunting for a job in Europe, so interested in knowing the processes. Thank for your reply. According to you spending an hour daily on practicing DSA(basically leetcoding) is worth? Also I have never worked on data modeling, I know the basics and now thinking to improve my skills based on resources available on internet, any resources do you suggest? Other data warehouse toolkit?\nPersonally, I've never been a fan of improving skills using tools such as Leetcode. Imho it's not what the actual work looks like. Writing a Python function to e.g. \"sum all positive integers in a list\" doesn't show DE skills (rather basic Python skills). It's more important to know when to use what data tool, what model to use, how to structure for performance etc.. so I prefer to up-skill through mini/side projects (better yet, try to do consulting for small businesses and get paid for it).\nMake sense, Its been quite a while not interviewing, while researching about the interview process, I saw leetcoding a lot. Thanks for your responses\nIm a grad student with a masters degree in the uk and is there a possibility of applying for jobs beyond the uk but still work remote in eu? My stack is the same as yours . Any would be appreciated\nI live in the EU (resident not citizen), and have no idea how UK employment works for EU companies, but it should be possible through EOR/freelance agreements, but dont ask me ;).\nThank you . When you mean remote it means fully remote right ?\nYes. Like live in a different country remote. Very likely a good percentage of the jobs I applied for didnt make this distinction in the jd, but then I apply anyways.\nThank you . Appreciate it . Ive had a tough time in the uk and I might start applying elsewhere. Branching more towards analytical engineering roles\nAny decent offers?\nYeah :) 3 good ones, 1 average. The 3 good ones are 2 scale-ups and a start-up. The average one is a big corp.\nScale up? Thats new for me \nCongratulations, do they require you to have some level of communication in their language? Or English is ok?\nEnglish is fine for the ones I applied to. There are many jobs with language requirements though.\nWere all technical assessments similar? What kind of things did they ask? Zero experience with interviews, I'm curious\nSome were, some were not. Assessments included take home assessments (e.g. build an application that ingest user data from an api and build a basic etl, justify and document your choices of technology). Also reviewing data infrastructure or models and find identify performance issues and how to fix it. Also specific coding questions (e.g. build a python function that does xyz).\nI had this already in 2011/2012. It was not funny.\nBut I also see you're not European...yeah... probably some problems with that. Especially in Germany you have a lot of rejections if you're not European.\nYeah I think it contributes to the challenge. Though I think it gets a bit easier each time.\nNo came in fluffer reference :/\nGreat job man! Can you share the sources where you found all of the job postings?\nThanks! It is somewhere in the comments, but here they are again, mostly I used: - Glassdoor - Otta - WellFound - Y-Combinator - LinkedIn\n4/4 on final round interviews I to offers. Killin it!\nHow long have you been applying? Did you apply to every remote role or did you carefully read every job desc?\nAround 6 weeks, though all 4 offers happened within the last 3 weeks. For remote jobs I'm less picky, because there's so much competition, but I always/mostly make sure there is a good growth-contribute ratio (enough things I know to contribute, enough things I don't know so I can grow).\nCongrat! What factors do you consider when making the final decision for the company you will be working with?\nThis has been very difficult, I've definitely been suffering from \"analysis paralysis\". Unfortunate, there's no objective \"correct\" answer, it really depends on what you want. I've ruled out 3 offers based on the following: - more interesting products/companies - tech stack & more \"true\" forms of DE (sometimes orgs just slap DE on anything because there's \"data\") - better offers - balanced contribute-growth ratio - trusting my instinct for red/orange flags - start-ups/scale-ups over big corps The final decision I literally made an hour ago was for a start-up (2nd best package), the people seem great and I'll have a lot of impact.\nThank you for sharing, all the best in your new role\nI guess remote only is the problem here. Not many companies these days offer full remote work.\n100%. Also the amount of people applying makes it difficult to get an initial interview. Luckily once you get past it, if you have relevant experience, it [can] go very well.\nAny tips your post has been inspiring. Could you share any personel experiences with cv updates that led to positive results in your job search?\nYes! I've been part of the hiring process for DEs, so I have quite a few: - only one pager cv - don't rate your own skills (e.g. Python = 4/5) - don't add arbitrary stats (e.g. \"I reduced manual work by 15%\") - don't list your tech stack separately, rather work it into your experience - \\[ESPECIALLY JUNIORS\\], don't add all the tools you have worked on for 1h. They don't buy it. (I've seen people with 1.5 years of experience that \"knows\" AWS, Azure, GCP and Python, Java and Scala). Perhaps most importantly, people generally know whether they're going to consider you within 30 seconds of looking at your cv. So make sure the important stuff is reflected in your most recent (top of page) experience.\nWhere are you living currently? because I noticed it also depends even the job completely remote.\nI live in Portugal (resident not citizen) with a remote working visa. So I applied for companies in basically any EU country except Portugal. Generally Germany and the Netherlands had the most options (I think).\nThats cool data engineers are already in demand I guess compare to other tech roles.\nExperienced or fresher?\n5+ years in data (7+ coding in total). Challenge here was remote only roles.\nHi, i am also in the field and I am curious to hear about the salary offers for those positions. I know someone asked this already, but glassdoor represents just the average. And I know that there is a big variance for Senior DE (Remote) positions in those countries (65k-150k I'd say). When I was looking around a year ago, it was not so easy to find something in the higher range remote only. source: my experience from job history, offers and friends in fhe field.\nI would have another look, the range on Glassdoor for Germany (Berlin) DE salaries is between 61k-80k (which is fairly accurate).\ni don't know much about Berlin, but I know the dutch market quite well, especially Amsterdam and the point I wanted to make is that that range 60-80k doesn't represent a full picture. I mean, 60k is definitely close to the real minimum for a senior position, but 80 is nowhere close to many high end salaries. you can have a look at techpays.eu\nYou mean 150 000 as a SALARIED DE in the Netherlands? That seems absurdly high. Freelance sure, but 12 000 monthly on a salary for 5 years of experience doesn't happen\nIt is like that. Fin tech/banks/big tech pay like that (not all ofc, but most). The challenge is finding a remote one. 150k is pretty rare but 100-120k is definitely more common than you think in Amsterdam.\nRejected are rejected by you or them?\nRejected = them. Withdrew = me.\nWhat were the technical assessments like? How did you prepare ?\nI've explained this somewhere in a different comment, but basically a mix between take home assessments, live-coding and data modelling exercises. I didn't really prepare too much for the technical stages. I just did a refresher on some core concepts (mostly for Spark and Python), otherwise I just relied on my existing knowledge.\nNice illustration and congrats on the 4 offers. Where did you search for remote job offers? Linkedin?\nThanks! Check somewhere in the comments, I've added the list.\nGreat viz! Im in a similar process, what tool did you use to plot this? Ill share my journey in a few months once Ive got my job and share\nSankeyMATIC. Good luck! Will keep an eye out for it.\nAny suggestion for interview prep. Specially the data modelling part? How do present the take home tasks? How do you make the documentation?\nFor data modelling, familiarise yourself with OLAP and OLTP, what models is suitable for what system. Also what tools are suitable for both. Also have a look at other options, such as the Medallion Architecture. The take home task they normally review (e.g. in GitHub or you send a zip file), then they might ask to go through it with you in a next round where they ask you questions about it, and maybe extend it further. For documentation, you generally just need a good readme, and ensure your code is super readable. You can also add code comments, but good code (for simple take home assessments) shouldn't require much/any code comments (imho).\nIn one take home task I didnt add a good Readme file. They didnt proceed to next round for that. About cloud based questions? How deep they go into infrastructure?\nEU needs data engineers. The hard part is finding a fully remote job.\nInteresting as my first response to sankey is no, my second response is how we can generate something simpler, third is to viciously berate you for even wanting it in the first place.\nDid someone come in a fluffer though?\nI dont understand this question \nSorry for confusion, its just a Twitter meme: https://knowyourmeme.com/memes/came-in-a-fluffer. Some people (including me) cant look at Sankey diagrams the same ever since.\nHow many years of working experience do you have? All of these is in data engineering or have you also worked as data scientist or data analyst before?\n2 years in Software Dev. 4.5+ in data roles - 1.5 year BI, 3+ DE. (Only did a a B.Sc. Information Technology).\nHow many years of experience do you have?\nIdeally wanna put your CV here as well! 100+ applications and only few wanna talk to you , maybe you over or under or maybe you apply bulk and hope it catches...\nIll pass but thanks ;) The reason for low amount of screenings is that these are all remote roles, so competing generally with hundreds of other applications. The other takeaway of course is the almost half of my screenings led to offers.",
        "content_hash": "5771969def8f96d16d5f16317cb8dc10"
    },
    {
        "id": "151xsis",
        "title": "Data Scientists -- Ok, now I get it.",
        "description": "[DELETED] ` this message was mass deleted/edited with redact.dev `",
        "score": 574,
        "upvotes": 574,
        "downvotes": 0,
        "tag": "Discussion",
        "num_comments": 217,
        "permalink": "/r/dataengineering/comments/151xsis/data_scientists_ok_now_i_get_it/",
        "created": 1689588707.0,
        "comments": "Am data scientist -- 25% of these dudes are scam artists. Similar experience but worse in academia. It's one thing to lack performant code, but unsure of what it's doing? Ridiculous. Edit: Alright, I may have been a quite crude with the above phrasing. But, in my experience, it's very clear that a large proportion of data scientists/quants/researchers/academics have never actually had *a third party check their work.* For example, [a professor at Harvard Business School was just put on leave after it was revealed for a decade she literally faked data in her research that was published in the top journals \\(Data Falsificada\\).](https://datacolada.org/) If you read those articles, the fake data is outrageously obvious. It's incredible. There's not even a thought in the *Harvard Researchers'* minds that anyone will actually check their work. [We can also point to the fraudulent data that may have put Alzheimer's Research back ~10 years and wasted literally billions of dollars.](https://www.science.org/content/article/potential-fabrication-research-images-threatens-key-theory-alzheimers-disease)\nAt my company we call them the \"fit-predict data scientists\"\nThe script kiddies of Data Science!\nSame. Fit-Data Scientists. Fortunately, decision science is a nicer title than analyst and companies are moving folks there. Next time you run into your partner DS ask them, whats the difference between a linear and logistical regression?. Its insane how many I work with or panel interview cant answer.\nYeah, I know the feeling... Once, during an interview, I had a candidate who described himself as a senior data scientist, but he couldn't explain the advantages and disadvantages of using MAE vs RMSE as regression metrics.. He actually couldn't explain what those were and literally accused my colleague and I of gatekeeping. The guts he had... It is always hard to know whether you are gatekeeping or not since DS is now a broad field. Reason why I think you are right to check for fundamentals indeed.\nLol the whole point of an interview is to gatekeep, wouldnt want those folks in my team either\nI would pay $500 to see this interview.\nReally? dang shame. I don't claim to be the DS but know what's the difference.\nHow the hell do those people get jobs??? I am more competent than that at least, definitely a novice but eager to learn and become an expert and I can't even land an internship. Literally considering suicide at this point.\nNeopotisim. Its all about connections and hiring people who look like you and feel comfortable with it seems.\nAt mine they were the fake news gang\nI don't want to name the company but my organisation also does have a sub-company which works on the data science aspect of things. On closely speaking with the developers I found out it is practically doing a 20 lines of code of XG Boost fit-predict. I laughed internally, thinking, I could have done way better pre processing and chosen atleast an ensemble network for real life data and scenarios. But I would still remain an underpaid labour while they are a company with CEO and stuffs \nThis is why I think DS is a dying field. With enough compute you can brut force anything.\nI call them Jupyter notebook data scientist\nIf we were to just get rid of Jupyter notebooks .... ahhhhhh.\nThats why Im swapping from data science to data engineering. Business rules are very arbitrary, data scientists are bad at enforcing coding standards and extremely overhyped and data engineering is domain agnostic and more exciting. As a data scientist, I build PowerPoints to make clients happy. Id rather leverage my technical skills more optimally than use my quantitative skills to produce the right numbers pleasing to the business lmfao\nLiterally why I'm here. I've seen so many machine learning models built on absolute garbage data I want to leave -- better analytics engineering on a much simpler model will likely have better predictive performance and model maintainability (easier docs, easier selling to higher ups, etc). But now you have to wage a political war because the DS team doesn't want outsiders coming in and building a better system (they'll look bad). Edit: to be clear, the main reason I want to leave is the politics of working with data scientists. Maybe I've had bad luck, but so far it's been ridiculous. Literally pointing to obvious problems to people with MS/PhDs from elite schools and it's non-stop ego with wildly over-engineered models so they can show off how smart they are. We are not building GPT-5 on a supercomputer in Silicon Valley -- get over yourself.\nLiterally this, its gotten to the point where full blown political battles are waged over simple data quality initiatives that would make the DS models more accurate. Literally DS teams dont want to help themselves in fear of being exposed.\nWow. I feel you. As a many-hat data analyst (don't want to call myself a data scientist as the role kinda evolved into a negative connotation for me being associated to clout chasers), I feel like all I'm doing are producing the right numbers that will make stakeholders agree for them to present it to C-level peeps. It's draining to be honest and my motivation to wake up and do my job just isn't there anymore.\nall i do is build decks as a data scientist right now\nIn my experience it's closer to 75% are con artists. It's hard when management would rather hear a lie than the truth. The script kiddy data scientists I don't mind, if they're okay at research. That is, they know how to do their job, but their programming skills are incredibly weak. That's fine because I have strong coding skills. I can walk through it with them and in a friendly way help them out with it growing their skills. When your programming skills are low, writing code is like banging your head against a wall. Compile error after compile error. It sucks! They'd be happier with more programming experience. So I come from a place of helping them when they're stuck and they love me for it.\n>It's hard when management would rather hear a lie than the truth This is a big reason why a lot of Data Analysts and Data Scientists end up in Data Engineering. I couldn't deal with putting in so much effort into research and analysis only to have it shelved because the stakeholders didn't like the results. You then find yourself starting to massage some of the numbers to get things to look the way they need to look to justify all of the work. \"There are lies, damned lies, and statistics\" sums up my DA/DS career. They don't teach you that in the lecture hall.\nWow on that last quote. Can I steal it? Haha.\nIt's a Mark Twain quote, allegedly re-quoted from Benjamin Disraeli\nI don't think Mark Twain would mind. I think he borrowed it from someone else as well. Edit - this quote has a more interesting history than I thought https://www.wikipedia.org/w/index.php?title=Lies,_damned_lies,_and_statistics\nHow do you judge when a person has good code and when a person has bad code?\nFirst telltale sign for me is that they don't do the DRY approach. Also, codes that repeat a certain operation via loops instead of vectorizing it. Granted some sequences of operations can't really be vectorized, but on cases that it can, then it should be used most the time imo.\nfwiw, you're probably responding to a bot. There has been a surge of them as of late. The Turing Test shifted from being able to identify a bot from a person to not being able to identify a stupid person or young person from a bot.\nThis is largely because recruiters can't weed out fools. The concept of data scientist started out as very experienced programmers with a focus on data tooling and methods. Now it's just about anyone with a graduate degree, some stat courses and basic SQL knowledge that manages to swoon the recruiter out of the +500 applicants per opening.\nI got hired and taught to be the tool maker for analysts in my first data science position. Searching for work made me realize I got a unicorn, thinking I pretty much gotta switch to full on software development if I want to progress technically... Literally everything out there is either a joke job doing visuals in Tableau/excel, or impossibly difficult where they want someone to basically start from scratch and singlehandedly develop out the big data framework while simultaneously also doing all the analysis.\nI got my first job as a DS in 2010 before the title was common. In the early generation they were all data analysts that sucked at programming but were great at research and analytics. Back then it was rare to find a crossover, someone who was amazing at programming and research. The unicorn joke was coined from this, expecting a data scientist to be good at everything: Unicorns are not real. It wasn't until DS became advertised as the hot job in 2012 that software engineers interested in ML started switching job title, learning it wasn't what they thought it was (there is very little ML in most DS roles), and switching back to software engineering. For a while from 2012+ there was a surge of competent programmers who didn't know the first thing about science and the scientific method.\nThe recruiters are the worst fools. Never met a recruiter who knew his work. The second worst people are the managers. If they cant define the role of a person, what good does it do. Building a proper team takes skill, after 28 years of working in this field as a consultant, I think 90% of the people are mostly useless.\nThis is upsetting. I'm doing a master in data analytics and was hoping to do data engineering. I'm 17+ years as a software engineer and am blown away at the crappy code that's getting higher pay and might not even be correct.\nI'd say a lot of the recent entry level hires are mostly scam artists. These guys don't know how to code, don't know how to explain their model, etc. Granted, I only have ~8 YOE, but I have noticed this trend in the past 2 years with new hires, they don't really know anything, even giving them the benefit of the doubt as being inexperienced.\nAs a DE I worked with a lot of DS, a lot of them do not understand what they are doing. Most of them were just trying a bunch of models and see which results they liked the most or picked a model that their seniors or lead told to. Some that I work with did not even know the difference between nominal and ordinal categorical variable. And somehow blame us that our data is not clean enough because the nominal category column does not make sense after the sort. No shit sherlock\nI had a CEO who wanted events per timeframe explained according to the normal distribution. I think the word \"stochastic\" would have been beyond his comprehension.\nSo just tell him the mean and variance and move on.\nId say a vast majority of data scientists and enterprise architects are level 99 in bullshittery\nI like how they say Dataframes and Tuples to impress. Its just arrays and tables.\nNo. A DataFrame is a structure which has a schema definition and column names, plus ordering. Its mathematically a relation (or a set of tuples) with ordering. A tuple is a tuple. Different programming languages call these differently, but I think they are right on calling these algebraic data structures mathematically correct.\nA Dataframe is a kind of table. Saying Dataframe is correct. It's not to impress, it is the correct terminology. To give an idea, a Dataframe is closer to an Excel spreadsheet than it is to a table in most programming languages. ymmv ofc.\nI don't think anyone is using the term dataframe or tuple to impress, it's just the terminology many data scientists are most familiar with. If you work with pandas or pyspark you will think and talk about dataframes a lot.\nTuples are kinda arrays, the main difference between tuples and lists are tuples are immutable\nyes you learn this in the first semester.\nTrue story. I interviewed at a defense contractor a few months back, and that was my technical questions.\nThis made me laugh\nWorking adjacent to academic research is consistently horrifying.\nYou're not crude, don't apologise, you 100% right. I see this in my organisation as well. I even being a Masters in AI and midway through my PhD research do always ask for time to come back with answers when any situation arises, my so-called Data Lead knows everything _somehow_ and 90% of the time he is either wrong or his answer failed to answer the problematic sides of the asked question.\n> I may have been a quite crude with the above phrasing. I thought you were on the more polite side.\nJust thought I'd mention that you don't always need to adhere to DRY. There are scenarios where it will do more harm than good by introducing another layer of complexity to the code. The fact it was written by ChatGPT and they can't explain what it's doing? Yikes\nUse the rule of three: 1. Write code line the first time, yeah nice. 2. Write very similar code line second time, get that slight niggle feeling. 3. Write same code a third time, get the eye twitch then build an abstraction\n4. get a similar but not exactly the same requirement and start adding bloat to the abstraction\n5. Your codebase is full of half baked abstractions and nested inheritance and very few people understand how to use it.\nYou call it tech debt. I call it job security.\nah, i see you've seen the report function at my company.\n6. Code base is too far gone, and you hate the thought of even looking at it. Leave job for a promotion at a start up. Return to step 1.\nSQL doesnt play by the same rules procedural or functional languages do. Please repeat yourself - functions and views in SQL do not scale\nAlternatively, only use functions. Every function should have one function, be simple and have a useful name. Whether you use it once or many times is irrelevant. Just optimise for understanding in each case. I don't even think of functions as being a tool for DRY anymore. I just don't want big blocks of code doing too much. Does that sound reasonable or am I misled?\nI'm likely the wrong person to convince re: functions, I spent almost 2 decades riding the OO train. Choo choo.  I've built way too many enterprise apps in a CI/CD settng ... the best compliment I received was being told my project was, \"an oasis of sanity\" amongst the other projects. I think functions can work in smaller code bases, like less than 10k lines in total, but when you build larger projects abstractions (objects) are a must. I also laugh at OO puritans who only consider \"rich domain models\" as true OO (state and behaviour) versus aenemic domain models (aka transaction scripts) as OO. The best enterprise apps are aenemic, all state comes from contexts, mainly request but also session and transaction. NodeJS added to this with late-entrant async responses - something that was always buildable but not always done elegantly. I guess that's why I never jumped on board scala, waay to much hate for OO and often debugging was 10x harder than standard Java stacktraces.\n>There are scenarios where it will do more harm than good by introducing another layer of complexity to the code. I've found this to be true in a few work environments. Longer, repetitive solutions have potential issues: copying mistakes, nuisance to update in multiple places, length. But clever abstractions or less well-understood solutions create problems, too. If everyone else in your shop is a beginner-intermediate SQL user, are you doing them a favor by writing short, clever expressions and calling functions that no one else knows?\n\"Clever\" is the worst because it requires understanding it to use it, and that bothers people.\nThat's why you need to have documentation right? It should explain what a block of code is doing and new hires should be reading it as part of their onboarding process anyway.\nLol Lmao even\nIf the function has a name that matches what it does, then, yes, you are doing others a favour by letting them know it exists and giving them a pattern for using it. Why not introduce someone to a function like REPEAT(),REVERSE() or LOWER() if they have the use case? Many functions in SQL do what they say on the tin, or can be understood after spending a few minutes reading the docs. No reason to 'protect' people from them.\nI thinking writing with ChatGPT is fine for time saving as long as you actually know what its spitting out and can explain it and improve it.\nIt's good for de facto looking up documentation/ simple examples faster than you could before and that's about it.\nYou can have it write an entire script for you in 3 more seconds than it takes to ask the questionand then you can just tweak that. This is cope. Its not replacing you. Its augmenting you. If youre not having that experience, youre not promoting it well enough.\nWhenever I've tried to go further than three or four lines in one question, it's been wrong in a way that I couldn't solve without starting from scratch and taking longer than it would have if I'd gone slower. I've optimised how to ask the right questions for what I do.\nWanting admin access to production.. after admitting shit like this.. I cant\nYeah but he asked nicely, usually they just go whining to your bosses boss\n[removed]\nWrite access is reserved for the people that are capable of fixing the data set their bug produced.\nI'd be exiting stage left.\nDid the HoD understand your argument?\nRight. I'm so invested in this story. I need to know what was said once the explanation was given.\nI CAN believe it. I've had (former!) bosses ask what the hold up was and spend 2 days working on custom permissions.\nClone it, give him access, get popcorn.\n \"this is fine\" \nI think you just reinvented the blue-green deployment.\nHook up his boss' system to this cloned version for more excitement.\nCan I get a job where you work? I can use ChatGPT too.\nSure but you need 5 years experience as a prompt engineer\n5? I think they want you to crawl out of the womb with a keyboard and mouse in hand.\nId be more concerned with the chat gpt statement. What all did he feed that and does it violate any company security policies?\n>violate any company security policies yep probably.\nI get that this is going to be an unpopular question, but I've always kinda wondered what the realistic problem scenario is with this in the vast majority of cases. Like is it really plausible that someone is going to go dig through OpenAI's training data (obviously private to them, so it'd have to be OpenAI staff or a leak I suppose), find the schema of your customers table you entered into ChatGPT, and then build a business from it that directly competes with yours? I get that you probably shouldn't include entire repos and secret keys and what have you, but if I pass it the class definition for some internal object, is someone really going to be able to profit off of that in a way that threatens the business? I understand that company security policies have to be followed regardless, I'm just trying to understand the rationale behind the policies in the first place.\nOnce it leaves your company sandbox you dont know what could be done with it, including being sold by openAI or its successor companies (directly or indirectly as e.g. part of a training set). Yeah you shouldnt be giving repos or secret keys, but theres also trade, business, or other secrets you may not want out in the unknown wild too.\nIf a single person's PI gets out then yeah you could be screwed.\nThis one data securities\n\"write me a SQL query that does X\"\nX is clearly doing a lot of work for a sql query that OPs coworker doesnt know what its doing\nWhat's the protocol for handling this? Where I work it probably means I'm rewriting it for him.\nDepending on how your organisation is run and how often that occurs, it might be time to introduce best practices to the teams surrounding you. As a DE, I think its a worthwhile investment of time to teach this kind of thing to others. If they cant write or even understand intermediate SQL, you might want to propose new hiring and training practices, because this shit is going to spiral out of control and leave you dreading your job over time.\nPart of it is our data warehouse is pretty complex. We have roles that are basically dedicated SQL developer and I'm leaning towards pushing for every data science project having one of them attached plus someone who represents the appropriate consumption layer (dashboard developer if a dashboard, integration specialist if a model is feeding directly back to production systems). Basically agile teams.\nYeah, that sounds like a decent plan. Honestly, if the code quality is that bad, just having code reviews in place (maybe even just looking at samples as a team) with one experienced SQL dev and setting up linting/formatting might go a long way by itself. I agree though, having access to SMEs from every part of the lifecycle would be the best option.\nBest case scenario I agree. Budget constraints however. Only way I got upper management to listen to me was when the Python code was run at 1am because it was so slow, and it broke the DV+DW, thus no dashboards the next morning with finger pointing\nTried rarely works. So he designs a monster in Dev, I look at the output and rewrite 100% properly then share. Slow improvements over time if any.\nIts worth a try. Some people actually appreciate proper feedback. Code reviews are standard in software development (where best practises are followed). Just constantly rewriting the queries sounds really annoying, especially from an efficiency and monetary perspective. DEs are pretty expensive in general. Investing a few hours into training and setting up staging/dev environments with linting and code formatting seems like a good idea.\nOut of curiosity, what would be considered intermediate SQL? I have a hard time knowing what skills would mean you are an intermediate versus a high level begineer\nWell, the lines are blurry, though if you take DataLemur etc. as a way to rank, thats where Id expect you to be able to solve almost all mediums and a decent amount of the hard ones. Thats the I can code and understand SQL part. Then theres knowing your flavour of SQL, how to optimise a query, knowing a decent amount about the underlying technology of these database systems, being comfortable with window functions and complex joins etc. Also, being able to keep consistent style/knowing about linter and formatting tools and having a decent understanding of the ecosystem and applying its tools separates you from a SQL monkey.\nReject the pull request and ask them to re-write. There's no way it gets merged into main branch as is. And there's no way I do the re-write for them. They can figure it out with their team on how they want to move forward but it obviously presents a lot of risks as is.\nYup what I do. However a SP run at night and results in a static table, different schema.\nymmv depending on if the company has coding style standards requirements or what. Assuming this code is only going to be used in one scenario, it's premature abstraction to clean it up too much. Assuming this code needs to run faster, because of convoluted sql statements, it is premature optimization to clean the code up so it runs faster. Instead of assuming, figure out the actual issue. Does it run too slow? Then it needs to be rewritten with more efficient SQL statements. Will it be used a bunch of times? Consider wrapping it into a library so the DS can call it a bunch of times. However, when you have code you do not understand, it could have bugs in it. The convoluted sql statement could pull in data in ways that could be unexpected going forward. You need someone good enough to sit down and learn the parts of SQL query to verify it is doing what we expect, so we know it will not create future bugs. Or alternatively, walk through it with the DS creating unit tests of every possible data scenario to verify it is in fact correct. The problem with code no one understands is there is probably hidden bugs so if it goes to production the DS will probably come back with rounds and rounds of bug fixes you have to push out, giving you and them more work. It's best to do it right to begin with. imo not enough engineers share this philosophy. Today it's get it out the door, but you'd save time if you did it right and 99% bug free to begin with.\nI never expect data analysts / scientists to write deployable code, but the must be able to explain what the code is doing. Data scientist/analyst provide code that (should) works -> data engineer makes the code compliant and performant, and aligns to our DB migration tool -> Cloud engineer handles deployment.\nShare some example so we can judge?\nYea i would also like to see it\nI never thought Id see the day that people with data on their job titles dont know how to write SQL. But were here and it makes me feel old.\nI'm gonna be honest, if this happened to me, I'd have an immediate discussion with their +2 and +1. Their manager AND director. This is unacceptable, and shows a poor sense of control over their team. No one should be using GPT exclusively without understanding it. No one should be asking for admin access even if they do understand it. This is Exactly why there are controls in place.\nMy first instinct, Throw it into chat got and have it refactor the code then test it for the correct output then have them review it.\nRefactor the code this might be a good prompt for future. Thanks \nCommon, not like its going to break anything /s\nThis is that guy OP\nlol\nNot really a Data Scientist thing, more of a programmer thing. There's always shit ones.\nHe should have just asked chatgpt to explain the code line by line before admitting he used chatgpt\nAs a fresh CS grad with zero development experience, Im not sure what yall expect? It seems that everyone expects entry level DS to have 10+ yoe. I tried to focus my courses in DS and only have a breath of knowledge. The projects required in these courses were Mickey Mouse so nothing is learned about writing production ready code. Now granted, Im not an idiot and would never try to pass off code from Chatgpt as production and certainly would not ask to permissions to put anything into production. I come from 20 years of blue collar work and it was always on the job training so when I invested 6 years in a Masters Im thoroughly let down at my sheer lack of job ready skills that doesnt seemed to get any better doing these side projects with little guidance. Im genuinely curious what is expected of an entry level DS\n>Im genuinely curious what is expected of an entry level DS Data/ML Engineer here. Imo the basic things that entry level DS need to have is deep understanding on machine learning algorithm (deep learning/neural network, random forest, and whatever is the meta right now). How to train a model from it and how to fine tune the parameter. Usually the lead/head initially will help you to define the industry problem that needs to be tackled but as DS become more senior, they should be more adept with their company business context and brainstorm the idea by themselves If it is image/computer vision related DS, then a capability to do preprocessing in image data to become input in machine learning model is a must also How about SQL? To be honest if you are DS, this is not even a minimal thing, SQL and python/scala capabaility are def a must because without that how you can fetch required data to do training and actually build your model? But I would say if you just only understand SQL and what you do is just churning some data with tableau/looker or even worse excel, then you are actually either a Data Analyst or Business Intelligence not Data Scientist. The company either dumb enough to overpay you as DS or they tried gaslight you with DS title but DA/BI salary\nYoure fine. Theres a salty element on this subreddit and I think theyre going way overboard on this one. Any role could have done something like the original post, *including data engineers*.\nBut do they not teach fundamentals anymore? As a fresh CS grad (many moons ago) I had to work in a terminal without notebooks. And didnt command the salary some of younglings do today. I had trouble with basic ETL from the cloud because apparently my cloud engineering didnt understand reserved IPs (e.g. 10.0.0.x) werent routable. I bet all of them earn at least $125k USD.\nthe buying power of the salary you had when a fresh cs grad, is probably equivalent to 125k at this point...\n$30k USD in 1999, undergrad in Software Engineering. and thats with Y2K fears and before dotcom bubble. $70k in 2001 with grad degree (from Ivy League) in Comp Sci. I agrees with your sentiments on inflation but I believe students had a deeper grasp of the fundamentals back in my day. I know Data Scientists who make $145k but cant read and loop through data w/o pandas or numpy.\nIn all of my data courses (university and boot camp), we only used notebooks. I am comfortable working in terminal since I have had previous experience having to work on the command line.\n he last statement make me laugh \nOh yeah. Current role and previous. Plus Python by default will put the query in a transaction. You have to work for it not to. With (NoLock) doesnt help if in a transaction. /end rant Compromise is that I put in a stored proc the logic and generate the data in a static table in a different schema. Data is always 24 hours old. If he wants live included, a View combines two data streams. However 24 hours old was always Ok. In the SP I optimized the hell out of it. Guess the schema name. Python, so the Views, SPs and static tables are organized together. But he doesnt run the SPs its a sql job. The table is basically a multi join table to denormalize everything. Not all the joined table fields copied into, just what is needed. Column Store if it gets too big. Basically a IsDeleted field for one condition on update, otherwise, only inserts.\nHe does and can play in the Dev environnement\nWow. Karma is going to have her way with you one day.\nDont get invited to many parties, do ya?\nHave seen only a handful of data scientists write clean code and the only ones came from a dev background and were older. Sadly now as DS is trendy a bunch of people from non-cs like stats/math/physics get into a masters of DS and get jobs as DS. Only to write extremely bad code and have extremely messy notebooks. I have only seen good code from DS once but it was because it was a startup and there were only 10ish DS all in the same team and the lead DS worked as a backend dev for more than 5 years before going for a phd and switching to DS/ML. I got an MS as a DS as a non dev (background in industrial engineering) and knew this masters program was only aimed at research like most because the professors have never worked in industry and they havent put a single line of code in production. I get that notebooks is better to teach math and proof theories but I realized that they even didnt know how to write prod code or queries. Ended up switching to data engineering and love it. Now, people has started to notice this and created some fields like mlops to try to fix whatever sht data scientists have built in prod using notebooks lol. And dont get me started with analysts queries lol. At least I dont expect them to be too technical, data scientists usually let me down. Ive seen better code from analysts than data scientists\nSaying physics/maths people are becoming DS solely because it's trendy is fucking stupid. Their job is to do statistics, data analysis and give insights back to the business - writing code to production quality is *your* job as a DE: that's why there's two distinct roles.\nPhysics and math people are indeed only becoming DS because its trendy and there is (was?) a high demand. Back in the day they worked for insurance companies and banks doing risk modeling but tech companies starting needed them for ML and obviously it pays better and its more chill. And they are not hired as statisticians. They are data scientists. I could understand jobs as ai researcher or something like that they ONLY work on reading ai papers and putting them into practice in notebooks. Then others put it into prod. But that is not the job for 99% of data scientists. They need to be owners of their models and the code it entails. Its like saying a data engineer is also supposed to own tableau dashboards from analysts because they use data from my pipeline. Obviously not. If that were the case then data engineers will be responsible for everything in the company since it all uses my data lol.\nDo you actually know anything about statistics or ML modelling? For fuck sake, a huge degree of risk models *ARE* ML models (yes , in the contemporary sense) and have been for decades. Jesus christ my former employers consumer/product DS team was mostly built from risk modelling teams and were essentially doing the same thing. Data scientists are hired to do statistics and work on *statistical* ML models. If you're the type of person who thinks a CS bro that doesn't know what a p-value is but \"he/she can really code bro\" makes a good data scientist , then all i can say is good luck to ye. Our DS have to do a series of checks to make sure their code is of adequate quality but then is given to our ML Eng pod to deploy. Industry tried a naive dual-role system where data scientists were responsible for adequate stats modelling *and* CS skills for deployment - spoiler it didn't go well.\nI know risk modeling is the same as ML. Thats why Im saying the trend now is to go into tech to do data science instead of doing risk modeling back in the day. Its all the same. The difference is that in banks, no one really cared about your risk modeling predictions, at the end of that day that didnt make them money, you were an expense, code quality didnt matter. Now in tech, code quality is extremely important since data scientists are main source of income on companies (through ads, I work on ads domain). Having a shitty ML pipeline is now extremely bad. And I can say this from a non dev perspective because i have a non dev background in industrial engineering and math and then a masters in data science. I worked at banks in risk too before. I saw how no one tough SWE best practices to statisticians code in stata or matlab or r/python notebooks whatever other BS language was used in academia. They bring their bad code practices of notebooks to tech and everything becomes gets worst and worst exponentially. I learned the hard way though mentors, to implement SWE best practices, and like it so much i ended up switching to data engineering.\nNice. Glad to see there are other companies like this. And as you said, you guys are definitely an exception, almost no one does DS like that since its mostly run by non-dev people. My current company follow data mesh methodology so teams are so isolated from others that some ML data products might have near perfect code and best practices and other teams can have prd ML pipelines running on shitty databricks notebooks with horrible code on dev environment lol.\nWho cares if data scientists write perfect code? They're a customer handing you requirements and it's your job to parse out the requirements. If they loved writing optimized, prod level code then they'd probably become engineers. Granted, they should be able to explain their code...\nMy degree is in ChE but I wish I had gone IE.\nIE is amazing. One of the most underrated subjects out there, and that's after considering the love they get from logistics and manufacturing. Most of what we think of as DS is today was being done by IE/OR/MS guys like 30 years ago. The average problem in an OR textbook is a hardcore business problems, with dollars and cents attached, and an unbelievably broad set of tools from queueing theory to optimization to simulation... And IE adds all the context to the OR work. It's beautiful. I wish the IEOR approach became the dominant \"data science\" thing. I think it's what business people actually wanted. Not predictive models in notebooks.\nThank you for those wonderful words! I have several OR textbooks and use the business problem approach for DS where it is allowed. When a customer gets nowhere with DS or has a problem without statistically significant amounts of data I use optimization techniques from my books. IE/OR still lives today but its called Decision Science. What an apt name.\nI use ChatGPT a lot to get a starting point for my code. But I always make sure I understand what it gives me before I integrate it into my work.\nThis story hit very close to home!\nPlease tell me this guy has \"junior\" in his title. Because frankly that's not even a good perspective for a junior, but it might be correctable. But no one more senior than that should be telling you \"implement this in production, despite the fact that I don't understand this, and oh btw it was generated by ChatGPT and also I'd like admin access to production\". The number of fundamental misunderstandings that needed to occur for him to say those things are astounding. I want to hear him explain his rationale for why he thinks that's a good idea. Genuinely curious what's going through his head.\nIf you can't be transparent about your feelings on this task, you probably have a disjointed or dysfunctional data org. Your DE needs to understand that everything you have in production costs you time and resources, and nothing you don't understand is going to be used to run the business\nYeahthe issue extends to Python devs in general. Ive been interviewing a ton of people lately and noticed this. 5-10 years ago, Python devs all came from people getting into Django, flask etc. even if they were new and came from web courses etc, they all had to go through the motions of building front end, backend, api, db etc. Fast forward to the past few years, everyone is getting into development through data science courses and the only thing they go through is Pandas and notebooks. This is a HUGE shift in knowledge and mental model. While they were programming or sql experts at all before, they had to go through and learn a breadth of things and form mental models which made it MUCH easier to bridge the gap. I have seen so many candidates recently that I simply cant hire.\nI'm a little skeptical of this story for a number of reasons. Most obvious being a data scientist writing SQL code. Also, if anyone is faking code like this and can't even be bothered to ask ChatGPT to write test cases or optimize the code I very much doubt they'd be so bold to simply ask for admin privileges for prod. Also, counterpoint to all these people piling on all these \"fake\" data scientists. If you see this so often maybe it says more about your org and your hiring practices than the individual. Stones meet glass house.\nI hate complex sql. Takes 30 lines of code to do something that pandas/polars can do in 15 when pandas/polars is way more understandable.\nYeah but it will be more performant in SQL\nYa when you got millions of rows with 50 columns sql is much nicer\nPandas is more of a column store than a row store. Great for the kind of raw data a data scientist will be using. SQL is just a DSL that can be performant or not depending on the situation. Its not that hard to write crap SQL after all.\nHighly dependent upon the platform/tech and the business case. Nothing is a panacea.\nYep. The other thing is that SQL is pushing the query down to let the database do the heavy lifting, without an extra network hop, or having to manage a second (and often difficult to scale) layer of memory.\nhere is a benchmark. Also keep in mind Polars can do parallel data processing [https://duckdblabs.github.io/db-benchmark/](https://duckdblabs.github.io/db-benchmark/)\nData scientists are built for persuasive arguments etc. Good ones pass interviews below the technical boundary and make messes while gaining political traction in the organization. Ive met exactly two that were legit. The rest came up through the ranks before their disasters were discovered and had already socially engineered management and high profile folks. Ugh.\ni guess the guy used the gpt3.5 instead of gpt4 or doesn't really know how to use it. if you use gpt4 and make it reivew for the answer once or twice, you can't really get messy codes in my experience.\nTell me you're a junior without telling me you're a junior.\ni mean yeah that's who i am but i still don't think gpt4 is the problem. gpt4 doesn't violate naming conventions\nGPT4 writes some shit code sometimes, including that. I give it a chance daily at work, and yes Im using 4 plus the API.\nAt least he admits using chatgpt, he could have just asked chatgpt what it does xD\nMost of the data scientists at my company don't even know SQL so we have to pay for shit like Alteryx because of it\nWhy they dont use power bi (power query) I thought power query was also capable to make transactions\nPower query has poor performance in my opinion\nLol, same here, i can't express how much i hate alteryx because o this.\nsomeone is on a fast path to freedom\nsounds like a data quack to me\nHaha, this reads like a weird episode of Black Mirror \nAt first site I thought it was a joke, using chatgpt to generate code not understand what it's doing... wow scary story\nAbsolutely ridiculous. Until such time they're able to explain what the code does, it should just be ignored. It is not your responsibility to rewrite it for them\nI think I have someone like that in my team \nI am a Data Analyst, who is pursing Data/Analytics Engineering roles, is fortunate to work in team using GitHub for code reviews and pushing it to production. I learned a lot of standardizing and organizing SQL scripts when building data pipelines. Glad to have a process.\n>You have code that you can't actually explain as the developer yourself but you still expect it to be put in production? Oh it's the best, isn't it? Gotta love hearing that.\nBruh in my company Id send that shit back and say this is not going in production until its sorted\nHoly effn shit! My company shut down ChatGPT a long time ago.\nStop treating dss like they are gods you dumbass. They are literally like any other data professional with average understanding of topics. And most of the time the titles have a lot of overlap.\nTell him to f-off. Fix your code or I wont run it on prod. You dont mess and screw up my database.\nData Charlatan...\nCan you post it here?\nHoly shit, I would fire this dude yesterday. That's inexcusable.\n>He admits that a lot of it was actually written by ChatGPT and he isn't sure what it's doing. But he politely asks me when will his code be put into production I don't get why we have so many people in DS that are underqualified. Same thing at my company - we've been asking for several years now what is the DS team working on and doing? When they do present it sounds about the same progress as last time. I've also had to consume a REST API our DS team built and it was the worst one I've ever had to work with. Just atrocious. I find it amazing people can be in a technical role like DS (which I consider the same as a SQL developer, backend developer, front end developer) and not know how to code. I see the same thing with security/infosec folks. They just run code scanners built by third party vendors who hire the \"real\" experts. Then they open JIRA tickets to annoy the development team about their code scanning tickets that need to get done but half of them are garbage and we have to justify our decisions to people who know less than us simply because they ran a code scanning tool. Privacy folks are also a PITA because they are glorified box checkers: \"does our app do X, Y, Z?\" Yep.... sure. Ok. Done.\nDS was always a bad idea. True DS was defined as the intersection between CS, Stats and Business/Domain knowledge. Turns out that's really rare and hard to do. I think there just isn't enough time to train all those skills. If you're good at Stats and Business Problems but suck at code, you'll end up as 'just' a data analyst because you can't put things into production. If you're good at CS and Stats, upper mamagement will be dismissive of you because you can't communicate well or solve business problems. And if you're good at CS and the business problem, you'll write crappy models that don't fit well and just default to XGBoost to save you. But is it really realistic to expect good code, deep statistical thinking and meaningful business knowledge from one person? It honestly usually sounds to me like people from each of those three disciplines just dramatically underestimating what goes into each, and assuming a smart person from their discipline can grok the rest if they try.\nlol Chatgpt wrote all the code and the comments \nHow does one even get a data scientist job without knowing SQL pretty well? Where I went to school database courses were a prerequisite to even do the predictive modeling courses\nNo way!\nHahaha. I remember at my old job we had this senior data analyst who came across as some SQL Jedi. One day Im getting several slack messages about people complaining why their query wont execute and I take a look at the queue. This fucker was executing the most hideous outrageous query that Ive ever seen. Nested queries upon nested queries, calling the same tables over and over again, nested case statements that just went on and on.\nIt's infuriating because I know my friend who now does his entire work using chatgpt as a data scientist and getting 50x more money than me regressing hard in phd ...\nWhat is DRY?\nNo - this is where you **sit down with him, explain that you can't put in production code no one can reason through** (and also because of the well thought-out company policy, of course), **the dangers of doing such** (so he *really* understands), and figure out what he needs through requirements and then **write it yourself**. *THAT* shouldn't take more than a couple days. If you can't do this for whatever reason, get your boss to do it.\nSQL should not use DRY, really hope youre looking at python or something functional\nAdmin access to prod- oh my GOD. BOY, the confidence even.",
        "content_hash": "83b576dfff88ea7d87c7a0dd49799b6b"
    },
    {
        "id": "1f56kuu",
        "title": "80% of AI projects (will) fail due to too few data engineers",
        "description": "Curious on the group's take on this study from RAND, which finds that AI-related IT projects fail at twice the rate of other projects. \n\n[https://www.rand.org/pubs/research\\_reports/RRA2680-1.html](https://www.rand.org/pubs/research_reports/RRA2680-1.html)\n\nOne the reasons is... \n\n\"The lack of prestige associated with data engineer- ing acts as an additional barrier: One interviewee referred to data engineers as \u201cthe plumbers of data science.\u201d Data engineers do the hard work of designing and maintaining the infrastructure that ingests, cleans, and transforms data into a format suitable for data scientists to train models on. \n\nDespite this, often the data scientists training the AI models are seen as doing \u201cthe real AI work,\u201d while data engineering is looked down on as a menial task. The goal for many data engineers is to grow their skills and transition into the role of data scientist; consequently, some organizations face high turnover rates in the data engineering group. \n\nEven worse, these individuals take all of their knowledge about the organization\u2019s data and infrastructure when they leave. In organizations that lack effective documen- tation, the loss of a data engineer might mean that  \nno one knows which datasets are reliable or how the meaning of a dataset might have shifted over time. Painstakingly rediscovering that knowledge increases the cost and time required to complete an AI project, which increases the likelihood that leadership will lose interest and abandon it.\" \n\nIs data engineering a stepping stone for you ? ",
        "score": 566,
        "upvotes": 566,
        "downvotes": 0,
        "tag": "Career",
        "num_comments": 119,
        "permalink": "/r/dataengineering/comments/1f56kuu/80_of_ai_projects_will_fail_due_to_too_few_data/",
        "created": 1725052978.0,
        "comments": "I have 0 interest in being a data scientist\nAs former DS and current analytics manager, I agree. So much wasted effort.\nHow come? Im a data analyst who interested in DS but data engineering seems to be a more straightforward career\nBecause 80% of projects end up in the garbage\nBut... Isn't this supposed to be the BI guy's job? The engineer engineers and the BI guy wears a suit to show things to the suits\nWhy is that? Being data engineering gives you an edge for building dashboards and tools.\nHere's a sneak peek of /r/dataisugly using the [top posts](https://np.reddit.com/r/dataisugly/top/?sort=top&t=year) of the year! \\#1: [The famous \"county\" length unit](https://i.redd.it/bbxh00vq6zoc1.jpeg) | [277 comments](https://np.reddit.com/r/dataisugly/comments/1bhbkmg/the_famous_county_length_unit/) \\#2: [Not trying to mislead at all](https://i.redd.it/e05jm9zy79rc1.jpeg) | [134 comments](https://np.reddit.com/r/dataisugly/comments/1bqnfn7/not_trying_to_mislead_at_all/) \\#3: [This is by far the worst scientific graphic I've ever seen.](https://64.media.tumblr.com/93115b9c3097d2549651080a8388064d/35021d6627a8433c-d9/s1280x1920/5c438c2b3bc12f7911c839538f57f593a81511b3.jpg) | [114 comments](https://np.reddit.com/r/dataisugly/comments/1axe1e0/this_is_by_far_the_worst_scientific_graphic_ive/) ---- ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| ^^[Contact](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| ^^[Info](https://np.reddit.com/r/sneakpeekbot/) ^^| ^^[Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/o8wk1r/blacklist_ix/) ^^| ^^[GitHub](https://github.com/ghnr/sneakpeekbot)\nMakes sense. Thanks for clarifying\nI'm an ML engineer, and I'm trying to switch to data engineer. Data engineering is the past, the present, and the future\nCurious why you want to switch? Im considering this too\nYes but at the same time need to build up new capabilities around NoSQL\nYeah i also stumbled about that sentence I don't know any DE that wants to be a DS\nI pretty much became a Data Engineer running away as fast as I could from Data Science, definetly not going back there\nHow come? Im a data analyst who interested in DS but data engineering seems to be a more straightforward career\nIn general I wouldn't say that it's an easier career, probably it's an easier path to shift into, but that's mostly because it's less saturated than DS, commonly there is the need of having more Data Engineers than Data Scientists in a company (DS is just one of the \"clients\" DE serves). What led me away was basically that in many corporate companies a Data Scientist, is just a guy with an economical background with some basics of statistics, and that's just what they need, since they value more the knowledge of the domain (which I recognize it's important, but still) than a deeper knowledge of the machine learning related topics. I simply couldn't stand it, I like more the technical and mathematical aspect, and DE in a way gave me just that, while in general it's not really hard you have to have a solid background in computer science in order to do a good job, hence it's a discipline I feel more bound to the technical aspect\nI really appreciate the insight. Its unfortunate that you mention DS are mainly used for domain knowledge needing statistics because I already do that as a data analyst and the idea of working on machine learning models, or anything of that nature, seems exciting to me. You are correct that its an easier career shift due to the less saturated market. Youve definitely given me something to think about.\nWhy?\nFrom what I saw there are two macro categories of DS jobs: The first which is really interesting and it's all the DS close to research fields, you're asked to read papers about new technologies/algorithms, every project you're assigned is meaningful in the scope of the company and you use a great variety of techniques and algorithms. Thrilling. Then there are corporate jobs: you have the opportunity to pick among 2 or 3 algorithms (the most quoted before Generative AI were Random Forest and NNs), usually everything is already implemented within a framework, where the only thing left is to run an optimizer and tune the hyperparameters, after extracting some features. Not every project is meaningful, sometimes some random manager with a bit of budget ask you to create something quite stupid that probably won't even see the sunlight. Moreover the job may tend to become more \"political\" than technical, where you have to sell your projects to stakeholders, convincing them that Data Science would even cook their dinner. Sadly the second kind of jobs are the majority available, and I really couldn't stand it. It's just not for me. On the other hand Data Engineering is quite technical and broader as a field, so even in static environment you're asked to carry on several types of tasks, this makes it more interesting to me.\nHow would you advise to transition to DE? Im a data scientist but have been helping out a bit with DE tasks. That being said, I dont have a DE title and Im not sure how that would reflect on my ability to get hired.\nAll of the places I had the title DataScientist I ended up having to do all of the data engineering legwork for all of my projects because there was no real vision for how data should fit together. DataEngineers did EL no ETL data arrived pretty much in what ever schema it had been found from the source system. It never made sense to me to have to do two jobs but only really get the credit and pay for one. So I shifted and focused on data engineering and architecture. These days so much of the analysis you could want to do is already a lib that is well understood.\nThese are a pair of major issue in the world of data science too. \"Data Scientist\" title attracts more candidates and many company use it even if the actual DS is 10% of the work. And you'll likely end up doing DA or DE activities most of the time. The second one is one that i feel close, I've seen far too many people using algorithm in a \"black box fashion\", without really grasping what the algorithm does or how does it work, but since the industry is pushing more and more to automate AI tasks and make it available to everyone, nowadays there are tons of libraries and tools that allow you to obtain decent results even if you're not that skilled\nI am one of the people that was in the first wave of building some of those tools 15 years ago my algorithms were never truly black boxes but the 4 fortune 50 companies that still use the original libs I built at scale use them that way. Thats part of the problem few companies have actually built the Data Science Utopia we hoped for where research would be constant and iterative. I have been back to two of those companies in the intervening time as an advisor to get them off of my work and they failed to. I have worked with many that have claimed to have built the utopia most of that is just recruiting snake oil sales. My career today is in data security my penance doing the work trying to stop companies from being irresponsible with the tools we build because we could.\nI'm trying to transition out of DS into DE, but I'm finding it difficult to figure out what to do. Any tips?\nAccording to me DE, is a bit more bound to Computer Science than DS, so that could be a point where to start. SQL is a must have (in a way or another you'll end up using it everyday, basically every DE tool revolves around SQL up to some extent). Along which I'd study python (this one should be easy, since it should be quite used in DS too) Then I'd go for Spark, which is an evergreen (still based on SQL, but it differs from a Data Warehouse, being optimized for big data), and is used in a lot of companies. A bit of DE theory is always good, which in DS is not quite used, snowflake vs star schemas, normal forms, types of keys (primary, foreign...) Last but not least learn a bit about the DE tools of a Cloud provider, just more or less what they do and how they're used (with Azure we have: Data Factory, Databricks, Synapse, SQL Server...) These are the main things that pop into my mind\nAnd that 0 is statistically significant.\nThere isn't a lack of data engineers, there is a lack of business leaders who value investing into data engineering.\nExactly.\nThe way I explain it to other software engineers: Building an AI model is like building a compiler for a specific hardware platform. The data is the source code. Data engineering is like writing code for a specific applications, debugging it and adding new features. How much effort do you think you should spend on building a compiler vs writing your program?\nWow it's my first time hearing it explained this way and you couldn't have said it better. This is the way.\nHopefully you took the valued advisor route and seized the opportunity to educate the PO so that they can then become another sorely needed advocate interacting with the Product Managers and PMO who influence team staffing and role funding.\nHow would you explain it to them? Im curious\nAsk him if PM and PO are the same thing.\nIt's both and they feed each other.\nI meant supply of DEs. Demand is certainly lower than it should be imo if companies were better at leveraging and prioritizing data work.\nI think it's also difficult to higher DE because it's not a well known job, despite topping the charts in most valuable IT jobs in the recent years. Students interested by data are sucked into the data science black hole and add to the pile of junior data scientists that will never get a DS position. So there is a supply issue. Supply being an issue can also demotivate further demand. Because DE is not well known, business leaders without practical data experience don't know the role value (I have fought with those before). So they don't hire them and higher data scientists who want to do ML instead. As a consequence, less people get DE positions and so less experienced data engineers can be produced, which feeds the difficulty to hire DEs. For people who managed to get into it, it creates a pretty great job market with a lot of room for market share growth as business leaders progressively get educated.\nI hope so - currently a DE with 5 YOE seeing stiff competition. Im generally doing well in interviews and wouldve started a job by now if offer wasnt on hold due to hiring freeze but have been passed over in multiple final rounds with many of the companies saying theyve never had this many quality candidates before. Many places told me they want to hire more DEs if they could but they barely get enough budget for 1 hire. A ton of these companies have 1 small DE team support a company of 500-1000+. Its hard to see that supply of DEs is an issue when there seems to be so many talented folks available. Companies seem to prefer running lean as possible particularly more for DE than many other roles.\nPorque no los dos?\n\"Everybody wants the fruit but never wants to plant, nurture and wait for the tree to grow - me, right now, slightly high\nYes\nLove this \nLet all of those projects die so we can get back to doing more meaningful work.\nIs AI not meaningful? Genuinely asking as someone with no knowledge of the field.\nAs someone who has been in ML and DE, AI is oversold and usually the problems leadership thinks will be solved with AI isn't necessarily where the impact is. Furthermore, the most in-your-face AI takes an insane amount of R&D and is expensive and a hit on the environment. Also, oftentimes there are low-hanging fruit that make a huge impact with little investment. That said, there are great examples of where AI _has_ made a measurable difference. The first major application of neural networks to scanning addresses from mail made the post office manifoldly faster and require far few workers to parse locations. Some simpler autoencoders are great for lossy, but fast video calling. These aren't prestigious, but they are valuable and fairly low hanging fruit.\nHandwriting recognition in ATMs for check deposits has been a huge application of machine learning for a long time.\nGreat post, I really appreciate it.\nmachine learning is always going to have high value applications, but ime, the valuable ones are either hard problems people need to think through (ie translating a complex problem into something that can be modeled statistically) or theyre tabular data exercises you clean up and throw xgboost at. language models are really just doing one thing... modeling language. but because the industry has decided this is the vector of machine learning that theyre going to sell as \"AI\", many ppl are convinced that these models can do everything. lots of people are currently trying to apply language models to problems that need a different solution. usually a cheaper, more elegant solution that takes experience and domain knowledge to solve. the way i see it, \"AI\" has been developed specifically to do one task: beat the turing test. its worth noting that this is not a test of intelligence, but of the inability to prove the lackthereof in a specific context (communication with a human).\nI really appreciate the write up!\nMario should be the mascot for data engineering. A plumber yes. But also the unsung hero of ML projects.\nWhat, do they think we lack the creativity that comes with their job description? Ask yourself, do you want a \"creative\" plumber fixing your pipes? By the way, DS has the horrible tendency to fail, like, nine times out of ten, due to bad data quality. But you only know how good or bad the quality is until you've trained the damn model.\nPrestige is a bit of a dog whistle term.\nAs someone who has done a bit of data science, it just says lack of data engineering support is a factor. The fact is most AI projects will fail, whether they have a good team or a bad team, since most projects are just not improved by adding AI. The number of times I've been in a session to brainstorm AI use cases, and every time we think of something, it could be done nearly equally well or even done better, by a conventional method that we haven't invested any capability into developing or learning.\nI had this conversation today. A data scientist was disparaging us DEs because they didnt think we were agile enough. Ok. Lets see you try to get 14 billion rows a day into the edw\nIn a cost effective and reliable manner so we arent fixing shit all the time and giving bad results. And damn, I was proud of my 2B rows a day ;).\nask 'em to do it themselves in their notebooks and see how they go\nI had this happen to me, which was rich because the person constantly crashed our machines because they didnt understand how memory worked. Theyd try to pull in terabyte sized tables into a dataframe and then blame me when it didnt work\nOh yeah. One of the things I did in my last job was to teach the python developers to use sql to get their data in sets instead of reading it straight into data frames\nYeah. Well. They aren't ITIL enough!\nThis is the eternal problem though. When the need to react arises, data engineering is too slow. To me a data scientist sits on the other side of the bimodal delivery model. I could happily sit and complain about code handed over from a data scientist, but at the end of the day I see them as business users with enough knowledge to get the job done. Do we enable them with curated data to start with, of course. However the business needs both of us.\n> One interviewee referred to data engineers as the plumbers of data science. Kind of funny, because I often explain my job similarly. It also fits in nicely with the saying \"shit in, shit out\".\nAnd you definitely know when the plumbing isn't working!\nI'm a data analyst that works very closely with my engineering team and toying with making a career pivot to DE. That said, the article is a no shit Sherlock for me. Most firms can't even get basic descriptive analytics and BI tools correct so they sure dont have a good chance with anything more complicated like AI.\nStarted as a DS over ten years ago, and fairly quickly discoved that DE was the safer way forward. During the big ML hype of 2015-ish we had so many dumb projects fail miserably because managers thought it would magically solve all problems (it didn't). We're currently reliving that whole period again, but now it's GenAI. Luckily however, in my role as a DE tech lead I can mostly stay out of it. I still have massive respect for the select few DS's with deep knowledge of the field, but the vast majority doesn't do much more than import pytorch, and mess up my infra and cloud bill.\nHow do you stay out of it. More demand for data science and AI means more demand for data engineering.\nTrue! However, in our case most of these proof of concept projects are chat bots and stuff. So it's mostly custom prompts and calls to the chatgpt API. Some are a bit more fancy and vectorize a bunch of documents, but as we're mostly on Azure, they have services that make that very easy.\nI was going to write a whole post but this perfectly sums up my opinion\nI have got a phd. in AI research before moving to working as a senior ML engineer at a fortune 100 before finally making the transition now into doing data engineering at a startup company. Honestly, for me, I find it much more enjoyable than pure ML work. Being able to design how and what data is captured, deciding how it is ingested and stored, and how it gets displayed and visualised to the users feels much more powerful. Plus, it makes it easier to spot opertunities for ML, which can provide ACTUAL tangible business value, and then I can actually deliver it. Also, tbh training models isn't actually that difficult if you know what you're doing. We recently worked on a CV system for object detection on boats. Training and integrating a custom model took 1 day. Developing the pipes to get that data into the model over RTSP, connect it to other sensors, and provide hooks for downstream decision processes took weeks, and that is still just a prototype.\nI'm not as experienced in ML itself so can I ask how the custom model development took only a day? Did you mean just that running the training process took a day not including the time for designing and writing the model from scratch?\nIn that example, we trained a Yolo v8 model using the ultralytics library and an open source dataset from Roboflow. That only takes a few lines of code in a Jupiter notebook to get going. The training took a few hours, and then, when it was done, we checked the validation metrics. If they look good, test it on a few videos to see. If that all looks good, you just lift the final model binary and place it in an artefacts directory near the code. Then, within the deployment code, you just load the model weights from the binary into a model object, and then you can pass images to it to run inference. In that example, we may need to redo the model due to licencing issues from ultralytics. But for other types of models such as sklearn or pytorch models, it's a similar process. The difficult part is generally getting enough labelled data and getting it in the right format for your model to train with. Aditionally understanding how to perform validation and the different metrics (i.e., checking accuracy, precision, recall, etc) and setting proper confidence thresholds for your use case is important, just so you dont put out something that is broken or doesnt work. I'm not saying there is absolutely nothing to it, and people can and do spend weeks tuning every last parameter of their models and inventing wacky ml pipelines to get extra performance, but, generally, if you have the datasets at hand then it doesn't take too much work to train an ML model and have it as part of your pipeline.\nAh I see what you mean, thanks for the detailed explanation.\nHow is your job as a ML engineer? I am asking that because I am DA/DS who wants to transition to ML engineering. But I also think about DE sometimes too haha So I am bit confused and I would like to understand how is your currently job and your motivations for the transition\nI think ML engineering is close to both, but also frustrating because of that. For example, one of my ML engineering projects was on building a sentiment model. Our code basically took as input a JSON call transcript, split this into utterances, performed sentiment analysis on each utterance, and then packaged this back into a JSON and sent it on. The frustration came from the fact that we spent ages making this amazing sentiment model which outperformed every vendor solution we benchmarked against, but we had no clear understanding around what data got scored for sentiment or whether it was actually being made use of by downstream applications. Eventually, it became clear that nobody else in the company really cared whether the service was up or not. Most of our teams best products tended to be visualisation tools that just made sense out of data that was already there, or that would replace some expensive vendor application. The one that broke me, though, was when they refused to spend a modest amount of money to get the compute needed to run a side by side comparison of a fuzzy search solution that we had built against their current vendor. We had passed all the internal KPIs for the POC and MVP around performance and accuracy, and our analysis showed it would have saved them about $50m per year in vendor costs alone if successful. After that I said fuck this i'm out. I would rather be responsible for the whole pipeline and actually have control over how much impact my models will have than slave away on another cool solution that never gets used. But that's just my experience, I think company size has a lot to do with it too though.\nIve met many DS that transitioned to DE Met 0 DEs that want to be DS lol If anything I feel a lot of times DE look down on DS because they try and solve a problem with some crazy ai idea while the DE just uses sql to solve the same problem\nI think its easier for someone with a math background to switch to engineering than it is for an engineer to switch to a maths role. So this could explain it\nI'm a data engineer who was an SWE before that. I am finishing an MS in Economics from a Tier 1 research university and have learned a lot about data modeling from an economic perspective. I want to pivot to being an Economist who can model and program. In a sense, that's a data scientist, but I am targeting a job where they want someone with economic knowledge who has more acumen than just knowing how to program in R / Python.\nAlso an Econ Masters here from a similar route though I got into SWE after. It really just made me realize that the people who could really do both would do some incredible things. And they do, they just get paid really well by the hedge funds they work for to keep it to themselves\nSo, a data scientist with focus in a specific domain?\nIs it a lack of data engineers, or is a lack of data engineers a symptom of management teams that do not see the value in everything surrounding and supporting data science type work, instead they just want GenAI to replace their workforce right now and dont want to hear about all the technical mumbo jumbo it will actually take to make that a functional reality?\nI yet have to meet a real Data Scientist. They're all BI developers with a fancier title. I've also yet had to work for a company that uses data science properly.\nOur AI data science guy literally just feeds things into ChatGPT and only uses generative text to solve every single problem. One hammer, and every screw, bolt, and welding joint has become a nail.\nMy old employer had some 40+ data scientists. I never saw them produce anything that wasn't a slide deck. The only machine learning model in production generating meaningful revenue was produced by a team of software and data engineers. No data scientists.\nOne of the authors of Fundamentals of Data Engineering, Joe Reis talks about it eloquently in the first few pages of the book. He has trademarked the term Recovering Data Scientist.\nData Scientist. Aka rock star who can sleep under their desk and work tirelessly until some nonsense idea becomes practical\n\"plumbers of data science\". People should visit places with really bad plumbing infrastructure. In China, nearly every washroom smells like decade old piss, even the ones in private homes.\nNext article Businesses want to replace 80% of Data Engineers with AI.\n> The goal for many data engineers is to grow their skills and transition into the role of data scientist lol. lmao\nData science is meant to predict the future and help business leaders make profitable decisions. Yeah, that NEVER happens. Profit comes from treating your employees and customers right, and by _trying_ to stay ahead of the curve. Also, companies need to accept that sometimes other companies will beat them. Apple beat Microsoft in smart phones. But Microsoft is still a very profitable company.\nI'm a \"full stack\" data scientist and the data engineering part of my job is the hardest and most rewarding. But I get paid more for the easy models I spit out at the end, so that's what I do.\nWhere do you get 80% from? As in, why not 90% or 70% or whatever other number?\n\"more than 80 percent of AI projects fail**twice the rate of failure for information technology projects that do not involve AI**.\" But to your point, I would be curious where they came up with 40 for IT projects.\nNope I love being DE. I want other people to do smart stuff with the data I deliver to them.\nHow many should succeed, and how many are simply ill-conceived?\nDon't consider it as a stepping stone. It really underpins almost all technical aspects of a business - apps, BI, AI, network monitoring, security, etc. Most companies are misguided as their focus is on their core business but forget they cannot survive without their supporting structures. It is a matter of how it gets recognized and where. Many services companies are starting to make Data Engineering a core focus of their business for other companies. I think it is a matter of where we, as data people, where we want to be to make the most impact.\nbut, I thought with AI you don't need these expensive people??\nThats what AI has been selling right? AI will do away with the need for data engineers or programmers\nJust need the AI to make and deploy the AI for us oh, wait\nsurely for that theres AI?\nGood, I hope it gets an even worse rep, the less people doing it the better for the DE market\nBased on the findings, I would have thought hallucinations were the leading cause for failures. It's good to know some of us can still hallucinate freely.\nAs much as I thank you for putting this article up, the article does list this as the first reason for failed AI projects: **Our interviews highlighted five leading root causes of the failure of AI projects. First, industry stakeholders often misunderstandor miscommunicatewhat problem needs to be solved using AI.** This doesn't seem like a data engineering problem as much per se, though the other four root causes appear to be.\nI'm a new DE whose original plan was to gain experience in DE before moving on to ML or DS roles. As I've gained more knowledge and awareness of the data world, I'm glad I chose DE because I may just end up staying on the DE path and dabble in ML and DS in home projects. Giving presentations and briefings were what I dreaded most at my old job, so that's probably what pushed me more towards DE instead of DS.\nWhat you capture in your post is something [I've predicted in the past related to research](https://executivedecisions.substack.com/p/will-ai-be-the-end-of-employees-and). Research is very unsexy, yet without research, you don't have accurate information. Data engineers are the \"researchers\" of data. When you speak to data engineers, you realize how well they know their data. These positions are easy to gloss over as an executive. Yet once you realize how one tiny inaccurate data value impacts a data set, which will impact AI's affect, you realize the importance. Details matter! What's funny here for all data engineers is that you have first insight into company's who approach their data well. This means you know the organizations who will leverage AI. But the inverse is also true: you know the organizations who do NOT have good data or approach their data carefully. Those companies are headed for trouble. Research and related industries (what data engineering is with data) will only become more important with AI.\nAI project doesnt always mean training an AI model. you dont need data/data pipelines for calling pretrained models via API and integrating it into customer facing applications (using something like RAG or function calling)\nEven though the term, Data Engineer has been around for 10 years or so, I find it interesting that it now seems to be a mainstream role that software developers are flooding towards since the AI movement. So much of this was just expected of Software Developers.\nI've been writing and talking about this for the past ten years or so. Yes, there is a much higher rate of failure.\nAnd just like plumbers, electricians, carpenters, and the trades, Data Engineering will always be both essential, stable, and well-paid.\nThis happened at my job and we werent even doing AI. Everyone was concerned about how the dashboard looked and what it did but no one was concerned about the underlying data that I was getting.\nJup\n80% of AI projects will fail because they're stupid.\nNo. They fail due to not having any specific objective except for FOMO\nNot because of data engg. Because we want to solve all problems with ai\nMost data scientists Ive met and worked with do sweet FA. And of the FA they do, a tiny percentage of that is useful (usually none of it). As much as data science is an interesting field, I wouldnt want to be one.\nDo we even need all that ai I'm not an ai hater or any thing but like 25 extremely powerful ai seems good enough to me\nThis highlights a major flaw in centralized AI projects. FLock addresses this by decentralizing the entire processtraining, data collection, and even governanceso that the community collaborates and contributes. This way, expertise is shared, and reliance on a few individuals doesnt cause project failure.\nWhat kind of rubbish is this, why would a data engineer give up a more interesting and higher paying job to be a data scientist and get paid lesser and constantly fiddle with data..\nBeing a data scientist seems lame to me",
        "content_hash": "b4e72f817e05283a77d85a88d546727f"
    },
    {
        "id": "18ffzmx",
        "title": "I Was Happier Being a Bartender Compared to Being a 6 Figure DE",
        "description": "Used to be a resort bartender during college for 4 years. Been a DE for 3 years. Went to college for Information Science, graduated, got a job as a data engineer making $85K. Since then I\u2019ve been working on my MSDS part time, and 3 shitty data jobs later I am making $110K. \n\nData is interesting at its core but the industry ruins it. Layoffs everywhere, shitty tech stacks, bad stakeholders, boomer bosses, bait and switch job opportunities, remote roles being replaced with 100% on site. \n\nOverall this career path has been the worst decision of my life, despite looking good on paper. To say my life has declined in every way since going from bartender to DE is an understatement - I am absolutely miserable. Used to have friends, go on dates, socialize all day, have tons of free time. All of that is gone in favor of long hours, boring shit jobs, leetcode, living with parents after being thrown around the country for different toxic roles. \n\nAnyone else go into data and realize the grass wasn\u2019t greener? I am dropping out of my MSDS and leaving this industry in January. This whole journey was a waste of ~6 years of work. ",
        "score": 542,
        "upvotes": 542,
        "downvotes": 0,
        "tag": "Career",
        "num_comments": 175,
        "permalink": "/r/dataengineering/comments/18ffzmx/i_was_happier_being_a_bartender_compared_to_being/",
        "created": 1702249347.0,
        "comments": "It's not for everyone for sure. Seems like most corporate jobs kinda suck ass these days compared to when I started. Downside is often the most rewarding jobs don't pay much.\nLol whats worse, corporate or academia?\nAcademia, hands down\nAcademia is so much worse it is not ever a question.\nSeconded. I shake my head at the years wasted there.\nElaborate on why academia is so bad. I hear that all the time but have always wondered.\nIm not saying the opinion is wrong, but it is worth mentioning that most people saying this have absolutely no experience in academia. Academia can be incredibly rewarding for the right person.\nAll the bureaucracy, twice the egos, and no money. No money for employees. No money for the tech stack. Just no money\nFor me no 1 was low low pay. It was ridiculous.\njust as much bureaucracy yet you have low pay\nThis is burnout. Your complaints are not unique to data and I think you would find a similar level of disappointment in any corporate role. It might be best to take an extended vacation or leave just to figure out what you want to do with yourself. There are subreddits like r/findapath or r/careerguidance that might be helpful for you. Also keep in mind that most people in the world would exchange misery for what you say you are earning.\nThe truth is though that any \"path\" (read career) that you find has strong potential for burnout in this manner. Simply because working 40+ hours a week in any field is rediculous.\nIm salary. Get paid for 40 hours to work 20. No one cares cuz I get my work done and I have high performance. Unlimited vacation within reason. The careers are out there, you gotta find them and work hard to find the chill places\nHighly recommend the nordic countries for better work-life balance. Im working 30-37,5h per week in Finland. The compensation will probably be lower but raising kids/getting sick etc a lot cheaper.\nTheir visa laws are pretty strict. I used to want to move to Sweden and the problem was that even after 4 years on employment visa, the government could just choose not to renew it, and youre SOL if you wanted to apply for citizenship at the 5 year mark. :/ this was what I found in 2020. Has anything changed?\nAFAIK they're not particularly strict. I'm not 100% sure, but I imagine for Finland if they don't have a reason to deny your request they can't deny it. Reasons to deny extension to the working visa are at least 1) breaking major laws (speeding tickets do not count, for example) 2) being unemployed. Remember that it is a lot harder to be kicked out from a job in Finland than in the US. Currently the law requires there to be a \"good and heavy reason\" to kick you off, however our current government is trying to change this to be just \"a good\" reason, which would probably lower the bar substantially, but no-one knows how much until someone is kicked and they go to the courts. 3) or not paying your bills. However, Finland's current government has the major anti-immigration party in it, so expect the immigration policies to tighten. They probably will not touch the work-based immigration too much due to the pressure from the rest of the ruling parties, but getting the citizenship might become a more arduous process.\nYou need to speak their language or nah?\nIf you just want to work here for a while, it's not expected. You can live comfortably in the big cities with just English. Sometimes if you try, people will switch to English to make it easier for you, which might make learning the language hard. Usually if you want to be a citizen you'll have to demonstrate some ability in the country's language though.\nI dont even know if its the number of hours going into work or just the number of hours dealing with corporate BS. When I was in college I could get an awesome flow state from doing work, cant get that in my job nearly as often because every hour I get a ping with an urgent question. OP mentioned they were a bartender - hours may have been longer but far less draining.\nI think thats the key dealing with corporate bullshit, might be a good portion of that 40 h work week and stressful.\nTo piggyback off of this comment, if you're on a team and you're not feeling like you're making an impact (can't see the results of your work) AND/OR you're surrounded by people that DGAF about the work... could be the reason why you're feeling this way. People are social creatures. If your team doesn't care, it's gonna be hard for you to care. Burnout is burnout, regardless of the job you do. Bartending, I think, is more of a 'direct' job where you can see the impact and results almost immediately.\nVery good point. I think Bell Canada fits that DGAF syndrome.\nDamn, sort r/findapath by top for a dose of positivity and motivation\nHere's a sneak peek of /r/findapath using the [top posts](https://np.reddit.com/r/findapath/top/?sort=top&t=year) of the year! \\#1: [I don't want a job. I want enough money to retire and curl up in a ball and sleep.](https://np.reddit.com/r/findapath/comments/179pafw/i_dont_want_a_job_i_want_enough_money_to_retire/) \\#2: [I don't know a single adult who is happy with their life](https://np.reddit.com/r/findapath/comments/15tpptq/i_dont_know_a_single_adult_who_is_happy_with/) \\#3: [How can you work 8 hours every day for the rest of your life at a shitty job and not end yourself?](https://np.reddit.com/r/findapath/comments/154tnc0/how_can_you_work_8_hours_every_day_for_the_rest/) ---- ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| ^^[Contact](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| ^^[Info](https://np.reddit.com/r/sneakpeekbot/) ^^| ^^[Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/o8wk1r/blacklist_ix/) ^^| ^^[GitHub](https://github.com/ghnr/sneakpeekbot)\nI agree that it can get grim but it helps put things in perspective. It doesn't matter where you are in life or what you have accomplished. Burnout gets everyone and OP's post looks very similar to several other posts in the subreddit. Blind, for all of its faults, is also really helpful as a place to let some of it out.\nTop of this year? Theyre all incredibly depressing. I must be missing something\nI was being sarcastic\n>I think you would find a similar level of disappointment in any corporate role Bait and switch accountants? Marketing? Lawyers? Hell no! We're the only industry where you start by applying to a \"use language X\" job only to find out that it's actually Y with X being 5% of the time.\nI think that's a little naive. I think most people don't know what they're in for in most jobs. And you figure it out as you go. I think this only gets worse as you go up the corporate ladder.\nIts called the facade and its taken over the whole world \nI live at the intersection of data and finance, there are 100% bait and switch accountants. Im not 100% sure but lawyers can probably get it too. I experienced it when I was in finance. Companies can always be shitty. Bait and switch accountants are told they are doing X job like audits, and MIGHT do a little Y like financial reporting, only to be 100% reporting. Or Vice versa. Lawyer is going to mostly be a litigator turned into doc review.\nI really dont think that the DE career is the issue. It sounds like the transition from college into the workforce has been a bit rough for you, but I could be completely off. You sound like youre suffering from burnout, which is understandable given the current employment conditions in tech. The question you should ask yourself is whats next? A different corporate role? Are you planning on going back to bartending? Id love to hear more on your thought process on whats next. Ive found that working with data and helping leadership understand issues and how to solve those issues gives me a lot of purpose. But there have been other positions at other companies in the past that I hated getting out of bed to go do.\nI disagree. The tech industry is largely praised for the financial freedom it provides people, but at the same time lots of people are looking to leave it due to pressure, on-call response times, etc. A career that works for someone doesn't necessarily mean it's better for everyone else, and that seems to be the case for OP. Nothing wrong with it.\nTotally. Very social people , people who dont want or like stress , dont necessarily enjoy corporate America . Because they can be making 6 figures but the long hours and on call, kill the joy .\nThe material conditions of work have an effect on the laboring class. And in the case of knowledge work; the 20th Century style corporation with offices and CEO's and boards that meet in actual boardrooms is pretty much backwards for effective work. Hierarchical, regimented, with all of the risk on the worker and most of the reward going to management. It is possible to envision a world where the work that humans do is exploratory, occasional, focused on extracting outsized rewards from fine grained changes in production methods. And where the benefit accrues to those doing the work by virtue of shared infrastructure and resilience to climate change.\nI agree that people should find something suited to them. I disagree with the characterisation that the toxic, unsustainable work culture that many adopt is the only way to succeed, or even something we should accept. The most successful people in this career tend to find a balance that they can sustain for the long term. It's generally not true in my experience that people who value work/life balance just get replaced by those willing to grind. Most of the rockstars burn out. The slow and steady people end up as staff engineers with smiles on their faces (some of the time).\nNot sure whats next, but not this\nI would recommend that you try to take some time off and disconnect. The problems that you listed out in the original post are largely not about the data engineering work itself, but work life balance issues. Unfortunately, once you leave college a lot of that free time is consumed by work and other commitments. Its an adjustment, for sure. Not only are you working full time in a stressful job, but youre concurrently doing a masters program. I empathize with you because, I too, did a MS program in Data, while working full time. See if you can take a semester off and youll get some social time back that way. Also, try to learn to professionally say no to more work when possible. Sometimes you have to manage up with your manager to set up realistic expectations and timelines. As others have said, I dont think moving into a different corporate role is going to make a difference with the problems you have listed.\nIf you go back to bartending wouldn't that then be more of the same without the fiscal liquidity offered by the higher salary. You've made financial commitments you can't escape if you have student loans. I've worked for companies all over the country I 1099 after my day time job and I've moved once for my kids to have better schools 15 minutes east. Anywhere that wants to employ me is remote first and ask what guarantee is there regarding remaining remote. Also got a severance guarantee, where if I'm laid off or they recall everyone to the office I get a year at 75% of my salary. When you apply for a job the audition goes both ways.\nyou have to accep that reality\nThere are work personality assessments that you can take. Similar to a personality test but it focuses on how your personality interacts with your career or even vice versa. Ive taken two over the span of about 20ish years plus a regular personality test in high school. In your situation, I highly recommend taking one. It grades your personality in different categories. Theres no right or wrong. But then it goes on to tell you how certain aspects of your personality mean you will likely be happier in certain roles, fields, etc. I highly, highly recommend taking one. Its really a great tool. You may find that a different sector of tech is a better fit or that tech altogether is a bad fit for you. When you take it, try to do so on a day where you have at least rested a little and try to just be yourself and give honest answers.\nAs someone neurodivergent (autism spectrum and transgender basically) the transition was pretty terrible. I was a complete mess every day. Completely exhausted. Remote work is the only thing that saved me.\nIf you like data at its core - then I'd suggest considering another job. Just be very careful about vetting the company before joining: * Make sure the culture is great: talk to engineers on team, ask about the on-call process, the hours, the process, etc. * Make sure the tech stack is great: make sure you understand the implications of their technology - sql transforms may mean wading through thousands of lines of SQL to trace a data quality error with no unit tests. python notebooks means you may have goofy errors due to scoping. Azure means a focus on the microsoft stack. * Make sure the people are great: that you can learn from folks, that they value people over process, that they work to ensure onboarding is smooth, why prior staff left, etc. * Make sure you get to have fun with the data: given that you like data, make sure that you have the opportunity to learn the data, and come up with creative ways to use it. Because if you really enjoy working with data - this can be a very fun job. If you're in the right team.\n*\"I am dropping out of my MSDS and leaving this industry in January.\"* Let's just take a minute and talk through this. It sounds like, despite frustrations, you're making it work. I'm going to try to convince you not to throw it away. I had my own data science burn out episode in 2015. It was towards the tail end of the \"Big Data\" craze. I thought it was fake, and I would do something real: build houses. I spent a year doing construction and eventually got on a framing crew. I bled every day, maybe from hitting my thumb with a hammer, maybe from a splinter, maybe from falling down or something stupid. I was terrible at it. I moved to a converted garage and still spent more money than I made. I thought there would be some special pride in building houses for people, but everyone was run down, man, just trying to make it another day so they could eat and send a little money back to their families. I got a data science job when I came back, but my trajectory never quite recovered. Life is hard. Work is hard. But I think if you have a skill set that is valuable and people are willing to may you good money for, you should try to make it work. I don't know how much you can make as a bartender, but I'd ask whether that job would offer the same experience as it did 5 years ago, since both you and the world have changed.\nAgreed. The grass is always greener on the other side. If possible always try to find a way to try your alternative as a side hustle or if possible, a hobby. If you can't make it work as a side hustle or enjoy it recreationally, good odds are you won't enjoy it any more professionally.\nWait, so youre telling me that being a part time bartender at a resort while your only other responsibilities were college courses gave you more free time than working a full time job as a DE? Color me surprised. Its good that youre making moves to be happy, thats the most important thing. Just make sure you know that being in the workforce full time doing anything is gonna be a grind. No matter what you do, youre going to need to advocate for yourself within these jobs for work life balance. Good luck and dont give up!\nWhy not going into roles where you can still leverage your education? Perhaps product management or marketing.\nIt's sad to see CS-related subs flooded with self-harm and mental health posts (not this one, but more broadly over the past month). There are other roles out there that may be worth trying too.\nAs someone who loves problem solving and tinkering with things, and was literally programming since I was 5, this doesn't protect you from struggling with mental health while being forced to work in \"agile\" teams\nHow good is high throughput waterfall?\nEhh, maybe this is true for bootcamp grads or the like, but it doesnt sound like their background. And for what its worth, I am in this field, have been programming since I was a little kid, and echo a lot of the sentiments in here. Its not that this person doesnt love tinkering enough. There is little overlap between the traits of wanting to tinker with systems/solve problems and being skilled at managing expectations of bosses and clients, wanting to play the political games of a corporate career etc.\nThis is the result of social media telling young kids having fun is the essence of life. Wait till they find out what's it like to get married and raise their own kids.\nHaving fun *is* the essence of life. If you're not enjoying your life, why even bother? You need to get your life to a position where you're enjoying what you're doing, and enjoying time with your kids.\nWhy would you get married and have kids if it's no fun? What do you gain from it\nCompletely agree with you, it sucks! Love programming in general, DE, ML etc. But the jobs just take the fun out of it. 9/10 times your talking to people who throw around buzz words to sound smart and have no idea what their talking about. Not the mention, the ridiculous deadlines. It's never ending work/re-work which eventually burns you out. However, don't give up on your MSDS. Keep going and become even more skilled, then leverage that skill to create the lifestyle you want no the one forced on you. It won't be perfect but it will get better.\nAgree. The technical work can be interesting and fun, but it's all the people stuff that is endlessly draining.\nI understand this is probably a vent post and that's okay. So maybe you're looking for some encouragement despite maybe being a little heavy handed. Keep your head up. My past 3 years in DE have been amazing. I loved my job 9/10 days (just got laid off from that job, start a new one tomorrow). I've been in a place where I hated work and it's exhausting. There are good jobs out there. And if you find it to potentially be a burnout or depression issue, don't discount that either. I find life significantly improved after I got on some medicine and did all the \"right\" things on top of that.\nHow was the job hunt in this economy?\nI had to take time off from the corporate environment, started a real estate development company then realized that DE is actually a really good gig if you can land a role where you get paid and aren't working crazy hours, then use the income to support another activity.\nTaking time from corporate world can give you some clarity. I took a year off to live abroad and work part time on whatever Jon I could non 8h corpo, and realized after a year I wanted my desk job back. But it could be different for anyone. Stoping the ball and taking some time is never a bad thing if you have the means to do it. Rat race people, they never stop, and never seem to be happy.\nHow did the RE company do? I am looking to move out of computer jobs (or at least have a side hustle not fully related to coding), I was looking into RE/realtor/property management lately\nIt went well until interest rates changed, but I'm going to keep it going on the side. Also it is a really nice break from being behind a screen. You get to interface with people, build things. Can be stressful if it is your only source of income.\nHave you tried a smaller company where you have more room to work on what's actually important? It might come with a pay cut at first, but the work is vastly more stimulating and fun, not at all like the uphill battle against insanity that most corporate data jobs are.\nChiming in and saying I went this route and the projects definitely have more of an immediate impact in the business. Comes with some different challenges of having potentially more on your plate and very little data culture in place but its been very rewarding comparing to my previous F500 company.\nIMO all about finding the right company..Ive worked at five companies now. Been absolutely miserable at two, indifferent at one, and the other two Ive been really happy, grown a lot career wise, made friends etc. I dont think I did anything special to find those two companies, just luck of the draw sometimes. Reason I bring this up, maybe you picked the right career path and just got unlucky with three bad gigs. Id say keep trying..a lot of companies out there with good work life balance, socialization, tech stacks and engineering culture, etc, just need to find them and some degree of luck\nLove the work I do, love the pay I make, hate the personalities of the shitty employees and shitty leaders that live in corporate America\nI completely agree with you. I'm a hard worker, love the work. But the culture is crappy. It's dog eat dog.\nIf your finances allow, I would recommend quitting your current job and focusing on the masters instead. Like others mentioned, you sound burned out. Perhaps there are classes that look interesting to you and that may help you explore other career paths. Take care, OP.\nWhats your salary being a bartender? Whats your future prospect in that profession? Were all happier with simpler jobs, but those jobs rarely pay for lifes expenses.\nSounds like you are burned out. Have you considered taking employment as an expat? I also bartended my way through college, but after some networking got my first job as an expat. Fast forward to now, I've lived a fulfilling life, travelled around a lot. I made so many international friends that whenever I revisit a country, I always have a place to stay. Currently working and living permanently in Japan where my international salary really goes a long way. Consider taking the expat route and travel to decompress.\nMy company has a huge presence here in Japan and back home, so they ensure I have the right paperwork to \"lift and shift\" per se, seamlessly. I still have to workout other personal related taxes I own assets etc, but it's not much of a hassle. Kids are great! I wish I had them. My partner and I haven't had that discussion yet...\nMy mentor always says , there is nothing called job satisfaction. Pick your job with your head and hobby with your heart. I dont like my shitty DE/Analyst role, but it pays decently well and keeps me comfy. With your continued education and curious approach to life I hope you are able to solve these problems with the new found/learned knowledge.\nNot sure why people are saying youre burned out so definitively without really knowing who you are and what your interests and strengths are. It could just be youre not meant for the office and that youd thrive more while working with people, not sitting at a desk. If thats the case, now you know. Good luck!\nhttps://medium.com/life-lemons/what-you-can-learn-from-a-mexican-fisherman-a8334882204c\nAmazing one, never fails to amaze me how we fall so easily on the rat races of life\nSymptomatic of burnout for sure.\nYeah, I'm a software and machine learning engineer that moved towards data over the last 4 years. Making a plan to exit it now. It's pretty dull compared to what I could be building.\nWhat could you be building? Do you mean other types of software? Or completely outside of software?\nThis is such a 20 something take on the real world. \"Work sucks I have to deal with an inordinate amount of bullshit constantly\" Newsflash, any real job you take that pays over six figures is going to suck, it's either going to be stressful, boring, pointless, bureaucratic or a combination of all of the above. Find a hobby that makes you happy outside of work and don't derive anything aside from a paycheck out of a job.\nI've been in a similar situation to you a couple of times. Sometimes DE work can feel very... Pointless and meaningless. I think this is excaberated by being a very sedentary job at times. The truth is, \"wherever you go there you are\" -- we need to fashion our own happiness sometimes, and in DE work a lot of that means seeking out social interaction and healthy movement since the job won't automatically provide it. Do you have a social outlet outside of work?\nNo social outlet, have had to move across the USA multiple times for work, now stuck back in small town\nOk I really sympathize with your post. Just wanted to say that youre not alone and working in any tech related field can be like this. I'm also going tru the same (being a data analyst) but unlike you, I have never been a bartender, which sounds really cool on paper too lol\nStart a bar. If you find yourself building an analytics pipeline for your pour and your hospitality costs and a predictive model for booze orders... that's when you know it wasn't the work it was the companies involved. As an aside: Does anyone else notice how many mandatory lifestyle upgrades come with that 6-figure salary? That is not a coincidence.\nWhat a privileged take lmao\nIt's called getting older. Everything was so much better when you were young and had your life ahead of you.\nBro - this happens in every professional industry- engineering, accounting, marketing etc. Personally Ive had a bunch of shitty jobs in different realms after my awesome job in college being a lifeguard. But I finally found a gig as a DE at a great company with great worklife balance and love it. Still deal with bad tech stack and consumers who dont know what they want, but thats my jobenhance the tech stack and to build relationships with my consumers to better understand their needs and the data tasks that will make my company more profitable. Sounds like you arent cut out for life in an office. Thats totally cool. Sounds like you enjoy bartending. You can make good doing that. Go back to that.\nYou've made a lot of threads about being unhappy. A lot of people make career switches and regret it. If I'm being blunt though, [you have already mentioned your issue](https://www.reddit.com/r/dataengineering/comments/186aeh4/anyone_else_stuck_in_a_bad_job_how_do_you_cope/kbana5e/). Your options are: * Take a break, accept what happens after. * Take a break, change career. * Carry on making threads on Reddit instead of focusing on improving your life. You're young and have plenty of time.\nI remember. I waited tables tendon bar. It was one of the most fun times of my entire life.\nCome work at AIS. Permanently remote for decades and we need Data Engineers bad!\nI can relate. Had similar feelings. I kinda realized the disappearance of friends had more to do with getting into my 30s where this pretty much happened to everyone. People have less and less time as they grow up and start their careers and families. I now have a small core group of friends I keep in touch with, but the days of having dozens of people to party with are long gone. Replaced with a few high quality friendships that I would not trade for those party days. My job is tolerable, but my coworkers fantastic. Thats what I focus on at work. If you dont love the people youre working with, youll find that the job is not completely but almost completely irrelevant to your happiness with your career. Also, I began focusing on my family and core friend group a lot more and making sure I can be a part of making their life better. It took me years to get here, but focusing on loving and helping my small circle has taken my job from being my purpose to just being a means to an end of helping out my loved ones. Being a contributing part of my little mini-community thats my purpose. Ive never been happier. With my position in life. Hope this helps friend.\nI did the opposite. I had fun low paying in forestry until I was 35. I had a few jobs that weren't really that secure. I started looking around at my older coworkers who were scared, old, and broken with a limited skill set. I went back to school and got a GIS certificate and a MS in Forest Biometrics. Now I am mid 40s making a ton of money and I can get a job easily. Playing with data is not rewarding for me. It's just a job. Hiking in the woods as a Forester, is rewarding and fun, but there is no money, it's hard on your body and you can always get replaced by a 22 year old for 1/2 the pay. I know I sold my soul. There is no way I could have sold my soul at 25 or 30. You have a fantastic skill set don't waste it. Just get a more laid back job.\nWelcome to adulting son!\nthen go back to being a fuckin bartender the less guys we have picking up bottom of the barrel work for peanuts the better the rest of us get paid\nI could not agree less. for me DE has resulted in me making more money than I've ever made, better work life balance than I've ever had, working remote most of the time, and more. I work to live not live to work.\nYou can't compare a real job with a fun job. You will get the same or different disappointments when your bartender job turns to a real job.\nWas a real job, did it 40 hours a week\nMove back bro, if your heart lies there but be cautioned if you felt same there after switching\nAre you sure it isn't all the partying that you miss. I mean comparing being a bartender to a DE sure one sounds more fun than the other. I was also a bartender before becoming a DE some 10+ years ago, and I feel exactly the opposite.\n[removed]\nYour comment/post was deemed to be a bit too unfriendly. Please remember there are folks from all walks of life and try to give others the benefit of the doubt when interacting in the community.\nWhen you talk about 'boomer bosses', it's really hard to take anything you say seriously. That's so immature that it makes it impossible to feel sorry for you. The industry will be better off without you.\nFound the boomer\nNot a waste, at least you can fall back on this if needed, or while you find out what will work better for you and your life balance.\nYou're still qualified to be a bartender, I'd just go back to that career path if it made you happy\nIt sounds to me like you're just at the wrong companies. Senior Data engineer consultant here. I have unlimited PTO, 100% health insurance coverage, and rarely work over 40 hours. Not the most I can make with my LoE but the work life balance is amazing.\nConsulting is the way to go for mid to senior level roles. You assume more risk but it typically pays better and once you get better at defining the scope and expectations of each gig, then you can select the best opportunities, VS. being on salary and having no control.\nAny 9-5 corporate jobs will keep you from socializing all day and limit your time. It isn't easy, but it is a part of adult life after college or trade school training. Just save and invest as much as you can and set boundaries on weekends. Memes and GIFS helped me endure.\nRespect that; you have made your choice in life. I think most of people are just trying to get a job and survive with it.\nI felt that way about corporate finance. I think something is just wrong in the fabric of our society in general.\nHave you thought of just paying off a house and your debt then just go back to being a bar tender? If you invest like 100k into an like the s&p500 ETF and just reinvest that over 30years you end up with a about a mil. Could just live off your bartender salary with no debt then.\nAny tips on getting your first job? I also majored in Information Science, but my school pairs it with data analytics.\n1.you had long hours as a bartender 2. That's on you for working more than 40 hours. Don't let the company guilt trip you into more. Log out and go on dates. Ez.\nThere are many applications of data science and data engineering you can bring to other fields Maybe consider which industrys have been underserved by DS/DE and find a interesting path that combines the skills you already have, to a path you have not taken.. Dont discount the skills youve built, the world is filled with opportunities that need you and what youve learned!\nI would say hang in..early years can be tough but as you grow the responsibility and roles change. If you are not finding personal time, talk to your boss for loss of pay extended vacation or a short paid one. Learn to say no, it works. Not sure how all your roles are toxic, zero down on what exactly is not to your liking.\nBro of course you where young and carefree. Now youve seen the data and didnt like what you discovered.\nI worried less about my finances when I whenever I went through a period of prolonged unemployment (both voluntary and involuntary). No kidding.\nAlienation of work. It's about the corporate world and probably about the loneliness and social life of becoming adults.\nI'm bartending now and am headed back to school and looking at a DE path... I'm going to miss this industry for sure but it really wears you down in too many ways. I don't think I'll ever be the perfect code monkey, but like yes - I'll miss the good socializing / other parts of my brain getting used. I guess I'll figure out how to get both at some point.\nHave you tried working in Higher Ed? Things are much more laid back and the pay is on par with the market. I experienced the burnout youre feeling working at a financial services giant and a startup, but now my stress level is at an all time low, and Im still remote. As a bonus, classes are typically free.\nWhat is a MSDS? Is that a Microsoft qualification, or a master of science?\n>Used to have friends, go on dates, socialize all day That sounds awful. I make six figures in my pajamas. I don't have to deal with drunk assholes and men trying to grab me all night long. Going on dates? Gross. My husband cooks, cleans and manages the house. I get to write code and play with my dog. No free time? Maybe you're doing it wrong. I think I \"work\" maybe 2 hours a day most days. Gotta put in your time kid. The party college days are obviously going to be more attractive when you're young but it won't be that cool when you're a washed up 40 year old behind the bar with bad knees and liver cirrohsis.\nI used to wait tables at the beach. Serving happy families on vacation and watching the sun set behind palm trees along the water. Earning my degree and getting an office job was soul crushing, and I had wanted it so badly! It was my picture of success, for whatever reason (probably brainwashed by my boomer parents). Id go back to the hospitality industry but I need my current salary and health insurance, and idk what the same job would be like these days since Covid happened after I left. I thought college and a career were the top achievements that I needed to strive for, now idk what to do with my chronic stress and anxiety. Deep down Id rather go back to crushing shifts, making tables happy, being in shape, and walking with cash. My only solution is to just sock away savings and try to get a low-stress job where you can pursue your own projects, hobbies, and interests. I freelance as a writer on top of my day job. I put aside my pride and took a small pay cut to apply for easier day jobs for the sake of my mental health. This mix seems less soul-sucking to me.\nMan IT is probably not a very good fit if you like people more than computers. I'm sorry for all of this happening to you, but I think a switch of career is better for your mental.\nI had a blast as a barista in college, but sure as shit would not want to go back to making minimal wage.\nI was happier working at a car wash than I am being a logistics contractor for the government bringing in 6 figs. Lmao life sucks sometimes but we do what we gotta do\nNot to be a dickhead but they dont pay you a great salary to do it because its fun and easy\nSounds like you got a bit unlucky. A good manager and employer can make a worlds of difference, but that's any industry. I'd stick with it because of how lucrative it can be if you find the right gig (financially and lifestyle wise)\nSounds like its the environment, not the job. Find a better company. Even if it means less pay. Now may not be the time. But my guess is by summer hiring picks back up.\nWelcome to real life.. corporate job, especially IT (where else you work in sprints and all this agile-estimation bullshit?) really suck out the soul of a person.. the only things which cheer you up is your payslip..until it don't. If you have kids at least you know for who you're making it.. if not - recently seen a META intern who denied 400k TC contract to pursue youtube career, while his channel was close to mediocre when it comes to audience. I dont even blame him\nMaybe try software engineering. We get paid a lot and have great work life balance (as long as youre not a hustle tech bro lol)\nI could've written this. Upvoted.\nI had a better time working at dominos than my clinical research job in regulations. Same problems you facing\nWhat your describing doesnt sound like it much to do with being a DE - your describing burnout and the realization that life after college is boring\nRandom but I know a head bartender making six figures. If I had to guess makes $130-150k. Has a new half million dollar home, young family, a weekend car project. His wife is SAHM and works part time online Lives in South Carolina so not terribly COL. He works for a luxury resort near Hilton Head. Had no idea bartenders could make well above $100k and not live in New York or Los Angeles.\nsmells like the US\nGo back to bartending. My buddy has been doing it for nearly 20yrs and still loves it. Invest 20% of your income in a VTI and youll be retired in 20yrs\n>Layoffs everywhere, shitty tech stacks, bad stakeholders, boomer bosses, bait and switch job opportunities, remote roles being replaced with 100% on site. Absolutely true of the last 2 years but not true of most of the last two decades. I'm not sure what the future looks like.\nYeah. Things are more fun when youre younger. You dont have to stop having fun. I had a great time through my 20s but things change, they always will. A 9-5 office job is different than working a bar and going to college. One has a future of work growth the other doesnt. You can make a life choice but know there is no going back in time to experience what was. It will never be the same.\nInteresting. I can't relate as I've had a far better career working as a DA than my previous shitty bank roles. You might just have terrible companies you are working for, curiously, do you work for big companies or startups?\nMy guy, I can relate! I bartended for about 8 years, right up until lockdown. Then I pivoted into Salesforce. 3 days a week, I second-guess this choice. I honestly think that once youve been behind the stick, nothing else really compares. Youre journey hasnt been a waste, but theres not much in your current life that can really live up to the super-high Highs youve experienced in the hospitality industry. I mean, cmon. 6-figures? And you dont have to deal with any drunks or accidentally putting your fingers in used ketchup? Youre not doing too shabby, now that you dont have to deal with any of that. Maybe you can find a gig as a DE that isnt a complete time-suck and then pick up a shift or two on the weekends. That is, if you dont have other life obligations like kids, partner, family, etc.\nGo to whatever makes you happy man, your life is only one, live at its fullest, you may not make as much but your wellbeing is first\nUm yeah that checks out! It's a soul sucking developer job, that's WHY it pays. I have fond memories of serving drinks and partying too. I think a lot of people got into our sector during the recent peak years (now in the rear view) thinking it was a ticket to easy street. That kinda was true at the time but many didn't understand it was a temporary state more an indication of a top being in than some kind of \"new normal\". Now it's payback time. I was around last go around and I hold up 1997-2004 to the light lining it up with 2019-2026(?). Three years of boomtime, three years of famine on either end with about two years of chaos in between. Hopefully at least the chaos period is just about over and sure enough I've noticed the recruiters back from the dead just in recent weeks. Like clockwork! We'll see how well the comparison continues to hold up though. Knock wood.\nI went from working in restaurants to working in BI and data analysis and I will never go back to restaurants. Too stressful, and income was unpredictable, plus no benefits. My experience has been pretty good in the corporate world except for one position, but the company had a reputation for being a grind. Elsewhere, I looked at Glassdoor or the like and read reviews before applying or accepting a position. Seems to workout doing that, but Im not a data engineer really.\nI mean, yeah, you definitely found out the hard way that it's not a picnic. The service industry is not a good standard for comparison to the working world. Good luck.\nHow far are you along your masters program? I would highly recommend you try and push through that shit and complete the degree. Then if you can afford it, resign and take a sabbatical for as long as you need to recover from burnout. Formulate a career path moving forward and prioritize what YOU actually want in that career. You wont get everything you want but try to get the top things.\nWhy dont you go back to being a bartender? No?\nI love my DE job and would never go back to service. \nDE roles should be viewed as transactional sticking to brief and deliverables. You may see majority of data engineer contractors are better off with these roles as they dont have to or care about corporate chaos\nDE isn't awesome, nor is it meant to be. I much preferred deving software, cloud, API you name it.\nYou are probably reeling from the pressure of high-paying tech jobs. It's not for everyone, if you were content with your life before data then by all means go back to it. You just won't probably enjoy a higher income. That's the truth of it, you can't have everything. It all depends on what happiness is to you. Some people, like me, are happy with success, achievement, and financial security. I honestly would be miserable and bored living a carpe diem existence. Some people are different. They are okay with a modest existence as long as they get to do hobbies and spend time with friends and family. Just choose what feels right to you.\nI feel you man. Came here from software dev and can't wait to go back and reclaim my sanity.\nI do not feel the same way but I can sympathise with you. At this point you could probably get into a manager role thats less stressful, involves less or no leetcode e.t.c\nIm in tech sales and would switch jobs with you\nDamn data engineers have to grind leetcode? Lol\nEven worse, both DSA and SQL leetcode\nSkill issue\nHa.. I was happier as a valet.. go figure.\nI feel the exact same way OP.. I bartended through college, was always active, making great tips, talking to people, having fun, 2 years into my corporate job all I did was sit on my ass all day staring at the screen completely isolated, dealing with old ass stack, managers who have no clue, then I was laid off and the thought of looking for a new corporate job is giving me anxiety, never mind constant pressure of upskilling and trying to keep up with every single piece of new technology that is coming out faster than I finish learning the previous one, I love learning new things but im worried that this is my life for the rest of my life? How am I supposed to enjoy life outside of work if I constantly need to be on a lookout of new things coming out that can automate me out of my job? Its so exhausting sometimes. In the same boat, love data but not sure if I want to continue this route.\nThe DE career can actually be the issue. Consider that when you are a bartender, you're part of the night life crew. You're paid based on performance, which is directly linked to your ability to quickly socialize with others. If I had to pick a job that was the polar opposite of bartending, I'd pick data engineering specifically (I'm a DS, btw). Even data analysts and data scientists do more socializing/presentations. And some of the crap things you have to deal with as a bartender you don't as a DE and vice versa. Would you consider a transition into technical sales? You can leverage your technical knowledge there considerably, can absolutely make as much if you're good (more actually), and you'd gain back some of the elements that you lost from leaving bartending.\nCheck back in in 20 years and let us know how you feel. When bartenders your age now have shit knees and bad backs and havent gotten a raise in two decades\nSo true OP. Law/accounting/finance - pretty much many corporate jobs are like this. Especially working for businesses that are devoid of any real significant mission or real work on world improving stuff and are just money grab institutions enriching people who really dont care about their employees nor anyone else in the world. I found corporate world full of narcissistic selfish types who were just clawing and stepping over each other just to prove to themselves how important they were and having stuff. Life is much more than that or it can be. Sometimes it is better to just make less money but at least make it easier for others to laugh or enjoy their free time as much as possible while you also have more personal time to explore, learn new things, and just enjoy life. Life is short. Heres a drink to finding what makes you happy OP.\nHmmm.... i feel bad for you, but not all DE positions suck this much , and not all bartender jobs are cool, i have done both. I just think DE has more to offer than a bartender job. To those reading this and at the beginning of the DE journey, this is not accurate unless you are bad at it :)\nBefore you abandon ship let me give you some perspective. Yes, corporations suck. Ive been in tech for over 30 years. Most of my career has been at jobs that suck but pay well. My happiest work was for small/medium companies. They appreciate you more because if you cant solve it then no one in the company can solve it. The problem with DE is its mainly a Fortune 500 hiring market. Only big companies have the need to manhandle huge volumes of data. I would suggest looking for a smaller company first, the second option is consulting, specifically one with an employee owned stock program(esop). This is my current job and I love it. You essentially work for yourself.\nI currently work remotely in sales for a large chemical manufacturer overseas. I've been doing this for less than 2 years. My base salary is $120k+ and my commission places me around $170k OTE which runs consecutively due to renewals and terms, so I'm more or less on track to be within the $225k+ range within 12 to 24 months. I've also managed to launch a startup selling services to the company I work for which nets an additional \\~$3000 profit per month. I hope to expand this service to other clients and grow it as side-income which can become main business or I sell it off to a larger provider in 12 - 18 months. However, I still look for work in hospitality and customer-facing roles for the evenings and weekends just to maintain that engagement and interaction. In my mind, if I can be earning while talking to people, I think it's a win. Restaurant environments are also really fun most of the time, and it's even better if you're not living dependent on that income. However, it's best not to make it known that you have other income which is sizeable lest coworkers and managers get envious or jealous. Emotions run high in these places. Also, I work out 4-5 days per week. I try to get it in the morning. I breakup my splits so it's not too taxing or some days I just do what I can do. Life and work are all about SUSTAINABILITY.\nDidnt read but yeah, any job where you can do coke w/ your coworkers is going to be better than software engineering. Ive many times daydreamed about quitting my job and making coffee for a living. But Ill say this - you can find teams that make it better and it seems to me the smaller the company, and the less pay, the less stressed youll be. So maybe take a pay cut for a smaller org with cool people. Edit: seems to me, not seems to be\nI've been working in tech for 20+ years now, and it's gotten quite bad in the past 5 years for most of the reasons you mentioned. For the first time in my career, I'm actively exploring thoughts of giving it up. Or at least, not working for other people...\nI've been in different data warehousing roles over the past 25 years and DE was the worst. And its getting worse. I can't tell you how often I would get terribly formatted JSON with 20 levels of nesting, that was missing key elements- and still expected to have it parsed and ready in a few days. Then having 10 different cloud databases, that don't talk to each other, that require extraction form a db, then movement to blob, then insertion back into a db. I realized the days of simple csv with quotes are over and everybody loves markup, but God, I hate markup (xml, json...). It made being a DE a horrible job.\nCan you give advice for getting into DE after graduation? What skills should I work on and is it possible to be ready within a year? I want to feel your pain . Seriously.",
        "content_hash": "5b4ee504cba919d2a502202d29d341cd"
    },
    {
        "id": "198kif0",
        "title": "My company just put out 3 data engineering jobs last year, guess who we got?",
        "description": "As per title, my company put out 3 entry level data engineer jobs last year. The pay range was terrible, 60 - 80k. \n\nWe ended up hiring a data engineer with 3 yoe at a Fortune 100, a data engineer with 1 yoe and a masters in machine learning, and a self taught engineer who has built applications that literally make my applications look like children's books. \n\nThey've jumped on projects with some of our previous entry level hires from 2019-2022 and made them look like chumps. \n\nAll of them were looking for jobs for at least 4-6 months. \n\nJust wanted to share a data point on the state of the market last year in 2023. \n\nFunny thing is that I don't expect any of them to stay when the job market picks up, and we may have a mass exodus on our hands. ",
        "score": 540,
        "upvotes": 540,
        "downvotes": 0,
        "tag": "Discussion",
        "num_comments": 112,
        "permalink": "/r/dataengineering/comments/198kif0/my_company_just_put_out_3_data_engineering_jobs/",
        "created": 1705455890.0,
        "comments": "So learn the most from them while you can. Talk with your manager or higherups about training, organizing speech and talks. Maybe once every two weeks and rotate.\nYeah we already do that. I've learned a lot from them just peer programming or hopping on CRs.\nOr just...maybe...recommend them a promotion and humanize them?\nHow on earth is this downvoted?!\nWhat is a CR?\nClippy Routines - its when you ask Clippy to help refactor your VBA code\nWhen is gpt-powered clippy coming?\nOmg laughed out loud\nIm guessing Code review\n*Colorado Rockies\nMaybe change request\nSomeone who has 3 yoe as a DE and being paid this much is pretty criminal. You're definitely right that these folks won't stay for very long. Wonder how long the pendulum will side on the employers side.\n>LeonCecil Yeah I completely agree. I sometime ask them to do less or work slower, and to just focus on their own development. So that at least they are paid relative to the time they put in. I think the pendulum will stay on the employer side for the remainder of 2024 likely. It will get better this year, but I don't think hiring will accelerate until at least 2025/2026 when interest rate go back to 3-4%\nYeah that sounds about right. All eyes on the feds as usual...\nIn US, I have 8 YoE and I am paid 70 in the Netherlands.\nYou should really look for more if you're getting 70 with 8. I'm getting 71~ with less than 2 now also in the Netherlands...\nNorth? I am in the south\nZuid Holland\nSo you earn 5.8K a month? I started (although very recently) at a startup in Belgium and my monthly salary is 2.8K (with car).\nPlot twist: it isnt swinging back\nI have seen DE jobs, here, in the UK, for that much experience list $55k, sometimes less. And it was one of the top automotive brands. The disparity is insane.\nThere is a lot of value in learning their strengths and becoming a leader of these entry level engineers. Especially as someone who has been at the company and knows the landscape. Leveraging their skill sets makes everyone look good. Dont view them as a threat, but rather foster collaboration with some skillful people\nI definitely don't view them as a threat. My view is that if I'm not good enough, or don't know enough, then it's my job to be humble and learn with humility. I don't put my job over my humanity.\nOP, I would love to learn more about their experience and qualifications if you don't mind but don't know how that would be possible without divulging private information. Obviously we're all striving to be like those data engineers but their description seems insane.\nYeah so everyone with the exception of the self taught person did their undergrad in computer science, and the self taught person worked a few years in mechanical engineering. All of them came knowledgeable on building production quality applications and pipelines. Which a lot of the hires from previous years are still learning\nWhat were you expecting? You hired 3 experienced data engineers/programmers that were good. Did you expect something less? My company picked up 3 data engineers last year. One was inherited from a team that was disbanded. The other was a friend of someone already on our team. And the third is a data scientist that they didnt know what to do with. They all stink IMO.\nMaybe it's not surprising that experienced programmers were good, but I think it is surprising that good experienced programmers would settle for that pay.\nThey didn't seem that experienced. One had 3 years and the other just a year. In a year or so when the recession passes your firm will either raise salaries for these individuals, promote them up to higher paying positions or they will leave. Depending on where you live, those salaries seem fair in a tight market.\nI was expecting less to be honest. In 2019-2022 we were fighting for people who only took an online sql course.\nI would have killed for a position like that. I've been programming since 16 years old and studied physics but didn't complete my degree. I am data science trained and published. Unfortunately my data engineering skills have rusted a bit since I've focus on the business side of things as a data analyst in healthcare. But I started out writing data pipelines and etl/elt at my team to fill a skill vacancy before we hires someone that was much more into it than I was at the time. I should have stuck with it. I am only now going back to software development after mentoring a junior software dev for their first internship/position. But still in the same position professionally. Looking to break out into a different role with bigger pay.\nfuck me! $50k net is considered not bad salary here in Europe for DE, and I have 5 yoe with comp somewhere around that...\nIt's the US salaries that are blown way out of proportion.\nHow can be salary in Europe be similar to India. We have similar benchmarks here, but way less cost of living.\nIt sounds unlikely that avg. EU salaries  avg. India salaries. If that's actually the case, fuck me with a stick lol.\nThat's salary. In europe you get health insurance and pension payments on top of that.\nThere's been an inflation in IT salaries in India the past years that don't reflect the average at all, so cost of living is far from following it. The reason is every software company flew there for well educated cheap engineers, and now there's big competition for them. I have colleagues from there telling me that they would have better life if they stayed there, so now they come to rich countries for traveling and a foreign experience in their CV rather for the higher salaries. As a consequence big companies are also moving to cheaper countries such as Kenya: https://qz.com/africa/2173517/big-tech-is-winning-the-battle-for-kenyas-talent\nyeah... the same thing is for Russia, working for a company there remotely but in EU now - salary can be on the same levels as in many European countries, but CoL is much lower edit: to clarify, average salary for senior backend devs in Moscow and Saint-Petersburg are around $45k after taxes, source: Habr Career. Don't know Indian salaries\nBy the way, similar to India?... Average salary for swe in India according to Glassdoor is around 500-800 dollars per month, it's like 7 times less than 5k... can't understand what you're talking about here heh\n50 net in Europe is like 100k gross right?\nMore like 75k (The Netherlands).\nTaxes are 1/3rd?\nThats still very low lol\nWhats your background?\nI'm self taught. I would say I'm just as knowledgeable as them now, perhaps even a bit more. But that's only because I've been grinding for many years. They would have eaten my lunch if they were compared to myself when I was originally hired.\nAny idea on where these superior skills come from? It sounds like you work on the same stuff but what makes this guy's work so much better?\nKnowing how to build production quality code would probably be the main thing.\nI'm gonna sound very stupid right now but I'm not even remotely close to being in the game. What does that mean and how/where do you think he learnt it?\nReadable, reliable, extensible is where I might start. Readable code is easier to review and maintain which leads to fewer bugs. Avoid being too clever and adhere to some code style guide. Reliable code does what its supposed to consistently. This means handling common edge cases and raising actionable errors. Tests help here. And a capable devops engineer to assist with monitoring and observability ensures you can improve reliability. Extensibility is an emergent property of thoughtfully organized code and applying design principles patterns so that adding new capabilities doesnt require significant refactoring. This is easier said than done, however. Scalability is another one. Decomposing problems into units of work that can be horizontally scaled allows your solutions to grow with demand. Theres definitely more to this and I think the best way to get better is to learn from others. Whether thats through code review or just a studying the code of the tools and services you use.\nGood to know I've been already educated on Readability and Scalability in formal education. No testing techniques or extensibility minded approaches tho. I'll get down on that.\nHey no worries, asking questions is never stupid. I didn't include anything specific because I don't want to give too many details. I'll dm you\nCan you just make a post at this point man lmao so many DM requests\nAny chance I could get the same DM? I'm in the same learning situation :)\nIf possible, could you share the same with me via DM? Thanks a lot\nDM request #4 \nDo they also implement testing?\nAlmost all our DE roles have been outsourced to india... is that happening at any other workplaces?\nYes. At many of them. I have had only two interviews since being laid off at the end of Oct and when it came to disclosing the team makeup, a majority was off shore Indian.\nMarket is total shit right now, indeed. But I know a couple of guys in IT and they have the habit to take a month long breaks no matter the financial situation. But their favorite shoes is sandals.\nYour company in the USA/Canada? I can attest the job market has been brutal since 2023\nUSA\nI'm a DE, totally from non tech academic background with 1.5 YOE of experience, working in a service based firm, cleared a 3 YOE interview to get on a project as Tech Lead. I'm self taught App/Data Integration for 6 month before transitioning into DE, A databricks professional, Kafka confluent stack on Azure/AWS. I never on my mind think about how much I got paid but the work I do and delivery. Before this all tech stuff I was a Management guy earning pretty well but didn't like the work. It all depends on what you like to do. If you love tech and overall aspect of programming, solving problem then it's a smooth walk.\nNo offense but you really dont sound qualified to lead anything technical.\nIt's alright, I too thought like that but we need to start from somewhere.\nHahah I mean big ups to you, congrats for pulling that off and hopefully it works out. Im just jealous because after 3 years of grinding my cock off Im about to become a senior\nIt's alright, Just believe in yourself...Sooner or later it'll come to you bro...\nWait 3 years is now considered senior??\n> Funny thing is that I don't expect any of them to stay when the job market picks up, and we may have a mass exodus on our hands. This is correct. They're only working at that rate while they have to. It's just a survival mode until the real offers come along. No different than waiting tables and working in a warehouse for shit income while waiting for that first offer that uses your degree you just got, that inevitably comes.\nYeah these are temp jobs for them to keep the lights on\nMay I know what industry and also city is your company located in? That made a huge difference\nWe're remote so that may be why it's a bit more competitive\nThere it is. May be its not them who is actually doing the tasks. They can get help easily if it's remote.\nLocation?\nRemote\nI mean the location in general. Like is it a us based company or something else?\nGiven he's said a 60-80k salary for an entry level position is terrible, it pretty much has to be the US.\nThis is actually pretty interesting, a little sad, and scary all in one.\nThe job market is gonna pick up?\nIm totally lost here. Im guessing the implication is that these are all very good folks and that the market is crap. Yes, the market is absolute garbage. However, Im also surprised that a DE with 1 yoe is being considered a very good candidate. I havent had any openings in the last year or so (market is crap ), but when I was hiring previously, anyone with less than 5 yoe was considered junior. I guess my question is whether I understood the post/implication correctly?\nMy main takeaway is that OPs company has a really weak team of engineers. Maybe they made some questionable hires when the supply was low.\nThats wild. Were any of them on visas? Hired as contractors or regular perm?\nNone on visas. Regular perms\nWhat was the self taught engineers project please share some details github if possible\nWhat county and currency? If you know a native English speaking data engineer willing to work in the U.S. for less than $100k with any enterprise experience, Ill be shocked.\nyou can hire anybody from the UK for that kind of money and they will be jumping with joy.\nquestion, did these employees need visa sponsorship? i notice that's where a lot of the 'looking for 4-6 months' comes from. i am surprised to hear that such qualified people struggled for so long.\nJust curious, what does your company seek to have in a candidate for fresher role of data engineer?\nI am also very curious about their particular skills and production quality code, so please DM me with some details if you dont mind. I am also self taught and grinding it out every day looking to get better. Also, not seeing it mentioned much here, but I would talk to your management, assuming you have a good relationship with them and can have open conversations, about increasing their pay and placing them within the market reference range for their skill set and years of experience. A good company will do what it takes to retain good people.\nI wonder how the rest of your team feels: Hot Fuzz: You've Been Making Us All Look Bad https://m.imdb.com/video/vi3317744665/\nThis gives me hope as a self taught building data applications\nWhat's the pay?\nHahaha your post reminded me of these old TikTok videos where the new dev is super unqualified https://www.tiktok.com/t/ZT8gHUpfL/ Sounds like a great learning opportunity.\nDamn, I still want to quit my job. Hopefully they are going to lay me off next week.\nYoure screwed\nCurious what kind of application did the self taught DE create? I have been working on self taught data science project and was curious if it is a web app that uses free data from anywhere?\nFrom all that you said, Im a fan of your humility, dude!",
        "content_hash": "9809ce0c5c397d54b48de86b1da96619"
    },
    {
        "id": "1hbsjl5",
        "title": "7 Projects to Master Data Engineering",
        "description": "",
        "score": 531,
        "upvotes": 531,
        "downvotes": 0,
        "tag": "Career",
        "num_comments": 46,
        "permalink": "/r/dataengineering/comments/1hbsjl5/7_projects_to_master_data_engineering/",
        "created": 1733920758.0,
        "comments": "This is exactly what I was looking for, thank you!\nThank you for reading it.\nSomehow chrome did a good job to suggest this article to me. I have this open in my browser for about a week but did not even go through it. DE veterans over here, do you think all of these are bad, good or way too much for beginners?\nRead somewhere, getting started before you are ready is important. Dont overthink if it is good or bad, just get started. There is no one size fits all solution for learning, the curve will be different for everyone. Get started and you will figure it out for your self.\nTrue that. Hopefully I'll be able to get something during the holiday period. \nCan you guide me please, I have basic SQL and python. Should I focus more on advanced python and sql or I can get started with these projects and learn along the way ?\nI am getting really tired of these types of posts. No, you won't master data engineering with this. This site is a tool vendor's wet dream. You will **start** to learn \"Python, SQL, Kafka, Spark Streaming, dbt, Docker, Airflow, Terraform, and cloud services\". There is so much more to data engineering than the tools.\nCo signed. In fact, the real value of this post is to take the datasets involved and do your own thing with it, completely ignoring the stack someone else picked.\nHey fellow redditor! I often come across these posts and always wonder what is meant by them. I think the tools available / the tools you pick strongly affect your space of what you can do. So im really curious on what you mean that there is more to that. Thanks in advance!\nThe most desirable data engineers are not ones that have learned a particular set of tools. It's the ones who have intuition, can think critically, and ultimately solve problems with whatever tools are at their disposal. Focus on concepts and patterns. Not tools.\nThis. In spades. Absolutely. Learning just the tools turns you into a one trick pony.\nThis question gets asked a lot. You may find this [previous post](https://www.reddit.com/r/dataengineering/comments/1gfgl02/comment/lujz7bg/) helpful.\nThank fuck somebody else said this. I thought I was going insane thinking this post was a steaming pile of shit. Maybe not steaming as it suggests this is fresh.\nNo, it's steaming.\nI read and agree with your post. Apart from SQL, data modeling is a critical skill to have. In my current project, 3 of us have to do data exploration of 5-7 applications which are around 30-40 yrs old and need to build new logical data model which will help the company to create brand new operational data base and build one single unified application for the business domain. There's way too much noise online and everywhere about AI hype, far too many tools evolving at rapid pace in data engineering space, yet they're all based on SQL. Ten years ago it was Hadoop and Hive, in last few years and now it's all Databricks and snowflake hype, followed by dbt, airflow and other stuff. Few years later, it will be some other tools. It's annoying to constantly have to keep up otherwise you don't get the job. We're still trying to catch up with data from 20 years ago. I think all the data should be released back to public and then we don't need all these fancy tools at all and not many engineers to write crappy code using fancy tools. Problem solved!!! Data is so rapidly changing that data from five years ago may no longer be valid in the current year and yet we keep shoving all that data into our databases.\nSince what should people who want to do this is a common question, I point them to a [previous post](https://www.reddit.com/r/dataengineering/comments/1gfgl02/comment/lujz7bg/). Yes, I am lazy.\nNah, you're right at pointing them to your post.\nWow, it's good to see comments like these. When I saw the wiki for this sub, it suggested learning concepts, but not necessarily specific languages other than SQL for querying and looking at aggregates, or maybe Python for scripting knowledge. I will try the projects suggested just to get practice implementing, but I'll be paying stronger attention to concepts being expressed, questioning the selection of tools being used, while keeping an eye on the fundamentals of SQL, relational DB creation, and DW, slowly trying to grasp how to build, interact with, or optimize them. It might be harder to take in SCD unless I introduce my own new requirements over time. I remember seeing free lecture PDFs from Stanford or MIT that cover these things you mention with simple examples (maybe it was more SQL focused at first with views, window functions, subqueries, and also SCD 1 & 2), I hope that is enough to help me get comfortable with these topics for future entry level job searching.\nThank you, youre a real one\nThank you. It means alot.\n\nThank you\nComment for later\nUppies.\nWow looking for this\nThank you.\nI can't thank you enough for this. Was really looking for something like this. There's very few sources like this I believe for DE for some reason. At atleast I am not able to find any.\nI write alot about DE on DataCamp and KDnuggets.\nWhat a gem!\nThank you.\nThank you Op\nYou are welcome.\nThis is awesome, already started the boot camp. Thanks a lot.\nCheck out the new cohort. They have introduced some new tools.\nWhats your experience on the boot camp?\nIt's ok, the instructor is a little bit fast. Apparently only 100ish people got the certificate of completion out of 1000+ who started so I don't know if it's hard or there's something else there.\nYou mean data engineering zoom camp, right?\nI'll give this a look later when i have some time. Thanks!\nUnfortunately, most of these projects aren't very ambitious or interesting. I'm looking for project ideas for my CV, but that's clearly not what's going to make the difference.\nShould I start with DA, then go for DE or should I go straight to DE?\nDepends on how much you already know, coming out fresh some basic conceptual foundation of data will go along way before DE.\nI recently joined my first organisation and boom, now I'm a DE. I know SQL ( learning and practicing the advanced concepts) Learning syntax of python and it's libraries ( already have good understanding of data structures but in Java) Any other suggestions? I am also reading the fundamental of DE\nYou may find this [previous post](https://www.reddit.com/r/dataengineering/comments/1gfgl02/comment/lujz7bg/) helpful.\nIncredible, thanks for sharing!\nupdoot for you!! This needs to be pinned as learrning material for newbies\nthis stuff is actually an anti-pattern for learning to be a DE",
        "content_hash": "f30f1b065b7e475ec96dbe71dd34eddc"
    },
    {
        "id": "18ix6hd",
        "title": "How Netflix does Data Engineering",
        "description": "A collection of videos shared by Netflix from their [Data Engineering Summit](https://netflixtechblog.com/our-first-netflix-data-engineering-summit-f326b0589102)\n\n* [The Netflix Data Engineering Stack](https://youtu.be/QxaOlmv79ls)\n* [Data Processing Patterns](https://www.youtube.com/watch?v=vuyjK2TFZNk&list=PLSECvWLlUYeF06QK5FOOELvgKdap3cQf0&index=3)\n* [Streaming SQL on Data Mesh using Apache Flink](https://www.youtube.com/watch?v=TwcWvwU7B64&list=PLSECvWLlUYeF06QK5FOOELvgKdap3cQf0&index=4)\n* [Building Reliable Data Pipelines](https://www.youtube.com/watch?v=uWmJxbhI304&list=PLSECvWLlUYeF06QK5FOOELvgKdap3cQf0&index=5)\n* [Knowledge Management \u2014 Leveraging Institutional Data](https://www.youtube.com/watch?v=F4N8AmScZ-w&list=PLSECvWLlUYeF06QK5FOOELvgKdap3cQf0&index=6)\n* [Psyberg, An Incremental ETL Framework Using Iceberg](https://www.youtube.com/watch?v=jRckeOedtx0&list=PLSECvWLlUYeF06QK5FOOELvgKdap3cQf0&index=8)\n* [Start/Stop/Continue for optimizing complex ETL jobs](https://www.youtube.com/watch?v=Dr8LMn-nJGc&list=PLSECvWLlUYeF06QK5FOOELvgKdap3cQf0&index=9)\n* [Media Data for ML Studio Creative Production](https://youtu.be/1gGi3NBZk7M)",
        "score": 509,
        "upvotes": 509,
        "downvotes": 0,
        "tag": "Blog",
        "num_comments": 112,
        "permalink": "/r/dataengineering/comments/18ix6hd/how_netflix_does_data_engineering/",
        "created": 1702636326.0,
        "comments": "To the devs reading the post, the company you work for is unlikely Netflix nor has the same requirements as Netflix. Please don't start suggesting and building these things in your org because of this post\nI worked at Netflix, heres why your mom and pop company needs Netflix tech to be successful -tech influencer right now\nZach Wilson? Lol\nThat would be more like \"Here's the missing tech impeding your mom and pop store from making at least $500k per month\"\nResume-driven development in the making bruh\nOne of the places I worked at was trying to push Spark so hard because thats what big tech uses. Their entire operation was less than 100GB. The biggest dataset was around 8GB, but their logic was that it had over a million rows so Spark was not an option it was a necessity.\nMan, over a million rows *was* big data when I was working for a university. Now I work in healthcare, and Ive got a table with 2B rows. Still trying to figure out the indexing for that one.\nYea that's probably one to be careful with due to the size the index could be.\nYep. Im relatively young as a DE, so Im playing it pretty safe. Im currently investigating sharding/partitioning for this quasi-DWH. Fingers crossed!\nYouve upgraded, next up is trillions of rows\nI dont think SQL Server can handle that much, capn! Were reaching maximum capacity!\nYou could run the whole company in excel at that rate \nDont give them ideas\nAre there any rules of thumb for when Spark is a good idea? I've seen these comments before and I know my company uses spark a lot for AWS glue\nThey were using glue as well. I think my main questions are. 1. Do we need to load this dataset all at once? 2. Does the dataset fit into memory? As an example: My old place used to call a vendor API and download data on a hourly basis. Each data ingest was no more than a few MBs. They would save the raw data (json) to s3, and then they would use Spark to read the historical dataset and push it into a redshift cluster. So, they would drop the table and rebuild it every time. Alternatively, I removed the Spark step and transform the json into a parquet file and saved it to s3 assigning a few partitions. Then, I created an external table on redshift to query directly from s3. The expectation was that the dataset would grow exponentially due to company growth, **spoiler alert:** it didnt. But at least we werent starting 5 worker nodes every hour to insert new data.\nI don't think we are dropping the tables each time but using a high water mark to determine how much to pull. I've been trying to talk to my team about this because they use spark for everything. I didn't know if there was a cost issue using all the nodes. I was gonna try to suggest Polars and not use any nodes. But I'm not as familiar with what they are doing to run the pipeline.\nWe have tables with size in parquets around 500gb to 1tb, found issues with redshift and migrate most of them to spark, serves us well enough especially we deploy all job to eks and scaling is managable\nThese initiatives are the ones where they design for future and imply everything that shouldnt be applied\nTo my mind it's a bit of a duh..you look at requirements of whatever it is your team or org wants to achieve and then you come up with a solution based on that. Which can be inspired by this but definitely shouldn't be a copy.\nI was a consultant and I can tell most DEs here, this skill is more important than most hard technical skills in this field. Knowing what tech and what tech to not throw at a problem to solve it is just as important as doing the actual work\nYeah I don't always know but I'm aware of my flaws and working on it. At least research the possible solutions instead of just applying some random shit.\nSerious question: how many paying the bill tend to recognize the difference in the end?\nhttps://github.com/sirupsen/napkin-math\nHelpful!!!!\nMan why does everyone hate on dbt lol\nI feel they push marketing too hard? Utilize 100% our analytics flow with dbt, seems good enough\nCan't believe the number of people who recommend Spark on here for relatively medium data flows.\nThere are hundreds if not a few thousand companies that have as much data as Netflix. There are likely tens or hundreds of thousands of companies with more data than frigging airbnb. Thats not the reason why you dont listen to these over engineered SV companies, your dont listen to them because they are over engineered at every step. And also because Netflix makes tools no one should be using (like conductor).\nWatching the first video, I figured that working as a DE in Netflix is probably less interesting than I thought. Note that they built a lot of custom stuffs but the most dreadful is the custom scheduler. So from my understanding DE are just YAML engineers who are supposed to understand their data -- so basically BI. But he did mention Scala/Python at the beginning though. I could be wrong but it would be much more interesting to work in the developer tool team, who builds those internal tools.\nThis is pretty much how I felt working as a DE at Facebook. I thought it was going to be inexplicably awesome because they had so much data from so many users across so many countries. I thought I'd be solving a ton of scalability issues, and doing complex data modeling, as well as building really robust pipelines. But I got there, and almost all of that stuff had already been written. My job was to make sure the dashboards were right and that I could explain any drops in the numbers by ensuring the data was fine. It was one of the most disappointing experiences of my career.\nMost fun youll have in this job is at smaller companies with a nice data footprint or start ups. FAANG shops wouldnt be what they are if they were hiring us in 2023 to solve big data problems. They are hiring us to maintain them\nMoney must have compensated for your disappointment, amiright?\nYes and no. In terms of base salary and bonus, the job I took after facebook, at a much smaller non-FAANG company, paid me almost 33% more. But I got lucky when I joined Facebook, so my stock options were wild. I was making six figures just on the stocks alone, simply because of timing. That would have all dried up eventually though as I was approaching the \"4th year crash\" of my total comp. At the time, I wasn't really someone who was motivated by money. I thought I needed to \"live up to my potential\" (whatever that means), and that I needed to be doing more to be considered a proper engineer. But I've recently had a bit of a revelation in my own views of work, so the money could certainly be a motivator for me at this point.\nInteresting - what was this consulting firm? Curious to hear what opportunities are out there\nI heard they do have those data engineers that are more like programmers. They just call it SWE. Also they have platform engineers.\nWere you able to save a lot of money and leverage that into a more interesting offer and company after?\nYou could think of it like that. I wouldn't say I \"leveraged\" anything, but I did pay off all my debts and then use the fact that I didn't need as much money anymore to transition to a role that still ensures I'll retire early, but in the meantime allows me to do more interesting work.\nSo the experience was disappointing, but overall it was still very beneficial. That's good at least, no time truly wasted haha\nI left. I made a post about it, which you can read [here](https://www.reddit.com/r/dataengineering/s/MWkoujgKGc).\n>My job was to make sure the dashboards were right and that I could explain any drops in the numbers by ensuring the data was fine. I do the same at my DE work too. But I also build data pipelines. Can you share more about the interview process? How was it different than regular software engineering role?\nI wrote about the interview [here](https://tibblesnbits.com/posts/de-interview-faang)\nCool. Thank you for the writeup. &#x200B; >I would say the best way to prepare is to do the hard Leetcode questions, but try to do them without using things like window functions. Facebook, and likely most tech companies, want to test your knowledge of the base language. The reason for this, as I understand it, is that while they understand modern approaches exist (i.e. window functions), some of the harder challenges they solve require a more low level approach, which requires understanding the base language. What do you mean by low level approach - can you give an example? I am under the assumption window function is part of the base language - meaning you can find it in the SQL standard.\n> I thought it was going to be inexplicably awesome because they had so much data from so many users across so many countries. Exactly my experience as a data scientist in corporate banking. Thought I'd be building out and deploying ML models, but the models are already built and it's mostly reporting, validation and explaining trends. Endless red tape and disappointing. sorry for venting.\nThis was pretty much my experience as a data scientist at meta. I thought the work would be way cooler than it was. Usually it was either (a) sitting around waiting for data to process or (b) trying to pull data out of cold storage so I could query it. Which isn't to say that I didn't do any interesting analysis, but it wasn't as interesting as I had hoped. The lesson learned here is that more data doesn't mean more interesting data or analysis.\nJust want to say I actually don't disagree with the decision but just think it's not interesting. It's more interesting to work on the platform teams that build the internal tools. This is purely my preference and yes I never worked in FAANG.\nYeah it's fun, but I want to move into a more lower level career so need to write code constantly.\nWhat is also cool is that Netflix data engineers developed Apache iceberg to address the limitations of Hadoop. The creators of Iceberg started a company called Tabular.io to create an independent data platform. https://tabular.io/\nTo ask you further, can you tell me what else a filesystem like Hadoop should do? Isn't its feature set complete? Can you compare Hadoop and tabular if you have time?\nHeres what chatty says: https://chat.openai.com/share/9e206827-d7ea-4bb8-b316-3290c75920dc\nHow about from someone who knows what they're talking about rather than incredibly generic hand-waving? I'm half expecting \"it's web scale\" in this waste of time list. Just to pick on one bit **Why Iceberg is better for large analytical tables:** >**Schema Flexibility:** Adapts to changes easily. >**Efficient Queries:** Optimized for analytics, reducing data scanning. >**Transaction Support:** Reliable for concurrent operations. >**Compatibility:** Works with various query engines like Spark, Flink. >**Scalability:** Handles large datasets effectively. I dont even like Hadoop but this is flat out horseshit. Hadoop is famously compatable with Spark and Flink, Hadoop file systems was sparks original use case. Likewise with scalability, most of the worlds really big datasets are still stored in HDFS once you dig through enough layers. \"Optimised for analytics\" means nothing outside slideware and schema flexibility is ridiculous, HDFS has no schemas if you want \"ultimate flexibility\" what can be more flexible than naked bytes?\n\"let's make chat-gpt answer a topic I don't know about, what could go wrong\"\nSchema flexibility != No schema\nwtf\nDrank a big glass of haterade this morning I see.\nDid you wanted to mean 'limitation of Hive'? Maybe I misunderstand how they are related.\nCan someone who's worked at a very large/sophisticated org like Netflix explain why these places develop their own in-house tooling so much? Just in the first video he mentions two - a custom GUI interface to query multiple warehouses, and \"Maestro\", which is a custom scheduler similar to Airflow. Why not just use existing open source or SaaS vendor tools? Developing your own from scratch seems like a gargantuan task, and you're on the hook for any bugs or issues that come out of that.\n>Why not just use existing open source They do, but with a caveat because of legal risks. Generally, big tech corp keeps tabs on sanctioned open source tools because the big tech produces proprietary software. In the worst-case scenario, big tech may be required to release their proprietary software under the same license: royalty-free. >or SaaS vendor tools? Cost and politics. SaaS vendors want to vendor lock you and then charge absurd amounts. Especially effective with big tech corp because cutting the dependency and integrations is a painful task. At some point, the cost that the vendor wants to charge outweighs what the cost of internally developing and managing the tool is (or so they say). In practice, this means that they (often a team in India) builds a replica of the tool and you integrate with it. The tool can sometimes be good and sometimes be bad. Nonetheless, you don't get much say, but just a deadline for when you need to deprecate the SaaS for the internal tool some VP shilled for his team to build.\nyou want to own your own destiny. also, some of the most widely used tools were created by companies! if they didnt create their own tooling, you wouldnt have many of the best open source tools to start with. off the top of my head - parquet, presto, airflow, hadoop, pandas- i think? might have been a financial company wes was at - iceberg, pytorch. i almost feel its more rare to use an open source analytics tool that did not start at these companies. spark is a big one that comes to mind.\nAgree. In short, why depending on other SaaS company when you can create your own one from existing resources.\nAnother thing to consider is that some of the internal tooling predates the modern OSS equivalent, and so it ends up being a question of continuing to invest in the internal tool vs replatforming onto the OSS version.\nThey do use OSS to build their own tools. Big tech build their own tools in order to not relying on anyone else, to have a whole controlling on their tech stack (quick update, quick customization, proprietary one .etc).\nEchoing some of the above. The SaaS vendor argument is very much a \"control your own destiny\" argument. Imagine paying 100 DEs to work around the bugs a vendor introduced, while the company waits for a patch. And then paying them to unwind the workaround after the patch. And not just with bugs, but even new features, catching up to new standards etc.... constant workarounds (with their tax), waiting, unwinding. I think you have a very good question though in terms of open source. That ends up being a harder choice bc forking an oss tool could be (usually is?) a really good idea. It has some pitfalls - for example, in a high change context you could end up paying a pretty high tax to keep in sync. Maybe less than build-your-own, to your point. And to be fair Netflix does do this - hive, spark.\nWhat happened to their notebooks? Few years ago they were very vocal that write their pipelines using jupyter notebooks (source: [https://netflixtechblog.com/notebook-innovation-591ee3221233](https://netflixtechblog.com/notebook-innovation-591ee3221233)). I hated it, i joined one startup when people followed their example and it was disaster, no tests, packages installed from notebooks in production during execution etc....\nTo my knowledge, they always frowned upon using notebooks for DE work, but their platform had an abstraction layer in it (one of many layers), that functioned exclusively in notebooks. https://papermill.readthedocs.io/en/latest/\nLot of orgs are moving to iceberg as a replacement for their current big data warehouses. Wonder if there are any documentation that talks about best practices, limitations and pitfalls of using iceberg in production for a wide range of datasets.\nMy understanding is that iceberg is not a replacement for anyone's big data warehouse. It's just a smarter more operationally friendly file/table format for your big data warehouse.\nYes my curiosity arises on the production pitfalls to look for while replacing existing hive/impala/cassandra tables on hdfs/s3/azureblob layers with iceberg\nu/EnvironmentalWheel83 do you have any pitfalls you're particularly looking for? I'm building out documentation for Iceberg right now. The hard part about documenting pitfalls is that it's very dependent on the query engine you're using. Iceberg at its core is a bunch of libraries that get implemented by different query engines or python compute frameworks. If you're using a query engine like Spark or Trino, there's less of a chance that you'll run into issues provided you keep the engine up to date, but if you're using your own code on a framework, that's where I see many problems arise. There are some documented issues that arise around specific query engines. Some that I plan to explain that are quite confusing (even to me still) are the use cases where you would use a [SparkSessionCatalog vs a regular SparkCatalog](https://iceberg.apache.org/docs/latest/spark-configuration/#catalog-configuration). It's documented but not well explained. Most Spark users probably have faced when to use this but I primarily used Trino and python libraries so this nuance is strange to me. Is that the kind of stuff you have in mind or are there other concerns you have?\nThe major pitfall is obvious: Iceberg has zero implementations except the Java one. I.e. it's not even a standard now.\nKind of agreed, but major open source implementations are object oriented programming\nThat's not true, there's already [PyIceberg](https://py.iceberg.apache.org/), [Iceberg Rust](https://github.com/apache/iceberg-rust) is close enough that some folks in the community are already beta testing, and [Iceberg Go](https://github.com/apache/iceberg-go) is coming along as well.\nPyIceberg first release was a month ago? Lol At least they don't use spark anymore, or do they?\nFD: I'm a contributor for Iceberg. No, we moved the code out of the main apache/iceberg repo. It's [initial release](https://github.com/apache/iceberg/tree/pyiceberg-0.1.0) was Sept 2022. Also yes, we use Spark but also have support for Trino, Flink, among other query engines. There's also a lot of adoption around [the spec](https://iceberg.apache.org/spec/) which has me curious why you say it's not a standard.\nEchoing the top comment - most time that you spend reading these and thinking about these ideas will be wasted. Its very unlikely you will ever need something like this. Its interesting but not relevant to 99% of companies and teams. So I wouldnt spend too much time studying these or expecting a return on time spend with them. This is social media influencing like any other.\nYou are missing part of the point. Reading about the cutting edge of the field when you are at a company that isn't there helps you get an understanding of what the optimized \"end state\" can look like, which can in turn serve as inspiration and provide a sense of direction for teams earlier on in their journey. The point isn't to get deluded into thinking that you/your team can apply these approaches today, which is what the top comment was warning against. True, most companies will never get to this state. However it's hard to argue that most companies wouldn't love to get there if they could.\nIm not missing the point. You missed my point. I said dont spend too much time studying these and dont go off trying to engineer those sorts of solutions for your 100M row operation. Theres value in working on whats in front of you. _And then_ moving on.\nHow is that social media influencing lol, Iceberg is open source. That's a company outlining their tech design. If you don't need these tools it's fine, but it doesn't make these articles \"social media influencing\"\nWell its broad strokes but its true to that extent. The majority of the content from tech company tech blogs is totally irrelevant to most companies. I mean, its a recruitment tool and a brag doc for those engineers. Its not a guide. Super interesting, sure. But the ROI on time spent with it is negligible. If your goal is to improve as a de then youre better off with other material.\nYou're just making assumptions about who is reading it, ROI.... These are articles about state of the art of DE. Deal with it. A newbie probably wouldn't make much out of it. Others will. Is the book Designing Data-Intensive Applications bragging?\nThe assumption Im making is pretty safe - most companies arent anywhere close to Netflix scale. Most people that read these and try to implement something similar are gonna be way over engineering some thing for their small shop. Not just fresh grads.\nMy weekend just got booked with this post\nIn the \"The Netflix Data Engineering Stack\" ([link](https://youtu.be/QxaOlmv79ls?si=RhlmBdOFf6OhCnYG&t=250) to timestamp in video) there is a mention of \"shared table standards\". Does anyone know more about this? Is this public?\nTo my knowledge these are not public, and are basically Nflx's specific implementation of plain old good data warehousing principles in a multi-engineer context. Ex. Lots of agreed upon naming conventions.\nThanks, do you know of any other similar ones that would be public?\nI don't but Google returned some good results IMO(quickly scanned). This seemed very solid to me: https://blog.panoply.io/data-warehouse-naming-conventions#:~:text=Let's%20summarize%20the%20core%20data,Use%20underscores%20as%20separators Maybe it doesn't need said, but IMO, It's important to not take these too religiously. Everyone's context is different.\nThx for sharing. Watch it on the weekends\nI wish Netflix wouldn't start playing things before you click on them to start. It's really annoying. What if other things all worked that way? Windows media player just starts, your car turns on, oven is on 500 degrees!!!! If you look at it, it's starting!!!!\nWard\nSurprisingly less exciting infrastructure than I had anticipated. Also, I find it more practical to look at less gigantic companies as a realistic benchmark.",
        "content_hash": "3a1501b341f8bb5af11f84a90847330a"
    },
    {
        "id": "1d5sd3d",
        "title": "I parsed all Google, Uber, Yahoo, Netflix.. data engineering questions from various sources + wrote solutions.. here they are..",
        "description": "Hi Folks,\n\nSome time ago I published questions that were asked at Amazon that me and my friend prepared. Since then I was searching various sources, (github, glassdoor, indeed and etc.) for questions...it took me about a month but finally i cleaned all the data engineering questions, improved them (e.g. added more details, remove (imho) useless or bad ones, and wrote solutions. I'm hoping to do questions for all top companies in the future, but its work in progress..\n\nI hope this will help you in your preparations.\n\nDisclaimer: I'm publishing it for free and I don't make any money on this.  \n[https://prepare.sh/interviews/data-engineering](https://prepare.sh/interviews/data-engineering) (if login doesn't work clean ur cookies).",
        "score": 508,
        "upvotes": 508,
        "downvotes": 0,
        "tag": "Career",
        "num_comments": 50,
        "permalink": "/r/dataengineering/comments/1d5sd3d/i_parsed_all_google_uber_yahoo_netflix_data/",
        "created": 1717263593.0,
        "comments": "That's very nice of you to share. Thanks\nYou're welcome :) also if you will have any suggestions feel free to let me know\nWhy do you need folks to login if you dont intend to make any money out of it?\nIt helps me see how many people use my website, not bots, but real users. I understand the question, I really do - the interview questions will remain free. Also, There would be no shame in charging for this kind of information, as people's time and expertise are valuable. However, I choose not to charge, not out of fear or hesitation, but because I believe it's the right thing to do.\nYou can use google analytics to track such metrics.\nProbably to avoid being scraped by bots. A sensible move IMO. The fact that it's free is already a win, why complain?\nNice resource, thanks Under Netflix, the answer for \"SQL Queries to find time differences between two events\" does not work if a user has logged in and out multiple times. A more correct answer would use window functions to tie each logout event to its associated login event before calculating the time difference. I would also rate this \"Hard\", at least compared to the others\nThat's the nicest post i run into in data related subs. Thanks for your work! As a DA, trying to expand to DE field, i will check and try to solve all!\nHey nice work Can you please add Azure as well besides AWS\nThanks for the feedback, I will\nYes pls need some azure questions too\nThank you so much! I was just looking for a resource like this :)\nGuess you're going to be building Prepare.sh into a business eventually.\nInterview questions were taken from public sources and I think they should remain public e.g. free\nThis guy is collecting email addresses for future spamming and monitization :-)\nYarp, it's super obvious when it's not hosted on an open platform. I hope others see this and pay attention\nThanks mate.\nAmazing! Will this be updated with other companies like Meta?\nEventually.. but it will take some time. I have raw files with Meta's questions+many other companies, but bringing them to clean, relevant, nicely formatted state and adding solutions takes a lot of time..weeks.. as I have another full-time job. I'll prioritize Meta, Microsoft, Amazon, Apple, DataBricks, Snowflake.. probably by end of the month they are going to be added.\nIs this something you'd like help in?\nI'd love that, but since its a free resource I don't make money out of this and don't run ads I wouldn't be able to pay. Only thing I can do is to feature contributors in contributors page with link to their profile/blog/or whatever..\nIm sure we all can do our part for the benefit of us all; this is great, good on you. I dont know if Im qualified to help though\nWould love to help out if possible!\nI can help you out. I don't care about the money as I just want to improve my skills. Let me know how can I help you\nThank you!\nDamn, thank you\nThanks for being generous\nCHAMP!\nThanks for sharing! For me (on iOS safari) the show more buttons do not work\nGood list! Although if I was asked to design a CDN for a data engineering interview I might walk out lol\nI think there is a bug, questions by company are working fine, but when you select by technology it mixes that with other categories.. e.g. you can have SQL both in DevOps and DataEngineering, but it shows them all.. I don't think CDN question was meant to be in Data Engineering category. I'll fix it tonight :)\nI just realized that I'm quite qualified and what pushes me back is imposter syndrome. Thanks for sharing!\nAmazing!\nRemind me! 2 days\nI will be messaging you in 2 days on [**2024-06-05 02:02:14 UTC**](http://www.wolframalpha.com/input/?i=2024-06-05%2002:02:14%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/dataengineering/comments/1d5sd3d/i_parsed_all_google_uber_yahoo_netflix_data/l6upsjf/?context=3) [**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fdataengineering%2Fcomments%2F1d5sd3d%2Fi_parsed_all_google_uber_yahoo_netflix_data%2Fl6upsjf%2F%5D%0A%0ARemindMe%21%202024-06-05%2002%3A02%3A14%20UTC) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201d5sd3d) ***** |[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)| |-|-|-|-|\nnow train a model to solve all the questions. \nJust an fyi.. under the data engineering tab. Show more button dosent work for me :(.. but overall such a great work..\nThanks for the feedback, regarding the bug - I know, its work in progress, i'm fixing those bugs on daily basis. Come back after a while, there will be more content and less bugs :P\n\nThat's rare to see suhh a genuine effort to share knowledge with the community. Thabks for sharing. Very nice of you\nVery nice, thank you\nyou're welcome, if you have any suggestion please let me know ;)\nthanks\nIt's giving 404\nit's all 404 error now\nBro seriously , after stating this resource is for free, everything is behind paywall. Labs are understandable but atleast keep questions out of it.\nHey man, what is under paywall can you give me a link? It must be bug with the code since the questions are free. Re Labs - I don't have labs yet. Just sent me in DM what is not working and I'll find time to fix it..\nThis is quite amazing! With more and more questions added, this website can become a one stop solution for FAANG interviews preparation. There has been tons of materials available for SDE role, but nothing in specific for Data Engineering. This is really useful.\nIs there a reason why I see these companies always use SQL instead of a \"data frame\" language such as python libraries?\nI can only assume but perhaps SQL is a standardized across various database systems. Also I recall in other companies (particularly in Amazon which Im refining now) there were quite a lot of \"data frame\" questions.",
        "content_hash": "0ca51c1dd79ed7dda13891ea02e18ce7"
    },
    {
        "id": "109fhh8",
        "title": "Happy (or not so happy) Wednesday! What part of your technical work do you dread the most? What are you doing about it?",
        "description": "",
        "score": 497,
        "upvotes": 497,
        "downvotes": 0,
        "tag": "Discussion",
        "num_comments": 69,
        "permalink": "/r/dataengineering/comments/109fhh8/happy_or_not_so_happy_wednesday_what_part_of_your/",
        "created": 1673470036.0,
        "comments": "what i truly dread is debugging processes that involve no code.\nthis is how I feel about \"no-code\"/\"low-code\" solutions\nWelcome to my life, my contracting department used Outsystems, and we are at a point where the old systems are going to be replaced by this new shiny fangled thing. I came in to see how their data structures looked like. Its a bloody mess. There are no onboarding process from their teams to get us up to speed as to how their applications development teams handled data. We didnt mind highly normalised tables; we mind, however, that there are no data dictionaries, no data definitions, not even a documentation (*ooh, just look it up on* Confluence), saying, this table is a master table (i.e. its data is mutable), and when they asked us to build Power BI reports, all of their analyses were made on **transactional** tables (i.e. tables that dont change its records, but records are added every time theres something is modified on an operational level). Dont get me started on the gaps analysis; they just tell the operation and the data analysis peoples that are using the old system, Hey, we are going to decommission the old system, have fun with the new one we are building *just for you*! I took a course in my university curriculum about enterprise resource planning, and one of the key things I took away from that is, 75% of all IT project implementations failed because of poor change management. I have experience in data modelling, SSIS & its related stack (MS SQL Server, C# and SQL), and I also have experience in the cloud, performing site reliability engineering for a company with a government service contract, and Python. DM me if you are interested in hiring me so that I can get the hell out from this workplace.\nIt's sitting in someone's hacky work around that tries to implement a basic operation that the tool doesn't conveniently support yet.\nLearning about stuff with no documentation. Steve, it's not personal when I say I don't want to be on a call with you for hours at a time explaining how the internal systems work and troubleshooting them when I could just be reading it all as a markdown file...*if somebody could be fucking ARSED to write it down*. What I did about it - be the person to make all of the documentation. Not very fun as the new person, but I hope it makes somebody a believer one day.\nThe flip side of that is poorly written, incorrect, or unnecessary/overkill documentation. I've got a colleague that documents EVERYTHING about a process, complete with many many screenshots. A lot of it has nothing to do with our process, and is just documenting features of the tools / services we use. It drives me mad, if you want to know about a tool, use their documentation instead of writing your own.\nI had a DE team lead who only used the VSCode UI for git, instead of the terminal. He wrote multiple confluence guides on how to do different Git tasks with the UI, with page after page of screenshots.\nat least he used git, I once had a team lead who doesn't know how to use git and refused to used github because of the pricing\nOnce I ended up in the documentation rabbithole discovering that this exists https://www.writethedocs.org/conf/portland/2023/\nwtf this is a thing?! :D\nSuch a gentle soul, of which we are so in need in tech-comps\nWhat I truly dread is integrating with SAP modules. Who ever build the SAP GUI is a sociopath.\nGermans, so you are right.\nThe French\nThats actually on purpose, it's a common thing at least in German big software companies. 1. Build terrible UI, use as many acronyms as you can and don't document anything, make it basically unusable 2. Customers can't do shit 3. Release the Consultant that have internal documentation to fix/Setup every single thing there is 4. Drown in money\nThe UI is terrible and then if you work in a company that doesnt value documentation. Finding what the German acronym means makes me want to quit lol\nGermany's retaliation for ww1 and ww2\nFuck, and I cant stress this enough, everything to do with SAP\nDude, I feel you but still someone has to integrate these systems from time to time. I wish you luck transitioning into DE. I am absolutelly amazed how SAP is an ERP leader with the lack of documentation, lack of modern integration and garbage GUI they have. Just dont get me started on this system...\nIf i may ask, do you take the data from SAP and integrate into a cloud storage like S3/GCP? I work only on internal SAP processes in some modules, so I dont know how a scenario like this plays out. Do you use Airflow/python/the usual stack to set up pipelines? I agree with you 100% about SAP. I seriously could rant for hours about it..\nShort answer is yes there are data analytics use cases where you would do that. In our case we would extract data from sap s4 and load them into snowflake via an integration tool and then create a OLAP data model in snowflake which different user groups and applications could use. For integrating two OLTP systems like SAP and a CRM you would use an enterprise service bus architecture where you implement a publisher subscriber pattern in a middleware. The problem is, SAP does not provide a unified API nor an event bridge for every operation in its modules. It seems they didnt want to move into SaaS nor creating a unified plattform with an api first approach. Hence in my eyes SAP is a legacy system. Which feeds on their vendor lock in.\nNetworking stuff. SSL cert errors, VPN/VPC/PrivateLink/DNS blah blah. Mostly because I don't understand it. But it really just grinds my gears when IT says \"well let's step back, what are the requirements exactly\". THE REQUIREMENTS ARE IN THE TICKET FROM 4 MONTHS AGO. I NEED TO CONNECT TO DATA, STEVE\nSame - this stuff goes way over my head\nNot having the time to do as much as I want, as well-built as I want\nSame\nVery much this\nI just left a DevOps team for a big federal agency. One of our duties was to login at 7am, look at a group mailbox with 80,000 emails in it, search for certain mails to verify jobs completed successfully, and then manually send another email to the team with all jobs statuses. Like, you couldnt automate that?? I would have done it but gave up and left.\nI guess the guy before you did the same :)\nJennas Airflow Journal Day 1: What is ETL?\nAnything that requires me to do analytics style work. Im a big picture person I like infrastructure and moving large amount of data efficiently and writing robust fault tolerant software to get it done. Anytime I get asked to create a table of aggregations for a report or to write a report myself I die inside and drag my feet for weeks because I cant be bothered to actually work on it. DE != Analytics and Im a DE specifically because I HATE DOING ANALYTICS. I also hate Frontend Webdev and only ever work on backend APIs Anything frontend facing gets a big fat nope from me.\nMan, I feel you. I can do whatever kubernetes, backend, nginx, airflow, databases... But you ask me for a small CSS intervention? I'm sending the letter of resignation.\nI feel that, my responsibilities at my company end at the aggregation/union/joining of tables for a specific report. If there are downstream problems unrelated to the actual data, not my problem.\nLol I'm a DE that does a bit of both. I really enjoy data integrity checks and health monitoring processes. Too bad we don't work together I would help you track your bullshit.\nTroubleshooting kubernetes while spending hours starring at yaml files to find that one misplaced tab\nI am in pain reading this\nedited: I no longer believe anything I've ever said here.\nIf you write Python well, then you don't need many levels of indentation, but YAML often needs multiple levels. The alternative is using the weird substitution thing in YAML, but I can never remember the syntax for it.\nIndent rainbow plugins help with this\nI dread writing postmortems. In the beginning, they were good, productive, and felt constructive. As we got bigger and bigger at Airbnb, they devolved into what felt like punishments and finger pointing.\nIs postmortem a data outage?\nA postmortem is when something goes down or a sev0, sev1, sev2 incident occurs in production (data related or service related).\nMapping attributes from source to target in a temporary pipeline. A lot of shit work that we're going to throw out in a few months as we switch sources anyways but we're driven to do it due to some contractual obligations and some planning done by our product managers and my director. Whatever. I work from home and haven't had to wear hard pants during the work day in months.\nNo way that would take six months.\nI'm sorry, but if you have to spend 6 months debugging Airflow code, you are doing something seriously wrong!\nLike, using Airflow!\nHaving to find out new fields were added to the source system and not to the Data Warehouse. Then, having to manually add those new fields to the Data warehouse tables. For example, Salesforce Accounts and opportunity fields. Solution, I ended up creating a schema comparison concept into the Data Warehouse. If a new field was added to a Salesforce object, the Data Warehouse would auto detect the field and add that new field to the source table. This way Data Warehouse is up to date with source system fields.\nHow do you compare source table schema in another upstream and downstream target table? Could you please elaborate which tools you have used ? ..becoz if its both dwh table then i can see how this is done ...sry if its a silly question ..i am still a noob learning out my way into DE\nJust like you mentioned, it is dwh table comparison. My trick is to initially load the data to a stage schema. The benefits of my stage schema is that there are no dependency to other tables or reports. So I can drop those tables in stage and recreate them each time with our any issues. Once I had the initial data loaded into stage, I go ahead and cross compare my production tables and identify any missing fields from my stage tables. Then insert any new fields to my production tables. Finally, then actually run the data load to my production tables. Note, this is all done through a few store procedures.\nGotcha thnks for clearing the doubt\nHere are a few of mine 1. Having to put up PRs in multiple repos to get a pipeline working. 2. Oh it works in dev, but we don't have perms/table doesn't exist/table name is different in prod. 3. Having to track down permission issue. Extra when I have to open a ticket wait for weeks to get it resolved. 4. Slow build times & small dev cluster, oh you need to test your code, here take 5 min to build your code and another hour to run your code and check the outputs. 5. No documentation or over documentation which links to 10 other unrelated docs without any coherence or structure for it to be ultimately out dated. 6. Figuring out why a k8s task is not running, and incorrect spaces in config files. 7. Getting very frequently pinged with questions like: hey can i do map\\['key'\\] or map.get('key')\nHonestly I like what technical work I still get to do. Writing strategic planning six pagers is much, much worse.\nAmazon?\nNo, but ex-AMZN leadership.\nI would be curious to hear more, mind if I message you? I am doing six pagers and it seems like the only way people understand what I want\nsure\nManually merging and deploying deploying code to like 13 environments Been waiting on a Devops team to setup CI/CD for us for like 2.5 years...\nisnt it easy enough to set up ci/cd yourself these days with something like bitbucket pipelines?\nwe are at the point where we will probably just do it ourselves soon, unfortunately I'm not the PM so I don't get to pick what work we prioritize\nI handed someone a databricks etl pipeline a month ago. Today, they called and asked if I could help them with building out something for it. I thought they meant an improvement. Nope, they tried to build it from scratch. I then told them use existing code, no need to reinvent the wheel, especially since you said this was urgent time crunch. Got a call later. Hey, it's not working. Okay what isn't? Well we made our own copy and changed some things so it made more sense to us. Cool then, I'm done helping you people.\nI guess they thought that a simple copy and paste would work. nope\nLol they didn't even have to do that. All they had to do was run the book. Copy paste would have worked. But nope ground up. Because 10 lines of code, most of which was date setting was too complex.\nReproducing production errors without being able to know what caused the failure. Especially unexpected data deletions from different tables, which is an excruciating task.\nBeing a junior by YOE but being the person who has to basically manage someone on the team who is technically my senior but unreliable. I have to prod multiple times a week to see if work has been completed and tell them how build a certain feature despite me reviewing how to do things like that in their first 6 months. Basically I think I need a raise.\nAny kind of OOP code that is labelled as easy to maintain and read. Once the person completes hacking the stuff together in a dark basement - not even a year later something in it is deprecated it looks like a complete spider web of stuff that does nothing but call other stuff that calls other stuff. You have to search through a million files to find one line of code because they dont want to tightly couple their classes together so they spread them out in the hopes that someones somewhere would want to mess around in it and change stuff in multiple little files. nope.\nthere's a lot worse out there than airflow m8\nMapping\nBro, you didn't try debugging kubernetes yet  #k8s Or even debugging Airflow on Kubernetes \nSo true, I could cry.",
        "content_hash": "6bebcc3b1783f65d8b59c21f2a77c10b"
    },
    {
        "id": "1dye1bt",
        "title": "If you had 3 hours before work every morning to learn data engineering, how would you spend your time?",
        "description": "Based on what you know now, if you had 3 hours before work every morning to learn data engineering - how would you spend your time?",
        "score": 475,
        "upvotes": 475,
        "downvotes": 0,
        "tag": "Career",
        "num_comments": 149,
        "permalink": "/r/dataengineering/comments/1dye1bt/if_you_had_3_hours_before_work_every_morning_to/",
        "created": 1720458104.0,
        "comments": "Learn the basics of SQL (learnsql.com) and Python (Automate the Boring Stuff) Use your new knowledge to load a csv via SQL using Python. Apply some transformations to the data with SQL or pandas (do a Udemy crash course on pandas if you want to use it; at this point, SQL should be more than enough though.) Write the newly transformed data to a couple of different tables in Postgresql. Google around for how to do this. Congrats! You've got a simple pipeline running locally. Containerize your Python program with Docker. Upload your csv to S3, and figure out how to deploy your program to ECR and run it in ECS, ingesting the csv file from S3 and writing it to an RDS postgres instance. You can do an AWS crash course or use the AWS docs for these specific steps. Congrats! You've deployed a simple end-to-end ETL program to AWS, and have a core understanding of the fundamentals of ETL. Read the book Snowflake Data Engineering (Manning). Follow along with a free account. Recreate the steps above but with Snowflake-specific commands/processes. Concurrently, read the books The Data Warehouse Toolkit, and Designing Data-Driven Applications, in that order. Much of the warehousing methodologies in there will be relevant for Snowflake and other warehouses. Go to udemy for dbt and airbyte courses. There are a couple floating out there that aren't more than a few hours each. Learn how to use airbyte to ingest data from a source and load it into a warehouse, and how to use dbt to apply a series of transformations. You can hook these up to your Snowflake instance. Congrats! You've got a basic grasp of modern ELT approaches. From there, dive deeper. Maybe explore Tableau or Prefect; you don't need to know dashboards as a DE, but having a basic understanding of how they work with your stored data will reinforce some of the data modeling concepts that come up in your learning journey. Maybe explore BigQuery as warehouse alternative, or use dagster to orchestrate your dbt transformations. Depending on your commitment and your current knowledge level, the above can take you anywhere from 6 months to 2 years. But you WILL have a pretty good grasp on the core processes and methodologies at the end of it.\nThis guy suggesting good things.\nNice post!\nThank you!\nThis is awesome info, I have a question if youre willing to take the time. Lately Ive been working on a project parsing the html data from Amazon product pages from the common crawler database, then use that to check inflation. Im still working out the details for the actual pipeline but probably something similar to what you said, uploading the data as a CSV to S3 then ingesting into Postgres and containerizing all of this using docker. I was also considering orchestrating it with Airflow and running on a virtual machine, or possibly making a front end where URLs can be passed in and it will find the data available for that URL. Do you think it would be better to do a simpler project where Im still using the tools you described, but using data that already exists instead of scraping data? Im currently somewhere in between a BI developer and analytics engineer but trying to switch jobs ASAP since Im grossly underpaid.\n>Do you think it would be better to do a simpler project where Im still using the tools you described, but using data that already exists instead of scraping data? Yea, simpler is better if it gets you to create something end-to-end. You can always increase the scope and add complexity down the line. If you've already got your web scraper working (or close to it), one approach could be to turn that into a separate data ingestion step if you'd rather start off with a simple CSV. Here's an example: 1. Create a simple CSV file that contains the data in the structure you'd think you'd want from your web scraper. This can be pre-existing data, and doesn't have to be very much. It's just there to use as a placeholder for your ingestion step. 2. Upload the csv to S3, and use whatever tool you want to load it into your data warehouse. At this point, you've got a simple EL process (the extraction-load steps in ELT.) You can program your extraction step to check this S3 bucket periodically and ingest any new files uploaded since the last run. 3. Build out/finalize your web scraper as a separate process. Build it so that it runs periodically, scraping the necessary data from Amazon, and then writing the data out to a csv in your S3 bucket. Now, the ingestion process you created in step 2 has access to new data being generated by your web scraper, and you've got a continuously running data extraction/ingestion process. There'll be some data modeling you need to do to figure out what you wanna do with the data in your warehouse when you start accumulating more csv files, but the beauty of it is that you'll have access to the historical raw data that was ingested, so you can experiment with the transformation steps for that data (i.e., what you want the data schema to look like for your downstream consumers.) When you've got step 1 and step 2 working, I think you'll find that step 3 falls into place rather easily (relatively speaking.) From there, you can add another ingestion step (i.e., scrape some other consumer index or use an API for price data elsewhere to measure inflation), and do even more with the raw data you're storing in your warehouse.\nWow thank you so much for the detail! This is awesome\nI love these suggestions! Any suggestion on how to pick up different ways to identify and eliminate duplicates? Or different loading methodologies? Im working on conceptualizing and implementing gaps in loading data from various sources not just files. Most of the cloud based tools make it easy to track and load data eliminating duplication. Its a bit of a challenge when the source is an existing RDBMS\nSaving for my own studying, appreciate it!\nbloody amazing this one\nThis is the way\nThe way I've rushed to save this comment. Thank you good sir/ma Please please, I'm begging you in the name of Amadi oha, don't delete this comment. Abeg\nI just wanted to come back and say a huge thank you for this post. It obviously resonated alot with other members of the community as well, so once again thank you for taking the time to share your knowledge.\nThanks for the detailed answer. What would be your approach for a cloud based DE solutions? For e.g. currently the app I am working on is Azure based, so overall folks want the ETL to be setup with Azure tools (DB is SQL Server too) and not be worried about any modern stack.\nUnfortunately I'm not super familiar with Azure! Though as far as I know there are a lot of parallel/corresponding services between it and AWS. I recommend breaking out your desired pipeline into a series of steps (where's the data coming from? what do we want to do with it and what do the transformations look like? where's it being stored?) and determining which of Azure's products best meet your needs. It could be as \"simple\" as writing a back-end service in your preferred back-end language (probably Python, let's be honest) that extracts data via one or more APIs (e.g., using the s3 sdk to pull in the csv data using the example from above), applies the necessary transformations, and loads the data into your storage solution, with the service running in an Azure compute instance and the data being stored in your preferred db instance, also in Azure. But depending on what you're trying to do and what Azure offers, there are likely more sophisticated approaches.\nDude thanks for this advice, im going to follow it \nI am doing this thank you very much!\nI am currently deploying a Docker image for different environments to AWS ECR :) happy to see this in your suggestions as now I feel the hassle is worth it lol\nIs learning the above helpful for backend engineering?\nIn part, but if what you wanna learn is back-end development, there are better, more direct ways to go about it. There's a lot of overlap between the tools used, but the philosophies and the way the tools are used vary considerably (and even some software engineering best practices, like db normalization, aren't going to carry over to data engineering depending on what you're trying to do.) You can start with Python and Postgres for software engineering and learn more about deployments with Docker and AWS, but you're also going to want to focus a lot more on API development, correctly modeling business logic, authentication/authorization and other middleware, etc. Once you have the basics of Python down, I recommend something like [testdriven.io'](https://testdriven.io/courses/learn-flask/)s Flask course. Their courses in general are great (if a little aggressive in their pace and the amount of information they throw at you), but you'll get great exposure to modern tooling and best practices, including Docker and test-driven development. Make sure to read the comments in the code as you go through it and compare the new snippets of code to the previous ones - it's where a lot of the learning happens. If you ignore those, it's going to feel like you're just coding along to what someone else is doing and you won't get as much out of it.\nIve been using glue and lambda with Terraform. Dumb question, but dont lambda and glue use containers automatically when theyre deployed and are you just suggesting getting some experience with setting one up yourself?\nYea, glue and lambda are serverless, and they are perfectly good replacements for some of the stuff mentioned above! I recommend learning at least the basics of containerization via Docker (and subsequent deployment) because I think knowing how to write your programs locally and manually deploy them in different environments is a pretty important skill for developers to have; serverless solutions are great, but come at a cost and can sometimes increase the complexity of a project. A lot of new-ish developers make good progress building out scripts or sample apps locally, but get totally stuck when it comes to deployment. I think it's easier (and more beneficial) for new devs to learn how to containerize their work than it is for them to spend time tearing it apart so that it fits into a serverless architecture, which is its own subdomain of expertise.\nThis is exactly what I did... An end to end pipeline, bit by bit when I have free time. I googled and chatgpt'd alot though.\n>I googled and chatgpt'd alot though. As must we all\nCan I get a job if I know all this?\nHard to get a job in this market regardless. But in general, showing that you know it is the most important part. The best way to show you know it is with job experience (chicken-egg situation), but if you don't have that, publishing these projects on Github as public repos and linking to them in your LinkedIn bio, resume summary, and cover letter is a good second approach.\nIn the example I outlined above with ECS, it's running an EC2 instance under the hood. So yea, it'll cost you money depending on how much compute power you're using. Costs should be negligible with the free tier and minimal data for a toy project though\nIm a bookmark hoarder and shiny object chaser thats never satisfied and drags out my learning. Your advice is very sound and is going to help me a ton. Much appreciated!\nWow. Thanks for this\n[removed]\nFor sure; if you can show you can do the work above and can land an entry-level DE role, the minimum I'd demand would be around 85k, irrespective of COL in your particular location. That said, getting your foot in the door is tough in this market, especially for junior devs, so another viable route is an internal pivot from another position like data analyst, automation specialist, etc. Basically, if you have a job that involves lots of small data-processes (even if they're all in Excel), you have an opportunity to automate some of that data-related work with the skills outlined above. That would be a great jumping off point.\nI would add some Machine Learning basics to the list. Working with tabular data, using scikit-learn, xgboost. Even for DE you need to know how basic of how ML is working. Many times, as DE, you need to avoid data leaks in data that you prepare to data scientist, and you need to be aware of data drifts, so learning some tools on how to monitor data drifts is a plus.\nThanks a ton :)\nHi sorry for asking this basic question but where does PySpark comes into picture in here?? Im confused..\n[PySpark in DE](https://dataengineeracademy.com/blog/pyspark-tutorial-for-beginners/)\nThank you that's such a helpful article but please dont take this rudely or anything, but what I meant by my comment is where does PySpark comes into picture into your comment because you didnt mention to use it... Does it comes in use when you said SQL using python and transformation?? Again really sorry for such basic query and cheers for article\nSpark comes in picture when you have TB of data that would eat up a lot of memory and can not be performed by modules such as Pandas or Polar. He suggested a simple roadmap involving pandas which is optimal for small or up to few GBs of data.\nsuch a great guide for start studying, great answer, i will save it \nHeck yes glad to see Airbyte in there. Its not the end all be all but its useful and has a good community.\nThat's a pretty narrow take on data engineering. The true learning curve after learning Python comes from learning Spark & PySpark. I would also throw in AirFlow into the mix. You could skip AirFlow as it is a tool like most other stuff you mentioned, and tools come and go. But Spark teaches you distributed programming. Used to be Hadoop a few years ago, but nobody bothers with it anymore as they've mostly switched to Spark. And besides that, you need a good fundamental understanding of how message queues, message bus, and streaming pipelines work. Those are the fundamentals. I would also add data lakes, at least to get a fundamental understanding. They used to be gimmicks, but they really aren't anymore and are being used by many/most large and even medium enterprises. Not trying to be rude or saying you're wrong. Just saying that these are far more important things than learning some vendor specific stuff. I mean, it's not even like Snowflake is the only data warehouse in town. Or dBT is not the first or last data mapping tool. And in this space, tools come and go. A few years from now, you will be using some other tool.\n>Just saying that these are far more important things than learning some vendor specific stuff. I made it a point to talk about concepts and methodologies, but it's pretty difficult to concretely apply things without the tools; I mentioned Snowflake because I personally found the manning book to be very accessible and it's the warehousing tool I'm most familiar with, but it's why I also recommended exploring alternatives like BigQuery, and why I proposed starting with plain old SQL, Python, and Postgres for the basics. I'm not really sure how you'd go about applying the concepts without specific tools. >That's a pretty shallow 101 level take on data engineering Yes, most people (likely all) need to have a good grasp of the underlying fundamentals of any field before they can master more advanced concepts. >But Spark teaches you distributed programming. Used to be Hadoop a few years ago, but nobody bothers with it anymore as they've mostly switched to Spark. Big data engineering is not the only type of data engineering. >And besides that, you need a good fundamental understanding of how message queues, message bus, and streaming pipelines work. Those are the fundamentals. Disagree; they are important concepts, and will come up in certain data engineering environments, but they're not the only or even the majority of data engineering architectures. There are plenty of batch pipelines running on schedulers. Ultimately, for someone getting started with DE, the steps I outlined provide exposure to SQL, Python, Docker, a cloud environment and cloud deployment, data warehousing, data modeling tools, and data transformation, which I think covers >80% of typical DE work.\nI basically did that, although it wasn't 3 hrs per day - more like 30 mins to 1 hr. I learned SQL by playing around with the Adventure Works database. Then I learned enough Python to connect to the SQL server and read data as pandas dataframes. Then I learned how to set up cloud storage and load those dataframes to it as CSVs. The majority of my day-to-day work now is covered by what I learned doing that stuff, it's mostly just variations on the same skills.\nI'm exactly learning this, also with adventure works. Do you have some projects on your GitHub I could see ? I also orchestrate with airflow. You didn't speak about spark nor docker, may I ask why ?\nUnfortunately I don't share my GitHub on Reddit. I was already in a data ops role before I needed to learn anything about Docker or Spark, so I learned those on the job. I've never worked with Airflow. These aren't bad tools to learn, they just weren't what I was focusing on in my free time.\nThis is the answer, skip the influencer crowd. Theyre just flash and so is their content (kind of had to be) but i think it misrepresents the work.\nI saw a YT short of one of them straight up explaining how he used FOMO tactics to get people to sign up for his course. Like I get that we know people do that stuff to get sales, but it seemed bold to talk about it so plainly.\nAlways interested me how FOMO (Fear Of Missing Out) could also be an acronym for Flavour Of (the) Month Only. Specific tool knowledge is nice to have, but fundamentals and experience transfer to all tools. Knowing what an index is, how to SSH into a server, what CICD is, why Docker is used, when to use commit vs. push in git etc. aren't exciting, but are fundamental knowledge that everyone working in DE should be comfortable doing. You can learn how to do all of that for free too.\nThis is awesome advice - downloading the adventure works database now! Thank you!\nWhat did your resume/GitHub look like to land a DE job? I know all of this minus the cloud and can't make the transition.\nI had about 1 year experience as an analyst using SQL. I wouldn't say my GitHub is stellar - I spent a little time making the landing page look nice, but a lot of my projects aren't really complete end-to-end projects. I made sure I had some SQL and pandas in there and that it was commented, along with some basic readme explanations. I transitioned from the analyst job to a 6 months data ops contract job, and once the contract was up I was hired as a DE. Granted, this was a couple of years ago when the job market was a bit hotter. I wouldn't be surprised if cloud was more important than some of the other stuff these days. I haven't written meaningful code in almost a year, and recently I've mostly been putting basic SQL statements into Azure Data Factory copy activities. I can't speak for the industry at large though.\nIf I were reviewing your resume, I'd expect to see some knowledge of Git / SQL / Python / Docker / Linux / Postgres / Go / AWS then some talk about what you (not your team) did and why you want to move to DE. Go to some meetups and try to get referrals to local companies though, because the in demand skills in your location may be significantly different from those here.\nTake it one step further. Learn all of that and then do it in AWS https://aws.amazon.com/free/?all-free-tier.sort-by=item.additionalFields.SortRank&all-free-tier.sort-order=asc&awsf.Free%20Tier%20Types=*all&awsf.Free%20Tier%20Categories=*all\nReading & implementing what I just read. Mostly focusing on the basics - reading up on data modeling (Kimball, Inmon), slow changing dimensions, building ETL pipelines, etc. I wouldn't worry at all in the beginning on which tech & tool I use for practice these concepts. I'd first focus on really learning the concepts. So download Postgres on your computer, or use SQLite/Duckdb, or whatever is easiest! Also to build muscle memory, I would practice these fundamental concepts using SQL and again using Python. This would help me be a god at both SQL & python. It would also help me realize that SQL is more than good enough at solving many many problems, and brining in python can easily lead to over-engineering solutions to problems. Once I'm feel pretty knowledgeable in these concepts, I would sign up on GCP, get the $300 free credit and start learning about the cloud & modern tools that you see listed in job descriptions (like DBT). I'd probably try to set up a project that pulls data regularly and build a data warehouse + some sort of quick n dirty reporting on top of it (a streamlit app or looker dashboard). If I have the time, I'd also venture into the DevOps side of things, get familiar with Terraform and using GitHub actions to build CI/CD pipelines for my project. Worth mentioning that along the way I'd continuously be reading up on best practices & trying to implement them. So for example, writing unit tests for my code. 3 hours is a solid session to dig deep. Doing that every day will get you pretty far after a couple months! edit: Forgot to mention, it's important to go outside and touch grass once in a while. Don't over do it and don't over-work yourself. Like others have said, it's more important to take care of yourself and do some fitness than sprinting to learn this stuff.\nSo, this is kind of what I have been doing or at least attempting to do - Initially I was reading up on the Azure Cloud platform, taking notes, reviewing and then working on a personal project. The idea is to build out a pipeline for my financial transactions. But my SQL and Python skills are weak. I think I have the right idea - but I need to focus on the fundamentals Python, SQL etc. then come back to the Project.\nhow would you put what you learn on your own time on your resume? Would it be just under personal projects?\nYeah I think the best way to show what youve learned on your own time on a resume would be through personal projects that encapsulate those skills. However Im new to this, someone else on here could have better advice.\nIf I had to apply to a junior/entry role today I would embellish and stretch the truth as much as possible on my resume. The sad truth is that your resume is first going to go through either an AI or somebody from talent acquisition who knows nothing about tech & data engineering. It doesn't matter (in most cases, not all) how amazing your resume or your personal projects are, TA is just going to look that it ticks off the boxes (such as having 10+ years experience using DBT lol), so I would just copy paste all the keywords from the job description into my resume for each role I apply to and put what I worked on while learning D.E under \"personal projects\" and the \"skills\" sections. Here's another brutal truth, and this may be entirely just my experience and not an objective truth, 95% of resumes & personal projects look the same, especially after going through a lot them . Projects & experience don't by themselves make you stand out. How well you can speak about them and defending why you did things a certain way will make you stand out a lot more as a candidate. This is where having a strong foundation of data engineering concepts instead of knowing all the hot new technologies really make you stand out. A boring personal project using boring tech & tools but with a strong knowledge of D.E concepts will get you a lot further than a flashy project that has the most cutting edge tech stack if you don't have a strong foundation and can't speak to why you chose those tools, what problems they solve, etc.\n1 hour SQL 1 hour Python 1 hour Cloud Provider of your preference\nIs that really all it is? You just load tables to the cloud and sql them for reporting? I think Im ready for de\nIt's way more complicated than that.\nSpends 5 ~~minutes~~ hours dealing with a config file for putting code into production.\nLOL\nThats what they all say\nSounds pretty easy\nObvious troll or just a noob?\nNo it's not that simple, he asked what to learn in 3 hours per day, so I went with that basics, but it isn't as simple as that\nIn some way yes many DE jobs look like that as seen from above, but all of those steps can be very hard depending on the context. For example dozens of shitty data sources you have to write custom logic for, very complex business data that takes months to understand and model with countless trials and errors, impossible SLA compared to the means you're given, very high volume that costs your company a fortune that you are asked to reduce etc.\nGo for a run, walk my dog, make breakfast then learn with the balance of time.\nI actually do have 3 hours before work every morning to spend learning data engineering. I spend that time working on my fitness instead.\nThis is such a useless and rude comment. Absolutely unhelpful.\nThanks!\nI disagree. It brought me back to reality that life is not a race and you should not forget your own health or enjoy life aswell.\nYou dont need 3 hours a day to exercise. 3 hours for learning is probably also excessive but OP is looking for specific advice, we dont need to assume they are ignoring their health.\nLSD ?\n[Long Slow Distance](https://en.wikipedia.org/wiki/Long_slow_distance) ( u/Placebo\\_LSD did say they're working on their fitness after all)\n3 hours is a real luxury! But I like the philosophy of kaizen. A little each day. Sometimes I do certs and hack away at them, but the best learning comes through giving things a shot. I try to stretch my skills when my kids go to sleep. My work environment is mainly politics and PowerPoint architecture so I miss engineering. I sketched out a blueprint of a solution I thought would be fun and challenging. Then found the components and put them together and everytime I hit a barrier I would have to overcome it through learning. Some things ended up being dead ends. And some things ended up becoming rabbit holes. The solution was a simulation of a hospital in realtime using FHIR, streams and lakehouse architecture. Using synthetic data generator. Custom flattener using stream processing. Poor man's lake house using parquet, duckdb , dbt and superset for visuals. Custom observability solution over Kafka using python and duckdb. Custom orchestrator using python. Running on docker. Leveraged openai a lot to help kick start a lot of the python. It remains somewhat of a messy skunk works but it certainly was fun and does continue to teach me a lot! Next step is plug in datahub and shift from parquet to deltalake or iceberg when duckdb can write to delta or iceberg (I hope). Also map the FHIR stream to datavault. Also incorporating local LLM to help with some engineering tasks such as schema inference, as well as free text categorisation. I predict it will take me forever to get around to it, but as it's my never ending 'dad project'\nIf you ever get a chance to document the project in detail, would love to read it.\nIf I had 3 hours before work I wouldn't spend it learning data engineering\nThis guy fornicates.\nWhat about the rest 2 hrs 58 mins?\nGym\n?\nIt depends what you can do and not. Spark, Kafka, dbt, cloud, docker, kubernetes\nI can't do any of these things currently lol\nI mentioned in another comment - but I am decent at pulling data into Power BI to build out reports and dashboards at the moment.\nLearn Python, sql and data modeling. After learn pyspark and dbt. These two shouldnt take too long. After take a cloud provider like google cloud and make a end to end project\nRead these foundational research papers to learn me some SQL: * [(Codd 1969) Derivability, redundancy and consistency of relations stored in large data banks](https://technology.amis.nl/wp-content/uploads/images/RJ599.pdf) * [(Codd 1970) A Relational Model of Data for Large Shared Data Banks](https://dl.acm.org/doi/pdf/10.1145/362384.362685) * [(Codd 1971) Normalized Data Base Structure: A Brief Tutorial](https://dl.acm.org/doi/pdf/10.1145/1734714.1734716) * [(Codd 1971) Further Normalization of the Data Base Relational Model](https://forum.thethirdmanifesto.com/wp-content/uploads/asgarosforum/987737/00-efc-further-normalization.pdf) * [(Chamberlin, Boyce 1974) SEQUEL: A Structured English Query Language](https://web.archive.org/web/20070926212100/http://www.almaden.ibm.com/cs/people/chamberlin/sequel-1974.pdf) * [(Chen 1976) The Entity-Relationship Model - Toward a Unified View of Data](https://dl.acm.org/doi/pdf/10.1145/320434.320440) Keyword: _Foundational_. I don't even like SQL or the ERM, but still would recommend them for anyone who works with relational databases. These are a lot smaller than the [PostgreSQL 16.3 Manual](https://www.postgresql.org/files/documentation/pdf/16/postgresql-16-A4.pdf) of a whopping 3000 pages (which I'm digging through lately). I did a little SQL test before I read these (including the manual) and was stuck at 37% and being very confused. Afterwards I could get up to 87. That was the [LearnSQL Skill Assessment](https://learnsql.com/sql-skill-assessment) (no affiliation, other than that I made an account, did the test and got a useless certificate out of it  ) TBF, I've read a whole lot more research papers (including relational Algebra and relational Calculus), but these I would argue lay a nice foundation.\nHey Ive found these helpful, I was wondering if you had any other papers or resources you recommend?\nI've got the full list of all his papers on my website under ['The Papers of E.F. \"The Coddfather\" Codd'](https://thaumatorium.com/articles/the-papers-of-ef-the-coddfather-codd/), since I wanted to find the papers via the web, and there was no other comprehensive source, so I made one. (if that link ever dies, search for it on [web.archive.org](https://web.archive.org/web/20240910183236/https://thaumatorium.com/articles/the-papers-of-ef-the-coddfather-codd/))\nI will try to find a linkedin learning course on how to communicate better with non-tech/semi tech/moron stakeholders.\nI'd get more sleep.\nSeattle data guy youtube channel has a video on learning data engineering in 100 days, maybe take a look.\nAwesome - I will check it out.\nJust projects\nSo this is what I intended to do. I am working with my own financial transactions with the hopes of serving them for analytics and maybe some machine learning to predict trends. However, I found myself leaning on ChatGPT to much for the Python so I thought I should re-evaluate my strategy.\nWhat do you already know?\nCurrently I am decent at pulling data from a source and building out reports and dashboards in Power BI. With that being said, there is a lot more I don't know.\nI bought stock market data from Nasdaq datalink. Then I set up a single node Apache Hadoop/Spark/Hive system in my basement (on Ubuntu), stored the data in parquet files, and exposed them via Hive. Then I installed Superset to fuck around - slicing and dicing the data, doing basic EDA, etc. I'm using it to learn machine learning, data science concepts, and just generally learn things about financial markets. My end goal is to find an index trading strategy that outperforms a simple SP500 Buy and Hold. It helps I have some background already in SQL, Python, Java, and BI systems.\nJust a note of thanks for bringing in this topic. I have 30-60 mins on daily basis and this would serve me well.\nOf course man! But dont thank me. This sub is amazing!\nI feel overwhelmed with all the tools (most of them doing more or less the same thing) and their place in the pipeline. My firm mostly works with Azure and slowly I am getting a hang of it (ADF and Synapse) and suddenly it was all about modern stack (dbt, Snowflake, Airbyte etc.)\n1.5 hours: stretch, cook a really nice hot breakfast, watch the morning news. Enjoy waking up. 1 hour: hands on programming. 0.5 hours: watching YouTube videos explaining things I don't understand.\nId learn cyber security instead\nThats a refreshing take. This is the 1st comment suggesting learning something else. Im interested - why do you think cyber security instead of days Engineering\nDBT and Python.\nBuild little tools that make doing data work easier and enjoyable. Even little sql or python snippets. You need to build motivation cycles for yourself. If its all theory or generic template projects, itll make you good a reading and copy and paste, not a better data engineer.\nMotivation Cycles - that makes a lot of sense to me.\nI mainly learn on the job. If I had three extra hours everyday I would not typically spend it studying data engineering.\nI would spend it with my family and sports.\nPeople are recommending books and shit, but I say just start building. Heres some examples of projects I started with: I learned on the New York taxi dataset. I did some data engineering using Spark SQL, created an ML model, then finally hosted that ML model using flask and AWS beanstalk. Next, I created an api to do inference. Finally I scripted it all out to deploy pipelines automatically. Instead of ML, for consumption, you could take the visualization route with something like Tableau or powerBI. My next project, I used AWS deeplens and their facial recognition service to stream realtime events (using Snowflake snowpipes & S3 triggers) when a face is recognized from a vector database. I hosted a react S3 static website to show events (hosted in dynamoDB). I got very scrappy with this and actually created a just in time registration app to index guests at an event using iOS shortcuts app and an iPad. Another project I did was scrape credit card intro bonuses using selenium and recommend me the best value intro bonus. It let me know when bonuses changed. I took this even further and distributed on lambda functions to scrape hotel websites for prices and categories to find the best $ to point ratio for stays. (Hint: its Park Hyatts and Andaz in Costa Rica). Scraping Craigslist for items you want is helpful to. When covid hit, I helped a start up scrape a given markets restaurant scene and menus to create a map of available delivery options. Theres a lot of cool pet projects you could do.\nAre you up at like 6?\nIm up at 5 to work out - and then from 6-9 I study and then start work at 9\nHonest question, how much do you sleep / at what time do you go to sleep?\nI try to be in bed by 9pm - however we have a 7 month old and were right in the middle of sleep training so this varies.\nWow, respect\nOutside doing something other than learning data engineering.  But that's just a reflection of my personal mindset at the moment.\nLOLz Im still chuckling about having 3 hours before work. But I digress, that amount of time should make ramp up pretty quick! \nSleeping\nThis is exactly what I need lmao\nI'd sync my financial accounts into am excel sheet. Hyperlink the ticker symbols of my investments, calculate % on open and close , run credit card expenditures against each other with cash back and points included on the daily, note checking / savings account along with outstanding loans or another expected billings biweekly, then transfer that to the calendar to have a bar and pie chart each day with tallies of saved money, spent money, money to spend. . . Automation of the processes is not feasible and the data obtained while being insightful is rather useless. Exactation is not required for the fluid of finance the issues come when people round down not up or vise versa. So I'm spending my time blissfully asleep and enjoying the three extra hours after work\nReading Reddit, Medium, Substack, and watching YouTube, then playing with anything that piques your interest.\nI do proof of concept work which I plan by how much time I have. I just did one that was spark ETL with Redshift incremental processing to get fastest and cheapest time and cost. I just used one table I had in prod as my subject so it didnt get to complicated. The final product was terraformed so we could use as a template for any source. I like to use scientific method where I have a hypothesis that I can make something faster better by do ing X and then I do my test in the most simple fashion so if its a success, its something I have facts to back a proposal on a project. If it fails it was a lost hour or 2\nMore sleep\nBuild all the things.\nRemind Me! 7 days\nLearning by doing. If you are already in software engineering - it's good. Read some of generally popular books like \"Designing data intensive applications\" and try applying knowledge from it. Try building Distributed Key-value database with LSM engine in any language of your choice. If you are coming from data analyst background - start with general software engineering. Not algos, how to actually build anything.\nProbably playing video games.\nLeaving that up to ai. lol\nexercising probably\nDepends what your goals are and where you are at right now. Having spent about 2 years now learning towards Data Engineering from an unrelated background, pivoting towards Data Analysis to get into the data industry is my current plan. So if you completely new with no degree to back you up, maybe consider Data Analysis as a stepping stone? Luckily a lot of the skills are transferable so I've not wasted my time.\nDrinking coffee, playing video games, going for a walk\neating breakfast, working out, trying to enjoy life before the grueling drudgery of work takes its toll on my good mood. If you want to learn data engineering, try to start finding ways to do it on the job (or use that time to start looking for a job where you can do so)..\nId spend 2 hours doing your mom and then spend 1 hour studying data engineering.\nhere's my $0.03's worth with a visual to boot; [https://lestermartin.blog/2024/02/20/becoming-a-data-engineer-yet-another-top-10-list/](https://lestermartin.blog/2024/02/20/becoming-a-data-engineer-yet-another-top-10-list/)\nWatch YouTube videos for half the time and then implement/code what you watched the other half the time. rinse and repeat. There are videos on EVERY language, EVERY Framework, EVERY DB Engine, etc on YT. You can literally watch CS classes from the likes of Yale, Harvard, Stanford online for free.\nScrape your favorite website and design your own database. That will teach you python and sql at the same time. Or even better get a data job, any kind of data-(insert over hyped buzzword title) will do\nDo you know Spark, Airflow, Kafka?\nHi everyone! So, i'm a Data Analyst and my company wants my Data Analyst team to build a Data warehouse using the Apache eco-system. I havent used Apache ecosystem and im required to build the Data warehousing using Hadoop and some of it's ecco system. Can you point me to where i can learn how to build this system? or perhaps share your views on how i can build this or the best technologies to use. Thank you\n*\\0/**\\0/*\\0/*OO_ Que om.\nKeep it simple. Sql, python, DE Paltform ( snowflake, dbt, AWS/Azure / GCP) In SQL - make sure you cover basics, intermediate and advanced concepts, leetcode easy, medium and then every interview question on youtube, make notes ( table, question, query ) In Python- basics, basic DSA, Leetcode easy and medium, libraries ( can focus on these if interview gets scheduld ) Pyspark- prashant kumar pandey udemy courses ( all de courses are amaze) Kafka ( Prashant kumar pandey, itvarsity from udemy)\ngetting tutored by ChatGPT\nSee this was my initial idea. I told myself I would just dive into a personal project and lean on ChatGPT to learn as I build it out, but now I am starting to 2nd guess this strategy because I don't know enough to understand why the output from ChatGPT was working lol - I think its definitely valuable but maybe more valuable with more knowledge.\nyeah, when learning I've discovered it's best to avoid ChatGPT. It gives good answers but that doesn't help you learn anything. Nothing reinforces a concept like having to figure it out on your own\nI would suggest spending an hour of that time on this free course, https://coursera.org/learn/genai-for-data-engineers-scaling-with-genai.",
        "content_hash": "9a7a1ae34e7d731bee6af3008f44a166"
    },
    {
        "id": "1e7fcmx",
        "title": "What I would do if had to re-learn Data Engineering Basics: ",
        "description": "1 month ago\n\nIf I had to start all over and re-learn the basics of Data Engineering, here's what I would do (in this order):\n\n1. Master Unix command line basics. You can't do much of anything until you know your way around the command line.\n\n2. Practice SQL on actual data until you've memorized all the main keywords and what they do.\n\n3. Learn Python fundamentals and Jupyter Notebooks with a focus on pandas.\n\n4. Learn to spin up virtual machines in AWS and Google Cloud.\n\n5. Learn enough Docker to get some Python programs running inside containers.\n\n6. Import some data into distributed cloud data warehouses (Snowflake, BigQuery, AWS Athena) and query it.\n\n7. Learn git on the command line and start throwing things up on GitHub.\n\n8. Start writing Python programs that use SQL to pull data in and out of databases.\n\n9. Start writing Python programs that move data from point A to point B (i.e. pull data from an API endpoint and store it in a database).\n\n10. Learn how to put data into 3rd normal form and design a STAR schema for a database.\n\n11. Write a DAG for Airflow to execute some Python code, with a focus on using the DAG to kick off a containerized workload.\n\n12. Put it all together to build a project: schedule/trigger execution using Airflow to run a pipeline that pulls real data from a source (API, website scraping) and stores it in a well-constructed data warehouse.\n\nWith these skills, I was able to land a job as a Data Engineer and do some useful work pretty quickly. This isn't everything you need to know, but it's just enough for a new engineer to Be Dangerous.\n\nWhat else should good Data Engineers know how to do?\n\nPost Credit - David Freitag",
        "score": 465,
        "upvotes": 465,
        "downvotes": 0,
        "tag": "Career",
        "num_comments": 77,
        "permalink": "/r/dataengineering/comments/1e7fcmx/what_i_would_do_if_had_to_relearn_data/",
        "created": 1721425435.0,
        "comments": "Learn database partitioning/indexing. Understand the performance impact of range partitions vs hash partitions, and when to use either. Same thing with indexes. Understand the impact and when to use a btree, hash, etc.\nFor a traditional RDBMS, sure, this doesnt really fit with the distributed cloud data warehouses referenced above.\nIt's far less important, sure, as the technology improvements of liquid clustering/automatic partitioning/zorder/etc. take the day to day thought away. But it's still good to understand partitioning to understand how distributed workloads work, as well as the basics of partitioned storage for optimized writes. I always see people starting out who don't understand this, try to write to csv, and get frustrated when they output multiple files.\nWhats the heuristic or rule of thumb you use for this? I.e. what to partition by etc?\nPartition based on the column which would split the dataset into roughly equal sizes with a cardinality that is neither too high ( which is bad, lots of I/O ) nor too low ( bad if each partition is too big to query ) Generally speaking, date columns from metadata ( which are either used as is or converted into years / months ) work well. I have seen tables partitioned based on date_trunc( creation_date, MONTH ) work exceptionally well for very large datasets of PB size. Partitioning Column's Cardinality till a few 1000s are manageable, more than that it becomes difficult as there are too many files , especially if all of the partitions are queried together.\nWhich is why it fits into the what else category\nOn the contrary, thinking indexing or partitioning isnt important seems like a lack of experience. Partitioning and sharding is absolutely important when it comes to cloud based storage or databases. There is also clustering and the way keys are designed can have a real impact on hotspotting on performance like big table. Even on columnar databases like BigQuery or anything really, choosing a uuid might seem great as identifier, until your data grows. Using ingest date as a partition might seem like a great catch all, until you realise that users are doing a full table scan based on that not being the relevant filter date. Even late arriving data can be scoped in on relevant dates if the ingest process is designed properly. Database design is important even on the cloud and I can cite numerous examples as above, that have affected companies, resulting in vastly increased costs and performance, particularly in deciding keys (which I see as indexes as well) and partitioning. They also had a similar view probably at the outset. In almost all places where Ive worked, Data Engineers with the outlook tend to get asked to make alterations by Finops or consultancies, to improve performance and cost savings because the issues were overlooked or not thought through because they were seen as unimportant. Ignoring lessons learnt from RDMBS systems is very short sighted. Many of these optimisations still end up relevant. I mean ignoring Postgres if youre using RedShift seems particularly misguided (though granted in that case, still RDBMS). Overall the more problems and solutions youve seen, the more experience you have. From a technology point of view, just because there is not an index defined on the table, doesnt mean poor key design and partitioning cant cause appreciably increased costs.\nSince you didnt read it, he specifically references Snowflake, Big Query, and Athena which are typically used for data warehouses but have a very different architecture from traditional RDBMS systems. In Big Query, for example, the resources and expense typically scale up linearly based on that architecture. Unfortunately your costs typically scale up as well. The upside is you typically dont need an index as the warehouse grows and in some cases it can actually decrease performance of Big Query. Also, in a traditional RDBMS the number of rows expected/returned can decrease performance so we do things like selecting the top 100, in Big Query the number of columns have a bigger impact on performance. Source: GCP Staff.\nWhat indicates I didn't read it? I mean that comment seems more like asserting your stance that traditional indexing/partitioning doesn't apply to BigQuery than any specific example that indicates I didn't read anything. Just because someone doesn't agree with you, doesn't mean they haven't read. Indexing doesn't exist in BigQuery, so it can't decrease the performance. It seems weird you're citing GCP Staff, which is either you are indicating you work for Google or you're citing just generally people who you've spoken to and then stating Indexing will slow BigQuery down when it doesn't exist. So why do I say indexing is important? I'm referring to key construction, because even if indexes don't exist, clustering, partitions and joins do. I mean tbh basic overall response is like something you'd get from asking ChatGPT about the difference between a columnar database and an RDBMS, not anything insightful. If you use a UUID, performance will be quick, but if you need to join on that field, it will increasingly get slow. While defining the field as a index metaphorically speaking as a index against unique values, you get a significant improvement or alternatively as mentioned using a shorter hash. I gave examples, but you don't seem to have appreciated it. The point is, joining on something with 32 bits column is way quicker than 64 bits column, even if the tablescan is the same. Keys still matter. This is why the comment by u/Typicalusrname seems relevant. Index creation, specifically from my point of view is the elements you need to join on. Which is still relevant, even if indexes don't exist. Regarding partitioning, it is pretty important to ensure that partition, if it can be, is against the date that is commonly searched, both HQL, hadoop, BigQuery and Snowflake all will reduce the amount of data scanned (not just extracted), based on a successfully identified partition. For example, if your entire analysts department is doing reports against order date and you partition by the date the data comes in, every single query will execute against a non-partitioned field resulting in a full table scan, if your datalake or datawarehouse extends up to 11 years (a max of 4000 partitions) you're talking a lot of data. It's a design flaw, but can be fixed with some views to limit data against ingest period. Again it's not noticed by end users, because BQ is generally quick and they get only the data they want. But when I see the queries, I can optimise them by using ingest date as a filter (with wide range to cover late arriving data) and cut the query costs down. This might seem small, but depending on how many years your data goes back and the analyst team size, this poor design could easily result in thousands extra. I mean sometimes there is luck and most queries will execute against cached data, but ideally design comes first. I've provided specific examples (that I've seen in the wild) in more detail and why database design in RDBMS can relate directly to implementations of BigQuery data. I appreciate these are the same examples, but they are in both more detail and were not acknowledged before. You're free to respond, but I'm not going to engage further with someone I genuinely do not believe has used BigQuery and is indicating how partitions aren't relevant and how indexes slow BigQuery down. Note: clustering can make writes slower, but generally only improve read performance. I would not really refer to this as indexing either.\nWhat would be a great resource for these? I always feel I have a gap in my knowledge here.\nDesigning data intensive applications. After that play around with a database. Lots of knowledge is hands on. Pull in a billion records and play around with it, and try the above mentioned things for varying requirements. Seems simple but thats actually what rounds you off from the database perspective\nThis is great, I believe I have that book already! Thank you.\nWhere can I can a dataset that have so many records?\nLook for some datasets on Kaggle. UCI ML data repository is also decent. Opendata.org is yet another public data set repository I keep bookmarked and review from time to time. The 1 Billion Row Challenge is also a good one: https://github.com/gunnarmorling/1brc\nhttps://huggingface.co/datasets https://planetarycomputer.microsoft.com/catalog https://archive.ics.uci.edu/datasets\nMy experience, learning Terraform (IaC) before starting infrastructure provisioning (cloud, VMs, Docker...) was a game-changer. It took me a day, but made my life much easier. Highly recommend it!\nIts awesome it took a day for you to learn Terraform. As someone who is very new to IaC, which Terraform resources are best to learn it in a short amount of time?\nYoutube to understand basic syntax then googling \"DE pipeline with terraform\". I often go to Medium source. It helps to prepare infras for most of DE demo project\nAny example projects youd point someone to learn terraform for DE pipeline type project?\nHere https://medium.com/@amarachi.ogu/building-a-real-time-streaming-data-pipeline-in-aws-using-terraform-87292be45455 https://medium.com/@todd_6710/infrastructure-as-code-built-data-lake-with-aws-glue-s3-athena-and-terraform-78bb54339f1a\nAny prerequisites to learn terraform?\nI like your list and this is a worthwhile post. But for what it is worth, in my experience, pandas is unnecessary\nCame to comment about pandas being not useful. Every time I'd create a dataframe starting with data from a database, I'd want to create a new column and go through all the hoops to create functions to apply, and then I'd realize, I could change the initial query to get one more column. Eventually realized it all could be done with sql and never used pandas again.\nehh, true and not true. transformations in pandas are 10x easier than raw SQL, specially if you want to apply user defined functions. its a debatable topic, i respect your opinion. but i can attest getting my hands on numpy and pandas has made my life so much easier\nId disagree tbh, but its a debate for sure. Im not saying Im correct, its just an opinion. One downside is for pandas theres a limit in data size, at some point youre going to end up using spark instead, whereas for sql, theres not really that same limit. Ive processed terabytes. It depends on the backend though. In most companies as well theres already a large amount of analysts familiar with sql. It does depend on your use case though, but Ive seen people struggling with pandas and had to work on a subset to get by. Ironically when they finished, someone converted their code to sql to process all the data. Another DE to Spark.\nIf i may ask, what of sql database you are using? I'm learning sql server, loving it, but people are saying postgres and oracle are the future.\nI work as a data engineer(just 3 yoe) in a consulting firm and some of our clients want us to clean their data, do little bit of manipulation and dump it into a database from storage like S3. In all those cases, Ive been using pandas as the data is not really big and not spark worthy.\nIf its not big generally Id prefer pandas (or R) as well myself for quicker analysis. Id always keep in mind some conversion if this was just a subset of data though.\nWould this still hold in an enterprise environment where big data and scalability are assumed, and your platform is architected to match (e.g. Databricks)? Then just use Spark dataframe as much as possible?\n> Would this still hold in an enterprise environment where big data and scalability are assumed, and your platform is architected to match (e.g. Databricks)? Then just use Spark dataframe as much as possible? You're essentially asking \"if your company already uses Databricks, and the data needs scalability, shouldn't you just use Spark?\", and the answer is \"duh\". But that's not the scenario the person above you was describing, so you're distracting from their point.\nI recently used to it optimize a database I was working on. I work with aggregated data sets or 1000+ fields for a single ID. Pandas came in handy when analyzing columns to see if there were any coulombs which had identical data. Often in these aggregates that will happen as they come from disparate sources. Pandas was super helpful for that.\nI'm going to be that guy and ask, have you tried polars?\nNo I havent but looks neat\nTry it out, you'll like it\nI use it sometimes if I need to do a quick join with an excel sheet not in a db. Also I like it for pivot tables.\nBut pyspark could very well be necessary.\nWhat solution do you suggest if not pandas? Are all data frames unnecessary or is it pandas specifically? If I pull data from a db how would I house it in python? I hear this pretty often nowadays but for some reason I cant find an example of an ETL that doesnt utilize some kind of data frame, especially when the source is a db table\nI am just giving my opinion that it is not at the same level as the others. Pandas, pyspark, polars, great for ETL. But you could go ELT and use dbt exclusively for all your transformation needs. These are all great tools\nYea for sure, I actually agree with you on pandas being unnecessary. Im trying to explore other options before pyspark and was wondering what alternative you had in mind for ELT. Is the alternative still using a data frame? Or do you somehow do it in native python?\nIf your data is in a database or a delta lake situation, then I would say dbt is unmatched in its features. Your data is processed inside the db it is currently in, projects are well organized, and there is massive community support. Once you're set up, you can collaborate with non-coders / SQL yaml people. Its integration with tests and automated documentation are also top of the line. Only way I would avoid it is if my project was so tiny that setting up dbt is overkill. In that case, dataframes would be the next step down. Native Python is best if you don't need transformations, but the second you have to join or do any aggregations, you'll need at least a dataframe.\nI guess starting as a Data Analyst did get me half way there then...\nYes especially advanced sql queries etc\nTbh I hardly even use SQL. I just run everything on pandas. The dataset I handle are all tiny. Edit: Going forward I'll probably force PySpark and SQL down some of my scripts just for practice lol\nTry using duckdb API instead of pandas\nAthena isn't a warehouse. It's a managed service that let's one query data stored on s3.\nAt least since Apache Hive, it has been common to call SQL query engine data warehousing tools, the file management layer is generally implied.\nAs a data engineering director, I heart this post!\nwhat DAG is good choice, i know nothing about DAG, but i seen apache airflow is one of the famoust, looks a little old imo, I seen prefect so maybe it works better?\nJust a quick terminology thing, since this is a post aimed for beginners. DAG stands for Directed Acyclic Graph. It is technically a data structure. Its a type of graph that edges within it are directed meaning that each node has a from and a to basically. It is also acyclic meaning the graph can have no loops. In less formal terms, I believe what you were asking is what is the best DAG orchestrator? Since DAGs are formally more used in declaring workflows, I believe this was the ask. Apache Airflow, Prefect and Dagster are the most common, if you are in AWS-land, then Step Functions (the service not the design pattern) are another popular workflow orchestration service.\nAirflow remains the most widespread but it is showing its age. Dagster is the most modern and ambitious (compared to Prefect) one that I recommend for new projects.\nWhat do you normally use bash for\nAdministrating the servers you use for your infra, managing files, light or occasional data processing. Bash has surprisingly performant data processing tools that require no installation and multi-processing is trivial. It can be a better choice than Python for simple logic. If you app is more complex, it's better to get into a more solid project with multiple files, logging, testing, external libraries etc. that require a proper multi-purpose language like Python.\nIf you're starting from scratch go for polars instead of pandas. Learn about database indices and their importance. Maybe partitioned tables too.\nSo basically basic software engineering skills\nYeah seriously. I know how to do all of those things and never considered it \"data engineering\".\nTransferable skills can be used in many domains for any number of purposes.\nDE sometimes forget the engineering part of the job.\nNever got a real affinity with pandas, really mostly because the syntax seems so inconsistent compared to using plain SQL for grouping and windowing, or even R with tidyverse or data.tables.\nDone till 9, halfway through 10, working on a project that used 11 Need to review a lot of stuff though, especially 6\nYou got a good example or say more about 11? Would that be something like GCP cloudrun being kicked off by your dag code for a python based ETL job? Curious how you normally do it.\nPandas used to be the gold standard. Now Polars, spark, dash are viable alternatives depending on infrastructure and data size.\nWhat about data bricks ?\nFor #12, does anyone have any good examples of projects to do for this / putting it altogether?\nthe worst part about these lists is you'll go years not using 90% of it, but only need it for either interviews or randomly, in which case you coukd just look it up and revise. Instead we're encouraged to keep a static mess in our heads just on the instance we need to be asked about it, not to perform it, but to tell people about it.\nI've got 1 through 4 and also 7 in the bag. Working on the rest. Thanks for the list!\nThanks for this. Im currently a 1-year QA Engineer who performs testing on hardware and software but Im looking to make the switch to Data Engineering as I find it more interesting & I dont want to fall under the label of Just a QA guy that I hear often among engineers. I have a CompE degree from college so other than 1 stats class and a few CompSci courses (C/C++), I was completely lost but this has eased my mind a little bit. If anyone has any tips of words of advice for me, Id love to hear it! Thanks again so much.\nSQL is the foundation! Doing data analyst work is the gateway into a lot of DE skills\nYour list has no distributed system and pySpark. No DE without that\nThis is a weird sentiment, no DE without your specific favorite composition of tools and design pattern? Most businesses can have their distributed data processing needs met with a solution like BigQuery that comes with infinitely less complexity and overhead vs a custom distributed spark setup\nThe point was more towards distributed system. Dont call yourself a DE if you dont know distributed system at all. Call yourself SE with knowledge of sql and programming language. Why spark? Cox you get to know actual distributed processing going on or atleast you can observe everything and learn. Now your point, bigQuery, i saw people failing with that with just using sql and no sense of DS and ending in chaos (issues with performance, cost, pipeline design, ingestion strategy, no query plan understanding etc) So one should focus on spark type system first to learn the concepts and then move on to whatever other system out there. And let me tell you, not everyone in their career stick to bigQuery only \nYou want to memorize sql keywords? You already lost me there, probably the most stupid thing ive ever seen on a de sub\nI don't think it means all of them but the basics like ``` select columns from table join other on ... where ... ```\nI assume its just a poorly worded way of learning sql. Or learning the sql syntax and what, does what. I mean sure we can particular about what is said, but I think its clear enough to mean that and learning sql is probably not a stupid thing to say on a DE sub, unless youre being particularly unforgiving?",
        "content_hash": "ae2a9818142fe4643e53f7861f34b95f"
    },
    {
        "id": "1gn7d8b",
        "title": "How to Benefit from Lean Data Quality?",
        "description": "",
        "score": 442,
        "upvotes": 442,
        "downvotes": 0,
        "tag": "Blog",
        "num_comments": 28,
        "permalink": "/r/dataengineering/comments/1gn7d8b/how_to_benefit_from_lean_data_quality/",
        "created": 1731148556.0,
        "comments": "I don't understand this post. I'm a huge advocate for ELT over ETL and your criticism of ELT is much more applicable to ETL. Because in ETL transformation steps take place inside ingestion steps, documentation is usually barely existent. I've refactored multiple ETL pipelines into ELT in my career already and it's always the same. Dredging through ingest scripts and trying to figure out on my own why certain transformations take place. Not to mention, the beauty of ELT is that there is less documentation needed. Ingest is just ingest. You document the sources and the data you're taking. You don't have to document anything else, because you're just taking the data as is. Then you document your transform steps, which as I've already mentioned, often gets omitted in ETL because it's part of the ingest. As for data quality, I don't see why data quality would be less for an ELT pipeline. It's still the same data. Not to mention, you can actually control your data quality much better than with an ETL. All your raw data is in your DWH unchanged from the source, any quality issues can usually be isolated quite quickly. In an ETL pipeline, good luck finding out where the problem exists. Is it in the data itself? Some random transformation done in the ingest step? Or during the business logic in the DWH?\nInteresting, because with storage price becoming cheaper, it's usually a good idea to land the source data as-is, and do profiling/discovery after to create your pipeline/modelling accordingly.\nMaterializing a copy of your data can be done equally easily using either ETL or ELT, so is not really a differentiator. However, at sufficiently massive volumes it can still be a challenge to store & retain a copy of the source data.\nThere are tools to avoid full materialization though. You can do logical materialization then parse out what you need.\nWhat do you mean by full vs logical materialization? A view vs persisting data to a table?\nYes, I know. You're describing ELT, i.e. agreeing with me. Unless you meant to agree with me, in which case your wording is confusing as it appears to disagree.\nI agreed with you. What I meant was that your insights are interesting.\nSorry, I misinterpreted your post then.\nI agree 100%\nPreach\nThat's not been my experience - unless we're talking about projects that people just hacked together. Regarding data quality: * Readability: With ETL you may use a general-purpose programming language like Python. Rather than have 600 lines of clumsy field transforms all mashed together you can separate each to separate functions. * Unit testing: Languages like python & ruby enable easy unit testing. While you can hypothetically perform unit testing with SQL the reality is that the setup is too much work. Almost nobody does it, and when they do they do so sparingly. * Raw data: super-valuable to keep around, and easy to do for both ELT & ETL. For ETL it's more likely to be kept in files and queried using something like Trino or Athena. Which works perfectly fine. Regarding Documentation: * For ETL solutions using canned packages this usually comes with the solution. * For custom ETL solutions it may not, and may require the engineers to build it. However, there are a few short-cuts for them: what I like to do is simply generate a markdown or HTML page from all my transform functions and publish this. It includes the docstring, names & types of input & output columns, describes exceptions and formats, and also has a link to the unit-test source code so my users can click on that to see what the tests look like. While my users are usually not programmers they have found this to be incredibly valuable - because they can actually read the unit tests. * For ELT much of the documentation focuses on data linage - since data pipelines can sometimes be as much as 30 model layes deep in DBT (yeah, I've seen this), and it otherwise becomes a nightmare to know where any data came from. But the documentation is often very soft on exactly what transforms are happening to a field - since the transforms are just mashed-together SQL rather than separate UDFs or functions. Imagine nested case statements, substrings & strpos along with some verbose built-ins. That crap is prone to human-error, difficult to read, and seldom well-documented.\nYeah - we land our data first. Most of it pipes into GBQ from there. Then we use dataform for our workflows. Dataform has assertions so we can choose to fail for data quality issues. It also has unit testing. Having all of our domain logic in SQL/JavaScript has been truly game changing for our team. Like others have said - most issues are upstream. Engineering teams should find them, then elevate them to the business, data scientists, or whoever owns the data for remediation. Even with streaming data, the tooling has grown. You can centralize transformation logic with ETL, but then you have logic at your access layer and your input layer. Hard to manage unless you merge them. We hardly even document our data anymore. It's documented in the pipelines via configuration, and that documentation is in an artifactory so it's uniform. From there I just enable a few APIs and boom we have lineage, and everything feeds through to a doc system. There's tooling beyond that which can capture your transforms for you. It's almost 2025. This is all really easy.\nI agree, ELT is perfect and reject management is easy. ETL is so pre-2010. I guess the meme maker made a typo.\nRemember SSIS? I don't want to.\nI still deal with it. Using it for strictly telecom. Every else a sql job with sprocs. Convert to Python when possible.\nIt was such a nightmare. I will say, when choosing tech, I avoid shiny objects. I need to hire engineers that can work with a tech, and I don't want to change EVERY tool I use every two years. Python and SQL are great. I hate no-code pipelines.\nYeah I found this picture confusing. ELT is way better than ETL for documentation and data quality. Just on its face, you typically have the code documenting how to get data into the transformed state whereas you are likely to not have this if your ETL process relies on stored procedures and triggers. Edit: downvotes are perplexing here. Im agreeing with the person Im replying to. Are you all really suggesting ETL is better than ELT for data quality???\nWho on earth upvotes junk like this?\netl or elt...is a trade off, none is a silver bullet. Also documentation issues will appear in both\nNever heard of LDQ. I need to look it up\nI don't understand hardly any of these posts. From my POV, \"ETL\" is just the name for the placeholder for data ingestion. Whether you do ETL or ELT depends on individual data feeds and the two are not mutually exclusive. One type of data may be better with ETL while another is better using ELT. It isn't a data ecosystem decision, but more of a feed by feed decision. Most everyone here talks about documentation from a technical standpoint. That is the easiest part of documentation. Linking the business metadata up with the technical metadata is the goal. Consider how you look for data in your warehouse. It isn't \"I'm looking for an int\" but \"I'm looking for net sales\". This is just one piece of the data documentation.\nwe solved tabs v spaces so people need another utterly worthless debate to hang on to\nIn my experience ETL vs ELT...the quality issues actually occur more predominantly prior to the E. So depending on your situation if you're a small nimble startup/nimble team or a huge enterprise with a lot of disparate sources and some being external partners ...changing sources with little coordination/documentation..TL or LT...a pipes going to break somewhere. ETL is schema on load. There is a designed model that is being loaded to ELT is schema on read. You figure out what you want when you consume. ELT has a chosen trade off for agility and speed...but harder to govern depending on the rate of changes and how tight or loose your quality checks are between your producers and consumers. People, Process and Tools. This is addressed more so by Process and People and less so much by Tools (Unless said tool is Data quality/Data Governance focused)\nPeople who dont use ETL and ELT as synonyms are silly.\nOr as a typo /s",
        "content_hash": "b231cd8aa47d9c5a4a2bdcf2a087bf47"
    },
    {
        "id": "tuobs4",
        "title": "Completed my first Data Engineering project with Kafka, Spark, GCP, Airflow, dbt, Terraform, Docker and more!",
        "description": "[Dashboard](https://github.com/ankurchavda/streamify/blob/main/images/dashboard.png?raw=true)\n\nFirst of all, I'd like to start with thanking the instructors at the [DataTalks.Club](https://DataTalks.Club) for setting up a completely free [course](https://github.com/DataTalksClub/data-engineering-zoomcamp#data-engineering-zoomcamp). This was the best course that I took and the project I did was all because of what I learnt there :D.\n\nTL;DR below.\n\n# Git Repo:\n\n[Streamify](https://github.com/ankurchavda/streamify)\n\n# About The Project:\n\nThe project streams events generated from a fake music streaming service (like Spotify) and creates a data pipeline that consumes real-time data. The data coming in would is similar to an event of a user listening to a song, navigating on the website, authenticating. The data is then processed in real-time and stored to the data lake periodically (every two minutes). The hourly batch job then consumes this data, applies transformations, and creates the desired tables for our dashboard to generate analytics. We try to analyze metrics like popular songs, active users, user demographics etc.\n\n# The Dataset:\n\n[Eventsim](https://github.com/Interana/eventsim) is a program that generates event data to replicate page requests for a fake music web site. The results look like real use data, but are totally fake. The docker image is borrowed from [viirya's fork](https://github.com/viirya/eventsim) of it, as the original project has gone without maintenance for a few years now.\n\nEventsim uses song data from [Million Songs Dataset](http://millionsongdataset.com/) to generate events. I have used a [subset](http://millionsongdataset.com/pages/getting-dataset/#subset) of 10000 songs.\n\n# Tools & Technologies\n\n* Cloud - [**Google Cloud Platform**](https://cloud.google.com/)\n* Infrastructure as Code software - [**Terraform**](https://www.terraform.io/)\n* Containerization - [**Docker**](https://www.docker.com/), [**Docker Compose**](https://docs.docker.com/compose/)\n* Stream Processing - [**Kafka**](https://kafka.apache.org/), [**Spark Streaming**](https://spark.apache.org/docs/latest/streaming-programming-guide.html)\n* Orchestration - [**Airflow**](https://airflow.apache.org/)\n* Transformation - [**dbt**](https://www.getdbt.com/)\n* Data Lake - [**Google Cloud Storage**](https://cloud.google.com/storage)\n* Data Warehouse - [**BigQuery**](https://cloud.google.com/bigquery)\n* Data Visualization - [**Data Studio**](https://datastudio.google.com/overview)\n* Language - [**Python**](https://www.python.org/)\n\n# Architecture\n\n[Streamify Architecture](https://preview.redd.it/lnpruyl305r81.jpg?width=1222&format=pjpg&auto=webp&s=48f5993d4a8e142b7c32846535ad1ae501e2a332)\n\n# Final Dashboard\n\n[Streamify Dashboard](https://preview.redd.it/0r3yfde005r81.png?width=1436&format=png&auto=webp&s=4d21c6f47ebbfe6f9201afdd4284ca83643e4e3b)\n\nYou can check the actual dashboard [here](https://datastudio.google.com/s/uVy77npmwvg). I stopped it a couple of days back so the data might not be recent.\n\n# Feedback:\n\nThere are lot of experienced folks here and I would love to hear some constructive criticism on what things could be done in a better way. Please share your comments.\n\n# Reproduce:\n\nI have tried to document the project thoroughly, and be really elaborate about the setup process. If you chose to learn from this project and face any issues, feel free to drop me a message.\n\n&#x200B;\n\n**TL;DR:** Built a project that consumes real-time data and then ran hourly batch jobs to transform the data into a dimensional model for the data to be consumed by the dashboard.",
        "score": 436,
        "upvotes": 436,
        "downvotes": 0,
        "tag": "Personal Project Showcase",
        "num_comments": 89,
        "permalink": "/r/dataengineering/comments/tuobs4/completed_my_first_data_engineering_project_with/",
        "created": 1648920638.0,
        "comments": "This looks really great, I would be starting this soon. Thanks for posting this. one question: why are we using both spark and dbt, when we can apply transformations using spark itself? or am I missing anything?\nThat's a valid question. I am using spark to consume the real time data with structured streaming. For batch I just went with dbt since I have some experience with Spark batch jobs so I found it better from a learning perspective to try dbt.\nGreat, that's a good way to learn by practicing. u/mamimapr suggestion was really a good one to consider. I'll say you can make that change. The reason I am saying this is, for example, if you put this in your resume and when they see the Architecture they will be confused and raises a lot of questions. edit: I could think of one more scenario where your architecture also makes sense, as we are using spark to push all the event data into the data lake and apply transformations using dbt/spark for the data required(business use case) and store them in big query for reporting correct me if I miss considering anything\nI agree with what you said. Data pipelines should be as simple as possible. I guess I approached the project more from a learning experience, rather than a practical one. But surely, thing's can be simplified by writing directly to Bigquery, I will explore that option. Thanks for sharing your thoughts. Edit: Yes, I don't think I can totally remove the dbt part. I need batch jobs for creating the facts and dimensions. I can though remove the writing to GCS part and create the staging Bigquery tables directly.\nReading through this cause this course is the best one Ive seen to date (IMO). Who cares if dbt is necessary for functioning as the pipeline, its more about visibility and modularity. Love that you used it and for the record I plan to take this course myself, even though I think I know everything but terraform. Great work and love this whole project\ndbt beats spark with it ubiquity and raw speed to onboard. Where you can download it and instantly be productive. Spark requires a team of NASA engineers & three Phd's - and once they're all done fighting with each other they may or may not eek out what dbt can in 11 minutes. I personally can't wait for Spark to die the natural death it deserves.\nLol found the analytics engineer\nYes, spark structured streaming could directly write to bigquery. Dont know why to add gcs and dbt and airflow to complicate everything.\nHey that's a good point. I didn't know that it could be done. I will surely check how to write to Bigquery directly. Thanks for that. Also I added dbt primarily for creating facts and dimensions. I could not find a way to do it real time without complicating things. Edit: added sentence\nhttps://github.com/GoogleCloudDataproc/spark-bigquery-connector\nEasiest way real time would be using databricks instead with autoloader picking up your stream files and delta live tables doing the transforms. Would be a fun task learning databricks to see the difference in setup.\nInteresting. Will check this out. Thanks for sharing.\nI have found myself preferring to write to a data lake in Google cloud storage vs straight to BigQuery. BigQuery external tables let me query the lake and take a schema on read viewpoint. I use dbt to define the external table schemas and of course for all the transformation work.\nSuperb, will try to implement this project in a few days,so expect some(maybe a lot:) doubts in your dm.\nFeel free to reach out :)\nCongratulations, coincidentally I just started the course today; any tips?\nHey, just take the first week to understand the course structure, understand your pace and the topics covered. It is surely not a small course. Keep at it and check the FAQs, there are a lot of answers already there. Search in slack, chances are you'll find answer to your error in a thread. Rest all you'll figure out as you go. Happy learning :D Edit: sentence\nHow long did it take you to go through the whole course?\nIt is roughly 5-10 hours of work per week depending on how much you already know. So that'll be 8ish weeks including the project.\nawesome project but this might be a stupid question : What is Airflow adding to this? dbt supplies it's own Orchestration right?\nYes, but that's dbt cloud I guess. I used dbt core. With dbt's scheduler, you can only orchestrate dbt parts, but for additional steps, Airflow comes through.\noh right, sorry i've just used cloud! Thanks for pointing it out :)\nThis is awesome I will start this! Unfortunately Ive been having trouble getting dbt installed on my system I think I have a python issue.\nIf you can, I'd suggest to get the 300$ free credit on GCP and work there. All my setup is on cloud. You'll face far lesser issues and get some cloud hands on as well. Make sure to make the most of those credits.\nCongrats on starting a personal project and actually having a nice end result! Not many reach this point, lol. Document this project well to be able to impress recruiters, and you should get great opportunities!\nThank youuu! That's reassuring. I have tried to document generously. I will keep on adding as and when I receive feedback\nI've documented on [Git](https://github.com/ankurchavda/streamify) itself. It's slightly more focused on the setup part. But you can still get an idea on the data flow.\nIt seems that your comment contains 1 or more links that are hard to tap for mobile users. I will extend those so they're easier for our sausage fingers to click! [Here is link number 1 - Previous text \"Git\"](https://github.com/ankurchavda/streamify) ---- ^Please ^PM ^[\\/u\\/eganwall](http://reddit.com/user/eganwall) ^with ^issues ^or ^feedback! ^| ^[Code](https://github.com/eganwall/FatFingerHelperBot) ^| ^[Delete](https://reddit.com/message/compose/?to=FatFingerHelperBot&subject=delete&message=delete%20i36ra64)\nNice one! Im doing the same bootcamp and your project is so much better than mine!\nHey, the project coincided with my job hunt, so I put a little more effort for this. But there's nothing in there that you can't do :)\nThanks. I'm also looking for a role, although not feeling very confident about it. I did update my project a bit though. I borrowed an idea from yours where I split the different sections into different READMEs! https://github.com/ABZ-Aaron/Reddit-API-Pipeline\nLooks amazing . I should pick this up\nLet me know if you do and face any issues :)\nThat \"Franco\" is in the top 5 artists drives me crazy context: Spanish dictator\nPeople here at Streamify love him, I tell ya!\nHow long did it take to finish ?\nIt took somewhere around 100 hours give or take. The setup part was the bigger unknown which took the most time.\nSorry novice question. Is there a reason to use two streaming services - both Apache and Kafka? Do they each provide functionality the other doesn't?\nSo the easier answer is that Eventsim only writes to Kafka for real time data. There was no option to read from spark streaming directly. Also, I am fairly new to streaming as well, so I might not be able to answer very convincingly on how Kafka's capabilities differ from Spark Streaming and are they supposed to be working together, or as replacements.\nWhat tool did you use to make the dashboard?\nIt is Data Studio by Google\nAbsolutely amazing. Thank you for this! I was looking for something like this.\nHappy learning :D\nThis looks like an awesome course! Im gonna start it this week. Thanks for sharing, congrats on your project, and good luck getting a DE job (assuming youre looking for one hehe)\nHey thanks. And you'll definitely enjoy the course.\nI just signed up - I assume you started this back in January? Is it the type of course you can start at anytime or do they need to kick off a new cohort? Most lectures are recorded so I assumed it starts anytime?\nVery cool! You mention that the course is free. Are all the tools/libraries you used for this project free as well (e.g. Google Cloud Platform)?\nYes, everything is free. You avail 300$ in credit on GCP by creating a new account.\nBasic question - My understanding of Spark is that it's a data layer and used to analyze data, *not* to store data. So in the diagram, is data being moved from Kafka and stored in Spark? Then transferred to Google Cloud Storage? Is the data in Spark being stored in RDDs and transferred from there to Google Cloud Storage? Thanks\nSo spark is used to consume the data from the stream in the first place. Then I do some processing on the data (minor cleaning etc.) and store the data to GCS. Spark is acting like a stream processing layer and not a data store. And yes the processing happens using dataframes (rdds under the hood). If that helps.\nTotally yeah makes sense. Thanks for elaborating.\nHow did you sign up for this course?\nFound their post on here when it was starting out. Now you can the take course at your pace since it has ended.\nHow did you get the project idea\nI knew about Eventsim, and I wanted to do a project with real time data.\nThats really cool project to learn new technologies I am going to replicate this for my learning too.\nGlad that you think so! Let me know if you face any issues :)\nDo you recommend the course to a beginner? I'm only comfortable with Python and SQL and have some rough idea of what these tools are, but can't operate them yet.\nI guess Python and SQL are a good foundation for you to get started. You'd have to do some side reading though as you progress. I did that as well.\nAre the resources mentioned by the course instructors (for the side read) or do we seek them out ourselves?\nI'd say you choose a couple of things you want to really learn and deep dive into that. Rest you can learn just enough to get things done. I paid more attention to the Kafka and Docker parts since I was completely new to it. If you try to learn everything that's taught in there, you'll get overwhelmed.\nGot it, so I'll probably learn those concepts separately and then have a crack at this.\nThis is amazing! Im just wondering.. whats the cost of having this running? (Since its on Google cloud). Im always scared of doing personal work on private clouds and use my credit card\nYou get 300$ in credit when you create a new account for three months. So you should he good. Also, I had the same fear as you. But turns out 300$ is a considerable amount, and it is not as easy to exhaust. I still have half the credits left.\nThats great, Ill definitely will check it out! I think this is an amazing work\nDamn, this is frigging awesome. I'm gonna go through this bit by bit and try to learn as much as I can, because this is right up my alley in terms of the kind of stuff I need to learn more about. Thanks for sharing, seriously.\nI am glad this will help you in your journey. If you face any issues, feel free to reach out :)\nJust a slight critique, but I noticed some of the dbt models are a bit hard to read. Especially your dim_users SCD2 model, which uses lots of nested subqueries and multiple columns on the same line. You may want to refer to this [style guide](https://github.com/dbt-labs/corp/blob/main/dbt_style_guide.md) from dbt Labs. I find CTEs are a lot easier to parse and read. But again, not really a big deal as far as functionality goes. Probably something I'd address on a second iteration.\nThanks for sharing the style guide. I agree with you, the query readability can certainly be improved. I will look into it.\nHaven't had time to review this in depth yet but just wanted to say great work! The DE community will be better for getting exposure to projects like this and for you it will be a great portfolio piece.\nHey thank you. I did not expect such a positive response. I am glad that this'll help atleast a couple of people if not more :)\nThis is awesome, thanks for sharing! I have a little experience with python, and a bit more with Tereaform and GCP, but zero experience with any of these other tools. Do you think this is an approachable course for a novice, or...?\nYou should be good. You can also take your own time to learn and progress. I'd recommend some side reading, especially for Spark and Kafka.\nSuch a nice, end to end project, congrats! Well documented and organised. I'm also working on a similar project and yours is really something to look up to :)\nHey glad you think this is good. Do share you project as well when done :)\nGreat work! One suggestion though: Why not try a lakehouse architecture with a delta lake or Iceberg?\nHi u/ankurchavda I'm developing a data engineering project as well - I was wondering where you were able to draw out the architecture diagram you did because I think you did a good job for that. Thanks!\nOr sorry I saw you're using Miro but if there is a specific template you used please let me know or if you can share me your miro board that would be great too!\nHey I completely missed this. Yes, I used Miro. No specific templates though :)\nThis course looks like just what I need! Trying to get into Data Engineering from GIS.\nDid you start this course in the end?\nI did, and then realized Data Engineering salaries in Canada are pretty similar to GIS salaries. I still want to finish the course.\nCool, just wanted to double check you can start and finish this course anytime And yes always a good idea to finish it, I've seen plenty of data based roles (either DE, DS or DA) that also ask for GIS so more options is never a bad thing!\nYeah pretty sure theres no timeline on it, and youre right it is always good to increase your skill set! The little Ive learned so far has really improved my work at my current job.",
        "content_hash": "b334ec3eb97d8615c18a472094e82eb5"
    },
    {
        "id": "1glu70w",
        "title": "Top Skills for Data Engineers - Data from 100 Fortune 500 Job Descriptions",
        "description": "I analyzed 100 data engineering job descriptions from Fortune 500 companies to find the most frequently mentioned skills. Here are the top skills in demand:\n\n|Skill Group|Frequency|Constituents with Frequency|\n|:-|:-|:-|\n|Programming Languages|196|SQL (85), Python (76), Scala (21), Java (14)|\n|ETL and Data Pipeline|136|ETL (65), Pipeline (46), Integration (25)|\n|Cloud Platforms|85|AWS (45), Azure (26), GCP (14)|\n|Data Modeling and Warehousing|83|Data Modeling (40), Warehousing (22), Architecture (21)|\n|Big Data Tools|67|Spark (40), Big Data Tools (19), Hadoop (8)|\n|DevOps, Version Control and CI/CD|52|Git (14), CI/CD (13), Jenkins (7), Version Control (7), Terraform (6)|\n|Data Quality and Governance|42|Data Quality (20), Data Governance (13), Data Validation (9)|\n|Data Visualization|23|Data Visualization (11), Tableau (6), Power BI (6)|\n|Collaboration and Communication|18|Communication (10), Collaboration (8)|\n|API and Microservices|11|API (8), Microservices (3)|\n|Machine Learning|10|Machine Learning (7), MLOps (2), AI/ML Model Development (1)|\n\n\u27a1\ufe0f Excel Sheet with data -  [https://docs.google.com/spreadsheets/d/1zB6wocrgxNgjWwo6Jkezje0SgJ3PXMIoCEyJwdY-nLU/edit?usp=sharing](https://docs.google.com/spreadsheets/d/1zB6wocrgxNgjWwo6Jkezje0SgJ3PXMIoCEyJwdY-nLU/edit?usp=sharing)\n\n\u27a1\ufe0f Checkout the full video with explanation of tasks (for Beginners) - \"What Do Data Engineers ACTUALLY Do? Tasks & Responsibilities Explained!\" - [https://youtu.be/XzqYdCov-LA](https://youtu.be/XzqYdCov-LA)",
        "score": 426,
        "upvotes": 426,
        "downvotes": 0,
        "tag": "Blog",
        "num_comments": 49,
        "permalink": "/r/dataengineering/comments/1glu70w/top_skills_for_data_engineers_data_from_100/",
        "created": 1730996195.0,
        "comments": "Ok. This is a lot better than the last one lol Good job.\nThanks lol. Hope to add some value - your feedback was correct - last list was too high level.\nyou can do something similar to market basket analysis to find out which skills are requested in combination\nThis is doable will take some work, can try over the weekend\nI'm trying to put something together to help with this, but man, these job postings love to conflate skills which are widely separated. Modelling data is not the same as managing a data lake / cluster etc.\nThis is a really good idea!\nMan I really need to branch out from just being a SQL expert. Finding the motivation has been tough though. This sub makes me feel bad for not having the drive to learn on my own time lol.\nJust learn some python, then learn business and you'll be fine\nInstructions unclear, I now own a pet store specialized in snakes\n\nAWS is actually very interesting and I think its high time you should diversify your skills for sure!\nAny strong clusters?\nCould you clarify what you mean by clusters? Groups of technologies being used together?\nYes exactly. Having a list is not that helpful because I will probably not use those techs in random combinations. But if you can cluster the skills into usual job profiles (or the jobs by skills) then you can give us insights into what \"collection\" of skills to study to have a good chance to get a role.\nI love how low communication and collaboration are.\n\ngreat job\nNo R.\nNot surprising IMO.\n3 mentions\nThis is something awesome. This activity actually helps in identifying the key skills and hacking through the interview.\nIn my completely unscientific vibes test, Hadoop should be way higher than that. Not because it's a useful skill, it's not... but I feel like I see an unusually high number of positions that ask for experience in it. Did any F500 companies ever have Hadoop clusters? It was pretty niche back in the early 2010's back before companies wanted to be \"dAtA dRiVen\". By the time F500 companies got data science fever, Hadoop was already obsolete. I just think its weird that so many postings ask for an obsolete skill that the company has never once needed at any point in history.\nHadoop is pretty much dead at this point. Buried next to SOAP and XML\nI agree with you 100%, this is solely based on job posting on LinkedIn. Could be based on disconnect between HR and the teams. Or maybe they are posting these roles under titles different from data engineer.\nCan you explain why you think Hadoop isnt necessary? What scale does a company need to be at for it to make sense?\nCloud computing and general advancements in hardware made Hadoop obsolete. You don't need to have a giant cluster of physical computers to work with big data anymore. You can rent and pay as you go with a cloud provider. It's also somewhat debatable if anyone actually NEEDED Hadoop in the first place. Look at the average companies Databricks instance. 90% of them could probably run on an on-prem Postgres or MSSQL instance.\nFrom job descriptions that are likely bullshit post that stay for weeks ( or reposted) in this market and they cant seams to fill them in. You cant trust this shit anymore.\nOnly 100?\nI got to around 350 companies to get these 100 jobs\nReally interesting point\n[removed]\nThese are very low frequency I have no idea why these are mentioned though\nI have even seen front end technologies mentioned in JDs of Data Engineer multiple times in my country.Not really a DE position,possibly due to this handled by HRs.\nCan we get a bot to automatically create a resume with the most popular skills? Where is the data from?\nThis is all data from LinkedIn, Ive mentioned the excel at the end.\nChatGPT\nHow did you extract key skills from job description? Genuine question as i am working on something similar\nI found out the top occurring key words and then created a list of keywords to look for. Not scalable of course but did the job for me.\nHow is this possible ? I do all of this, and i dont even work for a Fortune 500. Phhh .. amazing\nCan you tell me more? Do you work at a small company?\nThanks\nThanks\nBy this measure Id say GCPs gaining ground on the other hyper scalers.\nBigQuery ftw\nCan you divide this by experience level if possible?\nData engineering skills are so varied now like being a jack of all trades in data\nHonestly every role doesnt need you to know everything. But when you are preparing you have to learn everything and its good to build that foundation. Also once you join a company you will be maybe using 3/4 of these maximum.\nCool! Anyone care to do the same for Europe? I bet Azure would be higher than AWS and GCP would me virtually non existent",
        "content_hash": "ee006325187d391b282553ed763b2ca2"
    },
    {
        "id": "1ewpcss",
        "title": "Passed Databricks Data Engineer Associate Exam with 100% score!",
        "description": "Hello guys, just passed the DB DE Associate Exam. Here is how I prepared:\n\n* I first went over the [Data Engineering with Databricks](https://customer-academy.databricks.com/learn/course/1266/data-engineering-with-databricks?generated_by=257839&hash=6d91d1a67d6582652bdd111c171d9940a4d22e96) course on Databricks Academy. I took my time to go over all the Labs notebooks.\n* Then I went over [Databricks's practise exam](https://files.training.databricks.com/assessments/practice-exams/PracticeExam-DataEngineerAssociate.pdf). If you have followed the course well, you should be getting a score > 35/45\n* I then watched [sthithapragna](https://www.youtube.com/@sthithapragnakk)'s latest Exam Practice video. As of today, Latest version is from July 20th 2024. Here is link: [https://www.youtube.com/watch?v=IBONv\\_gdKNc](https://www.youtube.com/watch?v=IBONv_gdKNc)\n* Finally, I have bought a Udemy Practice exams course. You will find many, but I picked one that was udpated recently (June 2024), here is the [link for the course](https://www.udemy.com/share/10aEFa3@LEBqmD3Y8tv9TWEFNGVLr88aLmbF3l_OL__t1qX96WJqsfPGueDGYl8g8Qt_7tyB/).\n* **Note**: if you just do the first 3 steps, it's enough to pass the exam. Udemy course is optional, but since it's price is marginal compared to Databricks Exam price (<= 10%), I bought it anyways.\n\nhttps://preview.redd.it/ozs4kps5srjd1.png?width=1118&format=png&auto=webp&s=cdaf24041cfe0812ea649265124fdd1fc95b1369",
        "score": 423,
        "upvotes": 423,
        "downvotes": 0,
        "tag": "Career",
        "num_comments": 82,
        "permalink": "/r/dataengineering/comments/1ewpcss/passed_databricks_data_engineer_associate_exam/",
        "created": 1724138285.0,
        "comments": "Congrats!\nThank you!\nYou didn't use dumps?\nWhat do you mean by dumps\nDump of exam, you did not look at online question of exam, right?\nYour name is showing in the picture in case you wanted anonymity. Otherwise thanks for the tips and congrats\nyeah that was my firs thought!! :P\nHow much time would it take to prepare with these resources?\nOverall it took me around 25 hours of study, spread across 1 week * Completing [Data Engineering with Databricks](https://customer-academy.databricks.com/learn/course/1266/data-engineering-with-databricks?generated_by=257839&hash=6d91d1a67d6582652bdd111c171d9940a4d22e96) course with Labs takes around 15-20 hours. * Doing the practise exams and reviwing your answers takes around 1.5-2 hours for each. So around 7 h in total in you follow all the practise exams reccomended\nJust to confirm, you bought the paid course worth 1500 dollars?\nNo I didn't, I just followed the self paced course available for free on Databricks Academy. Just to recap again, to prepare for the exam, I paid in total 20$ for the udemy course and that's it.\nGot it. Thank you\ndo we have to be a databricks customer to access this ?\n>!didn't you have to pay either 100$/200$ for exam fees then??!<\nwas any knowledge of Apache Spark needed to pass it all?\nNot really, there were only SQL/PySpark questions that are related directly to keywrods used in syntax of Databricks Live tables and Delta Lake. just make sure you understand the code used in Databricks Academy notebooks.\nCongratulations. May I know your current profile?\nSure, I'm currently CTO for a small AI startup. I have a passion for data, software and AI\nThat's great . Being a boss yourself you are pursuing it , that's awesome friend . Keep it up\nDo you have hands in experience before taking the exam?\nIndeed I had around 6 months hands on experience dating from 2 years ago. I have to admit that half of the concepts of the course (Delta Live Tables, Structured Streaming, Unity catalog) were foreign to me when I started prepring for the exam. But the notebook and labs provided in Databricks academy are more than sufficient to pick up these concepts.\nCongrats. Would you be able to reveal how much this cost you?\nEverything mentioned ont he list is **free**, except for the **Udemy** course (which is optional) that costs **20$**. In case you're not aware, to pass the actual exam, you need to pay **200$**\n[removed]\nCongrats! How does passing the exam help you career wise? The reason I ask is because Ive been thinking about taking it, but wondering if it would help me career wise.\nIt's a great career booster, and definitively worth it if you are already using Databricks in your job (you may want to go even eventually for professional one). From recruiters perspective, certifications validate candidate's skill and set them apart from other candidates.\nHi, congrats on passing the exam! Do you think it is something that would help me to get a job? Im a DA with 3 yoe and am preparing myself to switch to DE, mostly learning Python and solving Leetcode on Python and SQL. Im trying to do the switch without any mentor or Bootcamp, so its hard to know whats important to learn and whats not, beside Python and SQL\nI would say you should definitvely go for a good Data Engineering certification. Because you have 3 years of experience, I would say it's worth spending more time to pass directly the Databricks Professional Data Engineer Certificate. Now if you haven't used Databricks before, and not really looking specifically to specialize in it, I reccommend that you explore other Data Engineering certifications from cloud providers such as [**DP-203**](https://learn.microsoft.com/en-us/credentials/certifications/azure-data-engineer) before you decide on which one to go for.\nThank you!\nCongrats\nThank you!\n>Thank you! You're welcome!\nthis is so helpful, thank you!\nYou're welcome :)\nCongrats! And thanks for the prep resources\nYou're welcome :)\nCongratulations and thanks for sharing the resources that helped you\nCongrats \nThank you!\nIs the community edition enough to practice?\nYes it is. For the concepts missing from it like Databricks repos, you just need to go over the DB academy course slides. I believe no real practice in these missing concepts is needed to ace the exam\nThanks much for the practical steps and suggestions. I was trying to study for it previously and got overwhelmed (despite using Databricks every day merp)\ncongratss! thank you for the tips. are there any particular benefits you get after achieving this? (just asking out of curiosity)\nDepending on where you are in your career, it can help boost your Data Engineeirng profile for future recruits\nCongratulations!  By any chance, are the databricks courses on the databricks platform you went through free of charge? I'm looking to get the certification as well, but I don't know where to start\nyes, they are free. I have used my company's email since we aqre customers of Databricks, but i heard that you can get access to Datbricks Academy by just signing up with your personal email. In case it doesn't work, DM me and I'll send you all the notebooks through email.\nThank you so much!\nAlso, I was wandering, since you passed the exam, how's that work?\nIt's an online proctored exam. You [register](https://webassessor.com/databricks) for the certification, you pick a date for exam, and you take it online through a secure web browser that you are asked to download. If you pass, you receive your certification within 48 hours.\nThank you for sharing!\nSheesh! 100%. ![gif](giphy|y0qGv0MHbzNQPt5DBy)\nThanks\nCongrats & thank you so much for the links! \nAnd you took almost 1/4 th of the allotted time to finish. That must be a record in itself \nJust gave the exam today. Thank you for your post. Stithapragnas videos were very helpful.\nCongrats! I'm thinking of going for certs and this list looks super helpful.\nGood luck!\nThanks for the tips\nAnytime!\nCongratulations!! Did you have any prior knowledge or experience before starting this?\nYes I had around 6 months of hands-on experience with Databricks. If you don't no worries, just make sure to follow The DB Learning Academy course notebooks and do the labs.\nOh great! That is reassuring. Thanks!\nWhat's the end result of this? Did you get a job or promotion? What job are you trying to get?\nTaking my test in 30 minutes!\nGood luck!\nThanks! 100% as well!\nCongrats!\nWhat do you do with this?\nWhats this for?\nData analyst here. Is this recommended to try to get into DE?\nit depends if you want to work on Databricks after switching in DE, then sure it's helpful.! else even AWS/Azure/GCP certifications will be more helpful\nLol dont you have friends to brag to?",
        "content_hash": "b7d24c5f815bb93f07b973727c18b921"
    },
    {
        "id": "ygieh8",
        "title": "Data engineering projects with template: Airflow, dbt, Docker, Terraform (IAC), Github actions (CI/CD) & more",
        "description": "Hello everyone,\n\nSome of my posts about DE projects (for portfolio) were well received in this subreddit. (e.g. [this](https://www.reddit.com/r/dataengineering/comments/nto0nd/data_engineering_project_for_beginners_v2/) and [this](https://www.reddit.com/r/dataengineering/comments/om3wl5/data_engineering_project_with_a_live_dashboard/))\n\n   But many readers reached out with difficulties in setting up the infrastructure, CI/CD, automated testing, and database changes. With that in mind, I wrote this article [https://www.startdataengineering.com/post/data-engineering-projects-with-free-template/](https://www.startdataengineering.com/post/data-engineering-projects-with-free-template/) which sets up an Airflow + Postgres + Metabase stack and can also set up AWS infra to run them, with the following tools\n\n1. **`local development`**: [Docker](https://www.docker.com/) & [Docker compose](https://docs.docker.com/compose/)\n2. **`DB Migrations`**: [yoyo-migrations](https://ollycope.com/software/yoyo/latest/)\n3. **`IAC`**: [Terraform](https://www.terraform.io/)\n4. **`CI/CD`**: [Github Actions](https://github.com/features/actions)\n5. **`Testing`**: [Pytest](https://docs.pytest.org/en/7.1.x/)\n6. **`Formatting`**: [isort](https://pycqa.github.io/isort/) & [black](https://github.com/psf/black)\n7. **`Lint check`**: [flake8](https://github.com/pycqa/flake8)\n8. **`Type check`**: [mypy](http://mypy-lang.org/)\n\nI also updated the below projects from my website to use these tools for easier setup.\n\n1. [DE Project Batch edition](https://www.startdataengineering.com/post/data-engineering-project-for-beginners-batch-edition/) Airflow, Redshift, EMR, S3, Metabase\n2. [DE Project to impress Hiring Manager](https://www.startdataengineering.com/post/data-engineering-project-to-impress-hiring-managers/) Cron, Postgres, Metabase\n3. [End-to-end DE project](https://www.startdataengineering.com/post/data-engineering-project-e2e/) Dagster, dbt, Postgres, Metabase\n\nAn easy-to-use template helps people start building data engineering projects (for portfolio) & providing a good understanding of commonly used development practices. Any feedback is appreciated. I hope this helps someone :) \n\nTl; DR: Data infra is complex; use this template for your portfolio data projects \n\nBlog: https://www.startdataengineering.com/post/data-engineering-projects-with-free-template/\nCode: https://github.com/josephmachado/data_engineering_project_template",
        "score": 425,
        "upvotes": 425,
        "downvotes": 0,
        "tag": "Blog",
        "num_comments": 37,
        "permalink": "/r/dataengineering/comments/ygieh8/data_engineering_projects_with_template_airflow/",
        "created": 1667046586.0,
        "comments": "Thank you!\nYoure welcome. Please feel free to reach out if anything is unclear or confusing or you have any questions.\nI'm gonna spend the weekend on those, thanks a bunch! I have a basic foundation in Data science, and looking to expand my horizon, which project should I start with?\nYoure welcome. Id recommend starting at https://www.startdataengineering.com/post/data-engineering-project-to-impress-hiring-managers/ this is the simplest. Once you have it running, and get an overview of the components( docker, ec2, Postgres), then Id recommend looking at this article https://www.startdataengineering.com/post/data-engineering-projects-with-free-template/ to understand how the components work together. And then try out the pipeline with a data source if your choosing. I use https://github.com/public-api-lists/public-api-lists to get some data API. Once you get a good understanding of how data is pulled and loaded along with how its scheduled, then Id recommend looking at this airflow project https://www.startdataengineering.com/post/data-engineering-project-for-beginners-batch-edition/ Hope this helps :) LMK if you have any questions.\nThe Legend returns \nHaha :) thank you for the kind words.\nProbably the most useful post I've seen here. Thanks for sharing\nBit of a noob here, but what's the difference between Docker and Terraform? They both seem to \"create\" the server environment.\nDocker is used to containerize your application. For e.g. [this](https://github.com/josephmachado/data_engineering_project_template/blob/main/containers/airflow/Dockerfile) Dockerfile is used to create a container and it specifies what OS it is, etc. You can run docker on any machine and you can think of it as running a separate os (not exactly, but close enough) on the machine. What Docker provides is the ability to replicate OS & its packages (e.g. python [modules](https://github.com/josephmachado/data_engineering_project_template/blob/main/containers/airflow/requirements.txt)) across machines so that you don't run into \"hey that worked on my computer\" type issues. Terraform helps you set up cloud infrastructure. For e.g. You can create an AWS EC2 [instance via code](https://github.com/josephmachado/data_engineering_project_template/blob/75592aa7408fa074dd774afe664779d0d28787be/terraform/main.tf#L76) with Terraform. It is usually preferred over creating infra with boto3 since terraform is easier to work with and it handles creating and deleting infrastructure easier to manage. In the template, I've used 1. Terraform to create an EC2 instance 2. Terraform to [install docker](https://github.com/josephmachado/data_engineering_project_template/blob/75592aa7408fa074dd774afe664779d0d28787be/terraform/main.tf#L107) on that EC2 instance 3. Docker (docker compose to be specific) to run Airflow , Postgres, Metabase within that EC2 instance. Docker compose helps managing multiple docker containers easier. Hope this helps. LMK if you have any questions.\nTitle: Exploitation Unveiled: How Technology Barons Exploit the Contributions of the Community Introduction: In the rapidly evolving landscape of technology, the contributions of engineers, scientists, and technologists play a pivotal role in driving innovation and progress [1]. However, concerns have emerged regarding the exploitation of these contributions by technology barons, leading to a wide range of ethical and moral dilemmas [2]. This article aims to shed light on the exploitation of community contributions by technology barons, exploring issues such as intellectual property rights, open-source exploitation, unfair compensation practices, and the erosion of collaborative spirit [3]. 1. Intellectual Property Rights and Patents: One of the fundamental ways in which technology barons exploit the contributions of the community is through the manipulation of intellectual property rights and patents [4]. While patents are designed to protect inventions and reward inventors, they are increasingly being used to stifle competition and monopolize the market [5]. Technology barons often strategically acquire patents and employ aggressive litigation strategies to suppress innovation and extract royalties from smaller players [6]. This exploitation not only discourages inventors but also hinders technological progress and limits the overall benefit to society [7]. 2. Open-Source Exploitation: Open-source software and collaborative platforms have revolutionized the way technology is developed and shared [8]. However, technology barons have been known to exploit the goodwill of the open-source community. By leveraging open-source projects, these entities often incorporate community-developed solutions into their proprietary products without adequately compensating or acknowledging the original creators [9]. This exploitation undermines the spirit of collaboration and discourages community involvement, ultimately harming the very ecosystem that fosters innovation [10]. 3. Unfair Compensation Practices: The contributions of engineers, scientists, and technologists are often undervalued and inadequately compensated by technology barons [11]. Despite the pivotal role played by these professionals in driving technological advancements, they are frequently subjected to long working hours, unrealistic deadlines, and inadequate remuneration [12]. Additionally, the rise of gig economy models has further exacerbated this issue, as independent contractors and freelancers are often left without benefits, job security, or fair compensation for their expertise [13]. Such exploitative practices not only demoralize the community but also hinder the long-term sustainability of the technology industry [14]. 4. Exploitative Data Harvesting: Data has become the lifeblood of the digital age, and technology barons have amassed colossal amounts of user data through their platforms and services [15]. This data is often used to fuel targeted advertising, algorithmic optimizations, and predictive analytics, all of which generate significant profits [16]. However, the collection and utilization of user data are often done without adequate consent, transparency, or fair compensation to the individuals who generate this valuable resource [17]. The community's contributions in the form of personal data are exploited for financial gain, raising serious concerns about privacy, consent, and equitable distribution of benefits [18]. 5. Erosion of Collaborative Spirit: The tech industry has thrived on the collaborative spirit of engineers, scientists, and technologists working together to solve complex problems [19]. However, the actions of technology barons have eroded this spirit over time. Through aggressive acquisition strategies and anti-competitive practices, these entities create an environment that discourages collaboration and fosters a winner-takes-all mentality [20]. This not only stifles innovation but also prevents the community from collectively addressing the pressing challenges of our time, such as climate change, healthcare, and social equity [21]. Conclusion: The exploitation of the community's contributions by technology barons poses significant ethical and moral challenges in the realm of technology and innovation [22]. To foster a more equitable and sustainable ecosystem, it is crucial for technology barons to recognize and rectify these exploitative practices [23]. This can be achieved through transparent intellectual property frameworks, fair compensation models, responsible data handling practices, and a renewed commitment to collaboration [24]. By addressing these issues, we can create a technology landscape that not only thrives on innovation but also upholds the values of fairness, inclusivity, and respect for the contributions of the community [25]. References: [1] Smith, J. R., et al. \"The role of engineers in the modern world.\" Engineering Journal, vol. 25, no. 4, pp. 11-17, 2021. [2] Johnson, M. \"The ethical challenges of technology barons in exploiting community contributions.\" Tech Ethics Magazine, vol. 7, no. 2, pp. 45-52, 2022. [3] Anderson, L., et al. \"Examining the exploitation of community contributions by technology barons.\" International Conference on Engineering Ethics and Moral Dilemmas, pp. 112-129, 2023. [4] Peterson, A., et al. \"Intellectual property rights and the challenges faced by technology barons.\" Journal of Intellectual Property Law, vol. 18, no. 3, pp. 87-103, 2022. [5] Walker, S., et al. \"Patent manipulation and its impact on technological progress.\" IEEE Transactions on Technology and Society, vol. 5, no. 1, pp. 23-36, 2021. [6] White, R., et al. \"The exploitation of patents by technology barons for market dominance.\" Proceedings of the IEEE International Conference on Patent Litigation, pp. 67-73, 2022. [7] Jackson, E. \"The impact of patent exploitation on technological progress.\" Technology Review, vol. 45, no. 2, pp. 89-94, 2023. [8] Stallman, R. \"The importance of open-source software in fostering innovation.\" Communications of the ACM, vol. 48, no. 5, pp. 67-73, 2021. [9] Martin, B., et al. \"Exploitation and the erosion of the open-source ethos.\" IEEE Software, vol. 29, no. 3, pp. 89-97, 2022. [10] Williams, S., et al. \"The impact of open-source exploitation on collaborative innovation.\" Journal of Open Innovation: Technology, Market, and Complexity, vol. 8, no. 4, pp. 56-71, 2023. [11] Collins, R., et al. \"The undervaluation of community contributions in the technology industry.\" Journal of Engineering Compensation, vol. 32, no. 2, pp. 45-61, 2021. [12] Johnson, L., et al. \"Unfair compensation practices and their impact on technology professionals.\" IEEE Transactions on Engineering Management, vol. 40, no. 4, pp. 112-129, 2022. [13] Hensley, M., et al. \"The gig economy and its implications for technology professionals.\" International Journal of Human Resource Management, vol. 28, no. 3, pp. 67-84, 2023. [14] Richards, A., et al. \"Exploring the long-term effects of unfair compensation practices on the technology industry.\" IEEE Transactions on Professional Ethics, vol. 14, no. 2, pp. 78-91, 2022. [15] Smith, T., et al. \"Data as the new currency: implications for technology barons.\" IEEE Computer Society, vol. 34, no. 1, pp. 56-62, 2021. [16] Brown, C., et al. \"Exploitative data harvesting and its impact on user privacy.\" IEEE Security & Privacy, vol. 18, no. 5, pp. 89-97, 2022. [17] Johnson, K., et al. \"The ethical implications of data exploitation by technology barons.\" Journal of Data Ethics, vol. 6, no. 3, pp. 112-129, 2023. [18] Rodriguez, M., et al. \"Ensuring equitable data usage and distribution in the digital age.\" IEEE Technology and Society Magazine, vol. 29, no. 4, pp. 45-52, 2021. [19] Patel, S., et al. \"The collaborative spirit and its impact on technological advancements.\" IEEE Transactions on Engineering Collaboration, vol. 23, no. 2, pp. 78-91, 2022. [20] Adams, J., et al. \"The erosion of collaboration due to technology barons' practices.\" International Journal of Collaborative Engineering, vol. 15, no. 3, pp. 67-84, 2023. [21] Klein, E., et al. \"The role of collaboration in addressing global challenges.\" IEEE Engineering in Medicine and Biology Magazine, vol. 41, no. 2, pp. 34-42, 2021. [22] Thompson, G., et al. \"Ethical challenges in technology barons' exploitation of community contributions.\" IEEE Potentials, vol. 42, no. 1, pp. 56-63, 2022. [23] Jones, D., et al. \"Rectifying exploitative practices in the technology industry.\" IEEE Technology Management Review, vol. 28, no. 4, pp. 89-97, 2023. [24] Chen, W., et al. \"Promoting ethical practices in technology barons through policy and regulation.\" IEEE Policy & Ethics in Technology, vol. 13, no. 3, pp. 112-129, 2021. [25] Miller, H., et al. \"Creating an equitable and sustainable technology ecosystem.\" Journal of Technology and Innovation Management, vol. 40, no. 2, pp. 45-61, 2022.\nWhat easy-to-digest resources have you used to set up a docker environment for your DS work?\nTitle: Exploitation Unveiled: How Technology Barons Exploit the Contributions of the Community Introduction: In the rapidly evolving landscape of technology, the contributions of engineers, scientists, and technologists play a pivotal role in driving innovation and progress [1]. However, concerns have emerged regarding the exploitation of these contributions by technology barons, leading to a wide range of ethical and moral dilemmas [2]. This article aims to shed light on the exploitation of community contributions by technology barons, exploring issues such as intellectual property rights, open-source exploitation, unfair compensation practices, and the erosion of collaborative spirit [3]. 1. Intellectual Property Rights and Patents: One of the fundamental ways in which technology barons exploit the contributions of the community is through the manipulation of intellectual property rights and patents [4]. While patents are designed to protect inventions and reward inventors, they are increasingly being used to stifle competition and monopolize the market [5]. Technology barons often strategically acquire patents and employ aggressive litigation strategies to suppress innovation and extract royalties from smaller players [6]. This exploitation not only discourages inventors but also hinders technological progress and limits the overall benefit to society [7]. 2. Open-Source Exploitation: Open-source software and collaborative platforms have revolutionized the way technology is developed and shared [8]. However, technology barons have been known to exploit the goodwill of the open-source community. By leveraging open-source projects, these entities often incorporate community-developed solutions into their proprietary products without adequately compensating or acknowledging the original creators [9]. This exploitation undermines the spirit of collaboration and discourages community involvement, ultimately harming the very ecosystem that fosters innovation [10]. 3. Unfair Compensation Practices: The contributions of engineers, scientists, and technologists are often undervalued and inadequately compensated by technology barons [11]. Despite the pivotal role played by these professionals in driving technological advancements, they are frequently subjected to long working hours, unrealistic deadlines, and inadequate remuneration [12]. Additionally, the rise of gig economy models has further exacerbated this issue, as independent contractors and freelancers are often left without benefits, job security, or fair compensation for their expertise [13]. Such exploitative practices not only demoralize the community but also hinder the long-term sustainability of the technology industry [14]. 4. Exploitative Data Harvesting: Data has become the lifeblood of the digital age, and technology barons have amassed colossal amounts of user data through their platforms and services [15]. This data is often used to fuel targeted advertising, algorithmic optimizations, and predictive analytics, all of which generate significant profits [16]. However, the collection and utilization of user data are often done without adequate consent, transparency, or fair compensation to the individuals who generate this valuable resource [17]. The community's contributions in the form of personal data are exploited for financial gain, raising serious concerns about privacy, consent, and equitable distribution of benefits [18]. 5. Erosion of Collaborative Spirit: The tech industry has thrived on the collaborative spirit of engineers, scientists, and technologists working together to solve complex problems [19]. However, the actions of technology barons have eroded this spirit over time. Through aggressive acquisition strategies and anti-competitive practices, these entities create an environment that discourages collaboration and fosters a winner-takes-all mentality [20]. This not only stifles innovation but also prevents the community from collectively addressing the pressing challenges of our time, such as climate change, healthcare, and social equity [21]. Conclusion: The exploitation of the community's contributions by technology barons poses significant ethical and moral challenges in the realm of technology and innovation [22]. To foster a more equitable and sustainable ecosystem, it is crucial for technology barons to recognize and rectify these exploitative practices [23]. This can be achieved through transparent intellectual property frameworks, fair compensation models, responsible data handling practices, and a renewed commitment to collaboration [24]. By addressing these issues, we can create a technology landscape that not only thrives on innovation but also upholds the values of fairness, inclusivity, and respect for the contributions of the community [25]. References: [1] Smith, J. R., et al. \"The role of engineers in the modern world.\" Engineering Journal, vol. 25, no. 4, pp. 11-17, 2021. [2] Johnson, M. \"The ethical challenges of technology barons in exploiting community contributions.\" Tech Ethics Magazine, vol. 7, no. 2, pp. 45-52, 2022. [3] Anderson, L., et al. \"Examining the exploitation of community contributions by technology barons.\" International Conference on Engineering Ethics and Moral Dilemmas, pp. 112-129, 2023. [4] Peterson, A., et al. \"Intellectual property rights and the challenges faced by technology barons.\" Journal of Intellectual Property Law, vol. 18, no. 3, pp. 87-103, 2022. [5] Walker, S., et al. \"Patent manipulation and its impact on technological progress.\" IEEE Transactions on Technology and Society, vol. 5, no. 1, pp. 23-36, 2021. [6] White, R., et al. \"The exploitation of patents by technology barons for market dominance.\" Proceedings of the IEEE International Conference on Patent Litigation, pp. 67-73, 2022. [7] Jackson, E. \"The impact of patent exploitation on technological progress.\" Technology Review, vol. 45, no. 2, pp. 89-94, 2023. [8] Stallman, R. \"The importance of open-source software in fostering innovation.\" Communications of the ACM, vol. 48, no. 5, pp. 67-73, 2021. [9] Martin, B., et al. \"Exploitation and the erosion of the open-source ethos.\" IEEE Software, vol. 29, no. 3, pp. 89-97, 2022. [10] Williams, S., et al. \"The impact of open-source exploitation on collaborative innovation.\" Journal of Open Innovation: Technology, Market, and Complexity, vol. 8, no. 4, pp. 56-71, 2023. [11] Collins, R., et al. \"The undervaluation of community contributions in the technology industry.\" Journal of Engineering Compensation, vol. 32, no. 2, pp. 45-61, 2021. [12] Johnson, L., et al. \"Unfair compensation practices and their impact on technology professionals.\" IEEE Transactions on Engineering Management, vol. 40, no. 4, pp. 112-129, 2022. [13] Hensley, M., et al. \"The gig economy and its implications for technology professionals.\" International Journal of Human Resource Management, vol. 28, no. 3, pp. 67-84, 2023. [14] Richards, A., et al. \"Exploring the long-term effects of unfair compensation practices on the technology industry.\" IEEE Transactions on Professional Ethics, vol. 14, no. 2, pp. 78-91, 2022. [15] Smith, T., et al. \"Data as the new currency: implications for technology barons.\" IEEE Computer Society, vol. 34, no. 1, pp. 56-62, 2021. [16] Brown, C., et al. \"Exploitative data harvesting and its impact on user privacy.\" IEEE Security & Privacy, vol. 18, no. 5, pp. 89-97, 2022. [17] Johnson, K., et al. \"The ethical implications of data exploitation by technology barons.\" Journal of Data Ethics, vol. 6, no. 3, pp. 112-129, 2023. [18] Rodriguez, M., et al. \"Ensuring equitable data usage and distribution in the digital age.\" IEEE Technology and Society Magazine, vol. 29, no. 4, pp. 45-52, 2021. [19] Patel, S., et al. \"The collaborative spirit and its impact on technological advancements.\" IEEE Transactions on Engineering Collaboration, vol. 23, no. 2, pp. 78-91, 2022. [20] Adams, J., et al. \"The erosion of collaboration due to technology barons' practices.\" International Journal of Collaborative Engineering, vol. 15, no. 3, pp. 67-84, 2023. [21] Klein, E., et al. \"The role of collaboration in addressing global challenges.\" IEEE Engineering in Medicine and Biology Magazine, vol. 41, no. 2, pp. 34-42, 2021. [22] Thompson, G., et al. \"Ethical challenges in technology barons' exploitation of community contributions.\" IEEE Potentials, vol. 42, no. 1, pp. 56-63, 2022. [23] Jones, D., et al. \"Rectifying exploitative practices in the technology industry.\" IEEE Technology Management Review, vol. 28, no. 4, pp. 89-97, 2023. [24] Chen, W., et al. \"Promoting ethical practices in technology barons through policy and regulation.\" IEEE Policy & Ethics in Technology, vol. 13, no. 3, pp. 112-129, 2021. [25] Miller, H., et al. \"Creating an equitable and sustainable technology ecosystem.\" Journal of Technology and Innovation Management, vol. 40, no. 2, pp. 45-61, 2022.\nI have used CDK to create infrastructure and I found it very confusing due to unclear documentation and methods. What's your opinion on using CDK vs Terraform ?\nI've had similar experience. I tried to use CDK for some project a while back, but due to it being complex to understand & Terraforms wider range of providers I went with Terraform.\nHmm not as of yet. Perhaps sometime in the future. If you already have dbt setup I'd start with dbt-expectations, although if you need more complex checks GE might be better for writing custom tests.\nLove the newsletter, thanks for everything you do\nThank you for reading :)\nThank you so much!\nThank you :)\nOh shoot, I didnt know you were on Reddit. Im connected with you on LinkedIn! Thanks for the post.\nI started here, and later started posting on LinkedIn. I prefer this subreddit for more focussed/deep conversations.\nThanks for sending me down a series of rabbit holes this morning! I haven't really dug into Metabase or Dagster before. The former seems like an easy way for me to play with visualizations and the latter seems like it might solve some of my traceability woes in Airflow.\nThat's great. IMO Metabase is really easy to setup and run locally ( I haven't checkout Lightdash or Rill data for viz)& Dagster is much easier to setup, run & test compared to Airflow & their UI is pretty slick.\nthank you\nAbsolutely love your stuff, super helpful, thanks!\nU/joseph_machado great post.\nThank u\nKing \nThis is awesome! Great job!\nFantastic resources. Thanks for sharing.\nShould check out plural.sh for setting up data infrastructure\nInteresting project (ha I used to work with their CTO back in the day)! They seem pretty legit, but for a portfolio project I'd want to use the tools that are most popular these day Terraform, docker and didn't want readers to have to learn another tool. Perhaps in a company setting this would make much more sense.\nThank You !\nIm sorry, Im new in the industry but I dont understand where I have to run the code like make\nYou have to run it in a CLI terminal.\nI encountered a crontab error #13 [ 9/10] RUN crontab /etc/cron.d/pull_bitcoin_exchange_info #13 0.581 \"/etc/cron.d/pull_bitcoin_exchange_info\":3: bad minute #13 0.581 errors in crontab file, can't install. #13 ERROR: executor failed running [/bin/sh -c crontab /etc/cron.d/pull_bitcoin_ I only cloned your github, did not change anything else. Am I missing something? Thanks\nits okay, solved it already. My crontab file has an additional blank line!",
        "content_hash": "14b9be80e473601b1faf1d74dea7b36b"
    },
    {
        "id": "1b1f95l",
        "title": "Expectation from junior engineer ",
        "description": "",
        "score": 419,
        "upvotes": 419,
        "downvotes": 0,
        "tag": "Discussion",
        "num_comments": 132,
        "permalink": "/r/dataengineering/comments/1b1f95l/expectation_from_junior_engineer/",
        "created": 1709049403.0,
        "comments": "I understand how people struggle to break into the field \nGatekeeping is real.\nJust curious but which of these expectations are unrealistic/overkill? Id think that if this is a new grad job then finding applicants with experience in Spark & Airflow/cron plus some basic experience with cloud services will be hard to come by (imo) because you learn a lot of that on the job (or at least that is what Ive experienced). I guess its like expecting that applicants to a junior backend role would have advanced knowledge of redis/memcache and basic experience with GraphQL. Idk. Wdyt?\nI think advanced sql is maybe unrealistic, and the phrase \"system design\" seems like a red flag. Edit: cleared up a thing I misunderstood.\nTo be fair, any good CS program teaches basic Cron in whatever class they teach \\*NIX systems in. Usually some variant of Systems Programming or intro to OSes.\nIt's a quite pretentious and bad written \"Knowledge of advanced SQL\", what's that supposed to mean? Btw we're spearking of a junior figure so \"advanced\" is not the word i would use considering that it may be a first employment... \"Mid level at Data Structures\" another nonsense, what does that mean? What the candidate is supposed to know? And how deep? \"Mid\". This is probably the product of a drunk recruiter that does not have any idea of what the job consists of and wrote down some random keywords.\nNot as a rule, but generally when I hear \"advanced SQL\" they mean window functions and CTE/subquery/temp table, whichever best fits the need. That being said it does seem like the recruiter might benefit from a conversation with the hiring manager to help refine candidates.\nMy kingdom for an established, accepted definition for advanced SQL. I ended up having a two month back and forth with a data scientist who was \"skilled in advanced SQL\" but didn't want to approve my PR over a window function that looked \"hacky\" when it turned out what they meant was \"I don't know what this is and good luck getting me to admit it\"\nThat's amazing. You can't learn if you can't acknowledge ignorance. You can't learn what you already know either, but that's a different kind of ignorance. DS had me curious as someone that was studying for a stats degree but the more DE work I did the more I found the mindset of the people I was working with was open to recognizing their own ignorance and focusing on solving problems, ego be damned.\nI do avoid window functions if at all possible. Perhaps because at my first job LEFT JOIN was too much for my coworkers. I had to create a huge flat table (MB scale) so they could get work done.\nHuh what are you saying?\nLol wat\nIndeed, that's probably what it means, still when the requirements are this generic a couple of examples in general help to clarify any doubt. Also it has to be said that companies sometimes push the requirements for a job in order to filter the candidates, moreover if it's a market where data engineering positions are saturated they can push further more (imagine having 100+ candidates for a role vs having only 10, it draws a line on who has the negotiation power)\nSQL noob here, what does CTE stand for? I will add it to my list of stuff to learn.\nCommon table expression. I just means with my_cte as ( select  )\nCommon table expression. it's \"proper\" purpose is for hierarchical queries or where you need the same subquery multiple times. I find they are often used instead of simple subqueries. But, that is entirely down to personal taste.\n> I find they are often used instead of simple subqueries. Because they make sub-queries easier to read, that's probably the main use for them.\nI think that depends though... they're executed differently depending on the db system\nI didn't learn about CTEs until *after I had taught SQL classes.* I honestly can't understand how they are not part of the curriculum. I remember doing some nested subqueries that were hard to read and would have been so much easier to explain as CTEs\nData structures is also super dependent. Id hope nobody is throwing a junior engineer into the codebase where complex data structures/algos are their decision. the only thing Id hope for is them understanding when to use set based DB operations vs for/while loops. Ideally you let the db determine how its going to search, and maybe let a junior dabble in indexing. If youre running custom binary searches/etc in code as a DE, you probably messed up\nTotally agree, implementing a custom db search algorithm would be itself alone quite demanding (and fool unless there are really specific needs), especially, as you said, because in general databases have good optimization engines that defeats the purpose of creating custom algorithms. I've never once been asked or found in position where I had to ask someone to develop a custom binary search algorithm, but maybe i'm just a poor DE :') In general it's cool if a junior knows the data structure, as this can lead to a better understanding of the indexing mechanism and a better optimization of processes, but I wont say it's something mandatory.\nBut it has emojis!!!\nSilly me, how didn't I notice, forget what I just said, then we should add 9. 3+ year experience :P 10. At least 2 advanced certifications ((00))\nAnd I love how \"testing\" is literally the final item on the \"nice to have\" section of the list.\nThe take home I give is essentially a unit test suite. If you're hired you'll be starting with debugging and testing. One of the best ways to get an understanding of what pipelines are doing without requiring a lot of hand holding and training. You're unlikely to bring the database down either.\nI get it though- you want data engineers that know more than a basic single table select statement. You can learn CTEs, window functions, joins, etc in a few days.\nthat's pretty much every job description now.\n>And how deep? Just the tip!\nBut wait, emojis!\nI applied for a job last year as an ML Scientist. The interviewer asked me if I could come up with a new DL architecture. Because in that domain no one worked before. I don't remember the problem exactly but his problem could easily be solved with a basic architecture. But he insisted that this problem cannot be solved with existing architectures. Also, he didn't know the basic terminology in the domain, lol. So, when they put big words like that, they don't really know what they are asking for. Probably, someone told them that they need Spark, SQL, Scala etc. Then they just randomly assigned mid/low/advanced keywords.\n> advanced SQL Like JOIN\nJunior DE: Advanced SQL. Tbh, whilst pure clickbait, it's an example of the worst level of expectations have for juniors.\nBut tbh, if you cant do advance sql then should you even apply to be de? Should junior de have the same sql requirements as a junior da?\n> But tbh, if you cant do advance sql then should you even apply to be de? Yes, absolutely. Everybody thinks every DE role = good SQL. Not all DE roles are the same although everybody is under the impression that every DE role is exactly the same. If you want to do a SQL only role, then yes, your SQL better be good. The modern DE doesn't do just SQL. I speak from experience as well - a lot of the people I have worked with all know SQL exceptionally well. The second you step outside of SQL (API calls, CI/CD, source control with git, any kind of ingestion which is outside of the SQL database), then they either know nothing at all or start to struggle. They literally only know how to do one thing one way, and that's with SQL. On top of that, what you mean by advanced SQL isn't always what they mean by advanced SQL. They also might not even know SQL at all, they just \"want a specialist\". Basically, a lot of the ad is bollocks.\nI haven't written SQL in a long time yet my work schedule involves complex graph data models. DE is just software engineers working with massive amounts of data, don't equate it with just writing SQL.\nHow about a basic understanding of relational and NoSQL modelling concepts? You know, normalisation, data types, indexes, constraints, partitions etc. given that's what most folk will be working with most of the time? How about the basics in your basics list?\nAs someone trying to break in the field this is great. Any other basics that actually related to the field to look up?\nWhat you really need 1. Good understanding of SQL joins and data modeling 2. How do you read data from a 100Gb file? -> spark, duckdb. 3. Knowing when to use a data lake vs Warehouse.(AWS, Azure, GCP) 4. Basic ETL (at least 2 projects /experiences) 5. NoSQL vs SQL usage for a specific job, drill down details if needed Generally, good data source design for querying and end to end data flow habits and approaches should get you the job\nMaybe add OLTP vs OLAP\n2. Just awk / sed / grep it\nIs this a serious suggestion? I'm about to give an interview for a data engineer trainee role and am curious about it\nI was joking and you could make a bell curve meme from this. But if you're given a 100GB csv file and your task is to extract a few rows once and maybe summarize some values why overcomplicate it.\nFun fact: That was literally why `grep` was written: to find matching rows in a file too big to be loaded into the memory of the computers of the time.\nHonestly I thought to myself, this might work if retrieving a single sentence/pattern but 100gb is a lot. Thanks for the explanation, I hope I do well.\nIf youre hiring where do I apply?\nStupid question. Is apache spark a data lake and snowflake a data warehouse? I plan on learning both but Im at the learning sql and python stage.\nI disagree. As a Jr. DE you should really just know the main concepts of writing efficient code, conceptually what an ETL/ELT is, being capable of manipulating/loading data from files, being able to ping an API, being able to do the same with pyspark (no optimizations), and being able to write SQL to join a couple tables with filtering. Couple things I have issues/questions: 1 - Why binary search? 2 - You can ask for familiarity of cloud services but no experience. I dont expect a junior to pay money out of their pocket to spin up cloud services. Besides, its not super hard to learn/teach cloud services. I think there is DevOps in DE, but you cant expect a junior DE to have experience on it. 3 - Kafka is not a core skills of DE. 4 - How do you measure the level of data structures? (If you are thinking leetcode, then youre doing it wrong.). 5 - What is advance SQL? Are you thinking tuning SQL? There is no way a Jr should be expected to do that. I like to think of Jrs as teachable chaos monkeys. The biggest responsibility of a Jr is being able to learn. They will break stuff, and it is your responsibility to catch those unexpected broken pipes and guide them on a solution. I dont expect any Jr to be incredibly helpful by themselves in the first year. First 6 months is all about learning and performing tasks with guidance. Second 6 months is more of prepping them to be able to manage a few simple pipelines.\nWho is agreeing with this anyway\nLots of employers. Which is why Im glad you posted this because it gives perspective to all the DEs in here. I dont agree with your points but I think it can bring productive discussions.\nI dont agree with \"its not hard to learn cloud services\" - applicable if you dont care about costs, scalability, security and so on, or you have a whole team taking care so you dont do any stupid thing\nJunior DE: this Principal DE: shouod be a god, a visionary, an IT department in itself \nI've hired probably 30ish data engineers over the past few years and interviewed many more, I have never asked a CS style algorithm question because DE =/= SWE. I don't care if you can invert a binary tree. It's not important. I care if you can write clean Python code to manipulate data with Pandas (or Polars which I like even more), if you can model data for operational and BI purposes, if you're very good at using SQL to get the data you want quickly and efficiently, and if you understand the tradeoffs inherent in various data systems (e.g. can you explain to me the practical problems inherent in event based architectures and how those relate to the CAP theorem). I care if you follow good SWE practices when developing data pipelines. A little Devops/IAC experience is a nice to have. But that's all for mid-senior career people. For junior staff I expect you to be competent in SQL and Python and be able to design and implement simple data models. The rest you'll pick up from working on my team and seeing how we do things.\n1. Finally learned all the skills 2. Salaries get reduced because of refocusing/economy/Ukraine war/\\[Insert excuse\\] 3. AI takes over engineering jobs.\nlol if a tech company tells you they can't pay you proper salary because of the war in Ukraine, I'd avoid that place as a plague.\n4. Land first FT DE job. The team lead simultaneously gaslights himself and me that I don't know python. Turns 20 line commit into 300 lines, does things like removes 1 variable in favor of 6x identical system calls. Get \"laid off\". Realize I have 10 years python experience to team leads 6. Realize team leads half dressed children showed up to half our calls and he was splitting his attention between our enterprise level system serving 30 million homeless, and babysitting his kids. I came to tech from finance assuming people would be more \"with it\", logical, cutting edge. Got a showcase of the opposite.\n4. This month's over-engineered fad comes along and unpicks all you've learnt to date.\nThis is comedy, that's borderline unicorn hunting\nim a junior engineer and i just know 2 and 3, the rest the company is teaching me\nNo one needs to know this much as a junior. This post is a joke\nEven seniors don't know everything. They are just extremely good at one thing and a surface level knowledge in other things.\nA lot of the advice in this thread hasnt taken a look at the experience wanted for data engineers in job descriptions lol. The amount of cloud knowledge they want alone is insane. I think the days of just understanding Python and sql and an ETL process to become a data engineer is over.\nNot too horrible. You can usually pick up knowledge of these things in a cs databases course and software engineering principals class. Replace kafka with any pub/sub since gcp exists. Move to necessary. Add enterprise routing patterns if dealing with a legacy rabbitmq inplementation. Id move spark as well. It is becoming legacy to write actual spark code. Most applications work with SQL these days.\nThats bullshit, you just need to know the basics of programming, SQL and Python. The rest you can learn on the job\nLooks like written by someone who has just heard bits about DE from LinkedIn. What kind of Junior DE wouldve worked with Kafka ? Im sure their interview process involves tons of LC too.\nCan someone help me understand why we need everyone to understand binary search so well its listed here? How often do DEs write search from scratch? I code, and could implement binary search pretty quickly, but not off the top of my head.\nNot that I agree with the list but binary search and search trees is critical to understand when designing any data retrieval system at scale. And for the sake of coding, I'd expect any half decent engineer to be able to knock out binary search over an array in Max 30 minutes tbf without needing to look it up\nThank you, thats a good self-testing goal. I always fall back to remembering specific examples and working backwards (how I sort playing cards), but would probably fail to create a working implementation under pressure during an interview.\nThis has Indian startup written all over it.\nId be happy to find a senior that is really ticking all these points\nI know none of this shit and am a senior engineer. Copy-pasting `KubernetesPodOperator()` and `SELECT * FROM BIG_FUCKING_TABLE` all day every day. edit /s kinda if it's not clear. If someone can blatantly shitpost on LI, I figure I'll do it on the safe space that is Reddit\nThese days, all I expect is for tickets to be updated and instructions followed. One can wish!\nEven a senior might not be aware of all the requirements. But its usually like this because hiring managers want the best candidates on their team. And when you start working, you will quickly realise only 30 to 50% of whats mentioned in the job description will be the actual job and rest is all made up.\nI refuse to take any job postings with emojis seriously\nLol, when it takes 8 months to find one junior DE job and still not get it, then come across these. It's like why even try anymore.\nI am somewhat junior myself, and I would appreciate it if someone can explain to me what is data engineering system design and how to test code in the data engineering field. Thanks\nI am a little baffled that there's no mention of data modeling. The end goal of DE is to prepare data for reporting/analysis/machine learning. How can anyone know how to transform their data if they don't understand the target? What I expect from a Junior DEis: \\#1 Basic understanding of data modeling and data architecture Then they learn SQL, Python, Kafka, etc etc\nWait this doesnt seem that bad. And Im an analyst/DS\nWhere can I apply to this job ? I'm new in the world of DE , I want to get experience as DE  I know python, golang and basis of scala 3. SQL, MongoDB, basic airflow and mage, dbt, pyspark, hadoop, gcloud, aws and azure. I'll appreciate if you know a company that offers internships for DE students remotely or hibrid in mexico. Or if you have an advice, I'll learn of your experiences Thanks\nThat's not a job description. It's a list of buzzwords. Don't apply; these people don't know their way out of a paperbag, let alone managing an engineering team suitable for a junior. They really want a senior for cheap. Let them try, but don't waste your time.\nDetails of this list aside ya'll should remember DE is typically a mid-level career that you move into laterally with experience in SWE or DS. This really isn't asking much. If you're freaking out about the basics here I'm not sure what to tell you.\nAll you need if you are a junior is decent Python, basic SQL, knowledge of what YAML and JSON are, a great work ethic, and a willingness to listen to your senior/staff engineers.\nLet's ignore for a minute how poorly that is written! The ask feels like it comes from a company that does not have anyone in a data capacity and have just searched for \"data engineer skills\". It is unrealistic to expect a large proportion of junior DEs to have all those skills. Especially the \"experience\" part. Text book and practice is not \"experience\". Some of the best junior and entry level DEs I have worked with have had barely any experience above concept but have gone on to be great engineers.\nClear indicator of \"I dunno how to manage so I'll try to find someone skilled and brilliant for peanuts to keep under my thumb\".\nThis is poorly written. Writing test cases should be considered a basic as DE is a subset of SWE. In fact, aside from a bit more SQL, there probably isnt too much more a junior DE should know compared to a junior SWE.\nEveryday I being asked the same question. How to get into IT. And mostly my answers is If you are not there, probably you aint. People who into IT get there due to intrinsic ability to understand structure and composition. Disclaimer I have been teaching for 10 years, been coding for 30 years. My Daughter is teaching for 5 years. I spent 1 year making my first computer. I was learning how to write in assembly for one year. I was learning C for two years. It took more than 3 years to write decent code. Did I want to be a programmer? No. Did I know I will earn a lot of money? No\nIf you are able to do like 5 of those... You're good to go with the hiring\nAs a lead DE in some companies lol\nThey don't expect everything like removing all the goods to have stuff... What they're thinking is that people learn DE in college?\nOk\nIt's just crazier and crazier\nname and shame\nIm senior and thats how I describe my skills lol\nWho wants to learn with me?\nDefine advanced sql....\nI would gladly take that person as a senior\nAnd i want the pay to be at least 100k , but we both know we aren't gonna get either of those stuff\nI had all of these when I got my first Junior DE job. I was already a backend dev for a year before that.\nThis is a pretty standard mid tier job. 2-4 years experience. This isn't junior.\nI expect the juniors to know how to code. Do they need to know python or scala? No, Java or a different language is fine, they can learn.\nPretty normal for basic.\nIdk actually seems loose depending on the company.\nObviously title doesn't match description.\nThis is clearly written by a recruiter who doesnt really know what theyre talking about- like tf does mid level at data structures mean? You either understand them or you dont But tbh nothing theyre asking for here is outrageous.. pretty much any DE at a large company will have what theyre asking for\nIm probably going to get hate for this but Poorly written but seems reasonable\nAn engineer on a junior salary.\nThe fucking emojis\nthe most disturbing part is having all the required skills(even the skills under nice to have) and still getting no interview invitation.\nThis guy was just trying to sell his course, this post has a link to his course to cover these topics\nBasically, what a senior does? If I am an Uber driver, do I need to have an FIA license to drive people? What will I tell them? To hold tight?\nThis is for Atlassian\nBut there are emojis so it must be a fun and easygoing and fresh and not toxic or mentally exhausting environment\nThat's not a junior position. Full stop. I hate it when this happens.\nPost showing people at HR don't know anything number #42069\nThis is a perfect example of recruiters and managers not knowing anything about data engineering. I recently had a recruiter ask me, \"What cloud sql integration spark data warehouse experience do you have?\" I was like is that 5 questions or 1 and then he quickly changed subjects. Like cool yeah I won't be working with you. Fun times.\nIt would be so much easier to hire if they changed to Desired skills instead insane lists of \"must haves\"\nYeah...technology first, data modeling second, while when I started some 12 years ago it was the other way around. A year later, they'll discover the need of a better data governance.\nI think the recruiter mistook hiring for IT department with hiring jr data engineer\nMy opinion depends on if \"experience\" means \"professional experience\". CS degree covers 1 and 2. Number 3 takes about 4 weeks. Spark/Airflow are a bit tricky to get started with solo, but if you want to do DE... I'd still negotiate to get the Jr. removed after completing concrete goals for the very first eval.\nA friend of mine (who is more on the business side) asked if I had any recommendations for a good data engineer. I took a look at their job posting. People have no idea what to put on a job posting. They had nice to have: data science and JavaScript The role doesnt have any DS and the role would never call for JS\nOh Jesus, the cute icon next to each bullet point is its warning.\nany job description with emojis is a no go for me",
        "content_hash": "f9fd036dff6116d6c42f4f8b1e3bff98"
    },
    {
        "id": "1c1cbfg",
        "title": "Common DE pipelines and their tech stacks on AWS, GCP and Azure",
        "description": "",
        "score": 413,
        "upvotes": 413,
        "downvotes": 0,
        "tag": "Discussion",
        "num_comments": 63,
        "permalink": "/r/dataengineering/comments/1c1cbfg/common_de_pipelines_and_their_tech_stacks_on_aws/",
        "created": 1712833615.0,
        "comments": "This looks like a slide taken from a consulting firms deck in 2020.\nYep, old. Predates MS fabric on azure.\nYea I was like.. this already feels old\nBilling rocket 101\nracket?\nI'm guessing this is tongue-in-cheek because it does say 2020 on the slide in the bottom right ;-)\nYeah, not sure how accurate this is for GCP at least. Dataflow, DataPrep, and DataProc not suuuper popular among the people I know. A company I work with basically skips that entire section of the diagram and it's all just Apps  Event driven Pub/Sub  BigQuery or the same thing, but using event arc with Cloud Functions v2. Infra cost is incredibly low. Stays in the 4 figure range and they stream data from around the world 24/7.\nIt's not accurate at all, it's marketing garbage probably made by someone who ISN'T a data engineer. I can't fathom how this post got 200+ up votes. Are we turning into r/datascience ?\nProbably bots and sheit\n4 figures a month or year?\nYeah, sorry per month. Per year would be pretty crazy. This is a data intensive company. _A lot_ of data in BQ. They're just on the ad-hoc plan too, so probably could lower costs over time even more, but workloads are relatively bursty. We talked about this recently, I think almost 25% of monthly spend is due to them needing a single Windows VM to interact with a specific 'legacy' technology partner and they haven't been able to rewrite some software to a newer .NET version yet. It's literally the license too, not the VM itself that is the majority of spend.\nData intensive but having 4 figure monthly cost including storage and compute on BQ while using ad hoc? Do we have a different definition of data intensive? :) even a couple dozen TB of data put you in a mid 4 figure range easily for ad hoc storage and slot-free computing.\nlol yes. In no world is 4 figures a month data intensive. Thats what I imagine 1000 smart fridges generate.\nI'd really like to hear more about how one can keep costs down with GCP for DE and MLOps. We're paying a lot of money for things like Composer - way too many environments (dev/stg/prd, sometimes multiple dev/stg so multiple developers work in parallel). Most of our pipelines are batch but I feel our costs are mainly fixed and not due to the volume of data...\nCheck into DigitalOcean. Even just moving your dev environment can save lots of $$ compared to GCP.\nI find it pretty funny how \"complex\" everything has become when in reality it's nothing but: \"Take data from the source, store it in an orderly way in a database, consume it to create business value\" The more shit you use the more it costs you, which is why they are all pushing that business model. It's fine for us technical people because we earn extra as well, but if I was a business owner, I'd not want to deal with all this shit.\nTotally agree. This looks hideously complicated compared to what it should be.\nI used a simple script that ran on an Azure function to clean and upload some data to a database. It was like a week worth of work. And then you see the overcomplicated shit some people come up with to solve the exact same issue and I just don't get it.\nCame here to say this.\nNailed it. This is ultra-expensive over-engineering at its finest.\nDo you have any advice for which services to use when doing data analysis properly in a data warehouse? We host out production database on AWS - its basically where all the data of out Webshop/Platform is stored. We are thinking of building a datawarehouse with ODI (Oracle) Are there any other better options? Redshift seems awfully expensive for what we are trying to achieve. I also thought about using a data mart from PowerBi. We would like to access the data in the end with PowerBI.\nMy first piece of advice would be to avoid cloud providers. My second piece of advice would be, if you are set on using PowerBI as your front end, don't use Oracle for your warehouse, not because Oracle is bad, but because it's unnecessarily powerful and expensive. PowerBI can store all the data it needs for the dashboards you create in an optimized format. Either in the Cloud (Power BI Service) or on Premise (Power BI Report Server). So, for your data warehouse, I would use PostgreSQL; it's good and has no extra licensing costs associated with it. Then, you use a PowerBI on-premise deployment. There, you create your dashboards and your data mart, which are stored locally on the VM and can be accessed by anyone in the company.\nBut we have a Postgre Database already in AWS - so we could just build our data warehouse there? Also deploying PowerBI on premise sounds unnecessarily complex. Why not just use PowerBI premium per user and use a cloud data mart?\nBoth of those options are okay. You just pay a premium to the cloud provider to avoid the \"complexity\" of having to host the stuff yourself.\nAre you saying that the technology hasnt actually changed and its just repackaged/renaming of the original VMware and db infrastructure? No way.\nSome namings are outdated (e.g. Data Studio -> Looker Studio). And very likely that some other stuff aren't included. Neat graphic to look at anyway.\nTry DIA, it's free\nData bricks is cloud agnostic, so it wouldn't make sense to sit in only the Azure area. Also, databricks has delta tables, so its kind of hard for me to see it only sitting in the \"Preperation and Computer\" section. Also, how common is a document store/NoSQL database used for data warehousing? I'm not like a guru or anything, but that seems like a bad idea? Maybe someone with more knowledge could educate me?\nConsulting bill: $250k\nIm looking at ask these services and complexity, and has the same thought. A rats nest of services that costs a fortune.\nLook at all those lovely layers of abstraction.\nI feel like the bell curve meme where the middle of this and both ends are airflow, s3, and postgres are fine\nNot at all accurate of Azure in 2024.\nVendor locked, services driven, scaleable. All price sensitive, at every stage.\nWhats the significance of Databricks being integrated in Azure? Cant DB also work in other clouds, or is it just Azure?\nThere is none, as one could even put Databricks on their own network of clusters, or one could not even use Databricks and install native spark on their own cluster network and not use a cloud provider all together. Its as many have said, vendor and vendor partners get rock solid RMR from the cloud service and consultants supporting it. That is until some CFO wants an hour by hour accounting of spend.\nI want to put together an image like this for my tech stack at my company. From what I can find, Tech doesn't even have something like this. Is there a tool or something that can do this or is it just copying logos and pasting into Paint?\nMiro is what I made ours in. It has image packages for cloud providers as well but its paid. You can also just find PNGs on google and use them for a totally free solution.\nI use draw.io . It's free and it's great. It doesn't have all the icons out of the box, but you can just add PNGs. You can save them in the desktop app and then easily reuse them for other diagrams you might need to create\nExcalidraw\n\"how many icons can we fit in this page?\" \"Yes\"\nWith gcp, I just use dataplex on cloud storage and then dbt to create incremental tables in big query with the help of a log scrapper. Composer for orchestration. At next 24 and it seems pretty common. Deutche telekom, t-mobile, uses pretty much the same stack. Cloud s ql for oltp workloads. DBT to populate.\nAnyone know where I can find more pipeline diagrams like this? These diagrams will be super helpful in explaining to non-technical people in an org how data works and why things arent as easy as doing a v-lookup.\nFYI there's no actual architecture in these diagrams. They're more like product maps specific to each cloud provider - \"AWS  sits  in the data pipeline\" - it doesn't explain what is actually happening unless you already know what  is for. The fact that the structure of the product map is the same for all the cloud providers also helps implementers familiar with one stack find the corresponding product on the other stack (e.g., \"GCP cloud storage : AWS S3\") If you want to explain to non-technical folks it's probably better to abstract out the confusing product names and just use the functionality, e.g., \"object storage\", \"event data bus\", \"data warehouse\"\nRelated to this image: I need to connect to Confluent cloud to pull messages to an S3 bucket. Cant use the connectors on Confluent Cloud. Any advice?\nhttps://www.getcensus.com/integrations/confluent-cloud but use S3 as the destination\nWhich part of the stack do seniors trust that a junior in the team can do with adequate training?\nAWS one is outdated, and even in the past it would have been questionable\nWhat would be interesting with that photo would be a the expected cost for each path of tooling used. Some of those paths are the best value in comparison to the high cost of others.\nWhy is databricks in azure?\nCan anyone provide the updated version for the azure stack?\nOk but why is the PDF dirty? Did you take a photo of your screen??\nEmail -> text message -> screenshot -> Excel -> DLQ\nDo people really use Azure ML for data transformations?\nHello guys . I want a presentation with attached axes . - When to move to the cloud? - Which provider to choose? - Which cloud solution to choose? - Which data to put on which type of cloud (Private, public, community, and hybrid)? - Make a comparison between existing open-source solutions (AWS, Azure, etc....) If someone has a presentation similar to the following axes please provide me with it.\npicasso, i like it\nIn Azure, how would you use Functions in the presentation stage? I've only used it for ingestion.\nYou can use it for super generic APIs which every team uses, for example Conversion Rates for currencies.\nI feel like Azure Synapse is missing?\nNo one really likes it as a spark host anymore, its clunky. Fabric is just repackaged Synapse, but the latest Databricks with Unity seems to be marketed towards former DBAs and it has some things reminiscent of Synapse, like the replacing of /mint with /abfss\nLooks like someone looked at a bunch of tools listed on cloud vendor websites and decided to call it \"common DE pipelines/tech stacks\". Marketing BS.\nExplain this to recruiters please\nHow would you guys orchestrate on each platform",
        "content_hash": "5f795f00863538047e283f96890e1d6b"
    },
    {
        "id": "1fi5xvf",
        "title": "Which SQL trick, method, or function do you wish you had learned earlier?",
        "description": "Title. \n\n  \nIn my case, I wish I had started to use CTEs sooner in my career, this is so helpful when going back to SQL queries from years ago!!",
        "score": 409,
        "upvotes": 409,
        "downvotes": 0,
        "tag": "Discussion",
        "num_comments": 196,
        "permalink": "/r/dataengineering/comments/1fi5xvf/which_sql_trick_method_or_function_do_you_wish/",
        "created": 1726496629.0,
        "comments": "Definitely QUALIFY keyword. So much simpler way to use window functions for filtering, no more nested queries\nWHAT? How could I didnt know about this keyword until now?\nIt's not part of the SQL standard and seems to only be implemented in expensive enterprise products so far\na few years ago i went from a company using snowflake to a company using sql server. my disappointment was immeasurable.\nLooking for a job using snowflake?\nIm in a sql server job coming from redshift. Its so painful.\nQUALIFY is definitely more flexible, but in the case where the statement just has QUALIFY win_col = 1 DISTINCT ON is often clearer and more performant. Obviously when it's not just 1 per grouping/partition, QUALIFY is your best and only option between the two.\nBeat me to it.\nSame\nYeah this works in snowflake and postgres but not MS sql where you still need to do this WITH CTE AS ( SELECT *, ROW_NUMBER() OVER (PARTITION BY some_column ORDER BY another_column) AS row_num FROM your_table ) SELECT * FROM CTE WHERE row_num = 1;\n+1 on this! I spent years doing CTEs in order to filter the window function columns, so glad QUALIFY came out\nPut the row in a struct and min/max the struct is a potentially faster way.\nRedshift didn't support QUALIFY until just a few weeks before we migrated off of it. I was actually a little sad I never got to use it on Redshift.\nThis is what I thought of too but for the past 1.5 years Ive been using T-SQL and Qualify isnt supported \nIts ok. Terrible inefficient on larger tables though.\nrow\\_number() makes cleaning data so easy.\nThis, combined with QUALIFY\nQUALIFY is awesome but not in every flavor of SQL. In PySpark or Hive you can put everything in a struct/named_struct and do a MIN/MAX on that and its much quicker. Anytime you see a ROW_NUMBER followed by where row_num = 1 is a good candidate for a MIN/MAX. Row_number is slow because it has to number each row when all you want is the min or max usually.\nCould you give an example of what this would look like?\nhttps://stackoverflow.com/a/39861036/1337644 In PySpark you can use a named_struct or a struct https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.struct.html\nthis with partition\nJust have to be careful to make sure its deterministic, or you can get some confusing results.\nHow do? Any examples?\nEXISTS can be used in very creative/efficient ways. INTERSECT/EXCEPT are also extremely useful since they do null-safe comparisons.\nWhat are null safe vs not null safe comparisons in sql?\n =\nThat's MySQL syntax. For Postgres and MS SQL Server it's a IS [NOT] DISTINCT FROM b For SQLite it's a IS [NOT] b\n>EXISTS can be used in very creative/efficient ways. Such as?\nwhere 1=1\nI just like throwing the optimizer a win every now and then.\ncan you elaborate on use cases for this?\nMany people use it to align their predicates and it also allows them to edit their predicates always knowing that any predicate of meaning is to the right of AND.\nwhere true For the pros\nWhenever I see this in production, my nose sticks so high up in the air. I agree it is useful for aligning predicates and tinkering in development, but get that garbage out of your prod code you child! \nWhat does it do\nNothing. But you can use it to deactivate a query for example in a cte that you dont want to run. Also its easier to add and remove other where clauses without missing a comma.\nOuter apply cross apply, Function as CTE SQL MACRO\ni know none of that lol\nI wish learn SQL online got into more interesting depths of the dialect than the basic shit it always is\nI must be honest, I barely use functions in my queries. Never used APPLY before but I am going to learn about it!\nI don't use cross apply often but when I do is because it was the only way to do it\nI used CROSS APPLY because I needed to split a VARCHAR field and make each splitted value into a separate row\nWhat do you use the apply functions for? Any examples? How about macros?\nSimple sample WITH FUNCTION with_function(p_id IN NUMBER) RETURN NUMBER IS BEGIN RETURN p_id; END; SELECT with_function(id) FROM t1 WHERE rownum = 1\nSay you want the top 3 salesman in every region. You can do that easily with cross apply. Without it, you need a cte and a window function\nOr just qualify\nCan you give example of top 3 salesman with cross apply?\nI'm a SQL Server guy so no QUALIFY for me. Here's the sales region query. Note that the \"join\" happens inside the where clause of the right side query. select r.RegionId, r.RegionName, s.SalesmanId, s.SalesmanName, s.TotalAmount from Regions r cross apply (select top 3 s.RegionId, s.SalesmanId, sm.SalesmanName, sum(s.SalesAmount) TotalAmount from Sales s inner join Salesman sm on s.SalesmanId = sm.SalesmanId where s.RegionId = r.RegionId and year(s.SalesDate) = 2024 group by s.RegionId, s.SalesmanId, sm.SalesmanName order by sum(s.SalesAmount) desc) s\ni used 2/3 for my internship and they helped so much!\nI recently discovered BOOLOR_AGG and its had a few niche uses in my models\nCould you elaborate what uses?\nUseful for answering the question do any X in Y have property Z?. It makes it easy to roll up Boolean properties at a finer grain up to a broader one. For example: * Do any orders have a line item thats out of stock? * Which US states have a customer on the enterprise plan? * Which batches have at least one part with a defect?\nSelect * EXCLUDE  Comes in handy during set operations without creating ctes\nBesides duckdb, who else has this? Oddly, I think GBQ reused EXCEPT here. WHY Google, WHY.\nSnowflake too\nPyspark, databricks\nGroup by all Not having to get those pesky errors of forgetting to group by 1 column etc is such a nice quality of life improvement.\nI actually have been advising against using this at my work because it enables bad SQL imo. Since it is so easy to use people just slap it on a query and are grouping by columns they maybe didn't intend to. It also masks giant group bys which are a red flag. I'd rather explicitly group by the necessary columns which is more readable.\nHow often do you really want non-aggregate columns excluded from the GROUP BY in practice? It is extremely rare for me. 99.9% of the time, I want all non-aggregate columns in the GROUP BY, which can get tedious to say the least.\nNormally I would just copy/paste the group by columns from the select to the group by columns, so doesn't really help accuracy in practice.\nWhen do you ever want to group by all columns? Wouldnt a window function then not be better?\nIt does not actually groups by all columns but groups by all columns that you would otherwise have to put in the group by clause. I.E select x,y,z , count(*) From table Group by x,y,z Just switch to group by all\nA window will produce the same aggregate value, but leave behind the otherwise repeated rows\nUnfortunately it isnt available for BigQuery \nIt is available! See [docs]( https://cloud.google.com/bigquery/docs/reference/standard-sql/query-syntax#group_by_all).\nI use \\`count\\_if\\` a ton. Postgres has \\`sum(blah) filter where (blah)\\` which is great. DE's need to know A EXCEPT B, B EXCEPT A and A INTERSECT B.\nI like watching the BigQuery release notes because they occasionally come out with updates for new, useful functionality, like GROUP BY ALL (thank you for not making me list 20 columns!!!), or finding we can now group by a struct or array. QUALIFY is definitely one of my favorites! Another is the bigquery-utils: [https://github.com/GoogleCloudPlatform/bigquery-utils](https://github.com/GoogleCloudPlatform/bigquery-utils)\nHaving to use DISTINCT or GROUP BY with a large number of columns (20?!) feels a little dirty, like a last resort solution. Like there's some problem upstream that leads to duplicates in the data.\nIf you need a group by 20 columns then its time to change the data model or you should use window functions.\nDidn't know about Bigquery Utils, amazing!\nNot a trick, but the single best thing that ever happened for my SQL writing was learning to use temporary tables for complex queries instead of trying to fit everything into a single, massive nested statement with multiple WITH statements.\nMost peoples go-to is CTEs, but mine are temp tables. In SQL Server, at least, it makes debugging way easier because you can split your logic up into discernible table parts, much harder to do with CTEs. I also find it more easily readable. My take on SQL development is that each SQL query should do one thing. So I may pull my raw base data into one temp table, iterate over string manipulation of various columns in the next, bring in any calcs in another, and so on. Temp tables also allow you to manually garbage collect memory back to the server as you go along because you can drop temp tables like they were regular tables. So if you have multiple nested stored procedures or something long running, theres a lot more performance options with temp tables (you can even build indexes on them). Again, this is with SQL Server though - your mileage may vary in other DBMS.\nNot sure about the 100x performance, but absolutely on the readability.\nIf it is working for you thats great. Just recognize that its not a silver bullet for all poor performing queries. A lot depends on the structure of the data, indexing, data scale, etc\nWeak preference for temp views over CTEs, so you can easily step through a process and see the data at each step while writing/reviewing/debugging. Both are miles ahead of subqueries though.\nAs far as the engine is concerned, nothing. It just makes it easier for humans to step through the process without having to make code edits, because you can query temp views independently, but CTEs only exist in the query they are defined for.\nLead / lag functions.\nJinja templating (not just for DBT)\nWhere else?\nI use it a lot to keep code DRY. i.e. the Do Not Repeat Yourself principle. Write SQL as Jinja templates with things like macros, for loops and render them before deployment.\nIs it possible to disagree more than 100%? Jinja is terrible IMO.\nWhy u say this?\nReadability mostly. Id rather use Python or most any language.\nI'd rather use Jinja in most cases because it's a templating language. The main language is SQL, and Jinja is just small parts of it. While I love Python, writing your queries that way becomes Python as the main language and the SQL query as the smaller part of it, making it harder to focus on the actual SQL query at the end of the day. Don't get me wrong though, you can definitely write bad Jinja, but it's usually not Jinja's fault.\nbegin; Do a thing Rollback; /* or if all goes well */ Commit; My palms are now much less sweaty when doing transactions\nSTRING\\_AGG for functions. For loops and cursors for keeping complex transformations inside the database. Can create tons of technical debt if you don't document carefully but really not any more VS pulling into python, creating a dataframe monstrosity, and writing back to the database\n, sum( case when .. 1 else 0)\nor alternatively, count_if(...) in case your database supports it\nSQL lets you order by column position, useful if you have a calculation you don't want to repeat at the bottom of the query. SELECT employeeID, ((ptoHours + sickHours) / 8) AS daysAvail FROM employees ... ORDER BY 2 DESC\nORDER BY is evaluated after SELECT, meaning you can just refer to the name daysAvail in the order by statement.\nReally? I always thought oracle didnt support the use of alias in order by so I either use the position or write a select on top the query and then use the alias . Gotta try it out.\nHonestly, I avoid any functions which are non standard because it can cause code to break when migrating. The last thing I want to do during a migration is a refactoring.\nIve never understood this sentiment. In nearly 20 years of developing/designing databases, Ive migrated to a different database product exactly twice. Every database system has its own performance quirks and considerations, so the code will likely need to be rewritten/reviewed anyways or its too small/non-critical enough to not matter. I dont really see the benefit in forcing ANSI SQL compliance because of this. Im sure there are some companies/jobs that have some need to migrate constantly, but Id assume those would be the exception. Its not worth the effort and database migrations shouldnt be looked at as being something thats lift-and-shift.\nThe most defeated SQL dev I ever met had a job that required him to write SQL as portable as possible with a short list of documented exceptions.\nI do it far more often. I am currently (right now) using 9 different SQL systems.\nWhat's the context in which that makes?\nI use 9 different sql systems so I want my queries to work across systems especially since I copy and paste my own code base often.\nNot the guy you asked but SQL Server - because the source documentation requires that and we don't want to pay for custom support. Essentially serving as a data drop. MySQL - serving a web app for some of our data cleansing Mongo - for copying data which is stored in a mongo like fashion Actian Avalanche - On cloud version of main DB Actian Vector - On prem version of main DB Clickhouse - development database we're moving some bits to Postgres - used for a different web service Moving between databases is tricky, even between versions can have issues, I've even coded up a transpilation helper to get a lot of the leg work done, but if you're using database specific functions frequently it's going to slow down migrations a lot.\nHaving just been involved in a huge migration, I feel your pain. On the other hand, it would be a shame to not use some of the amazing functions modern databases offer. Why pay so much money for a great product, only to limit yourself to the core functions? Not using the advanced/convenient functionality seems like a waste.\nI agree with the sentiment - but its not limited to migration (which is easy to disgregard). Its fairly typically I think now to have multiple database/warehouse/lake(house) technologies in an organisation. Maybe its your raw layer is in AWS Athena, your datawarehouse in redshift, a client app in postgres, being able to move your business logic from one to another without rewriting ensuring compatiability is priceless.\n+1 If everyone before me had written SQL standard code, my team would be a year ahead of where they are now with this database migration\nCOALESCE\nI wish I knew earlier how to obtain metadata on the underlying database platform that I am using. Querying syscat or sysinfo to find out what schemas, tables, columns, etc are available.\nContinuous dates and numbers SQL trick. Seem to come up in interviews a lot\nCare to elaborate on this trick that sounds amazing?\nSELECT date_seq::date FROM generate_series(CURRENT_DATE, CURRENT_DATE + '6 months'::interval, '1 day') AS date_seq ;\nI'm not a MySQL dev, but can someone please throw those guys a bone and add generate\\_series()?\nYou're using the wrong version of MySQL. Try the ones marked \"PostgreSQL\" to get the enhanced feature set.\nCTEs, Window functions & Qualify\nCROSS JOIN LATERAL Having a function pull out/calculate relevant info and add them to the column list under easy-to-use aliases. Especially like when it reduces the number of times the function has to be called in a query (DRY). https://www.postgresql.org/docs/current/queries-table-expressions.html#QUERIES-LATERAL\nCross Join Lateral is essentialy a for loop. That helped me a lot of understandig it. I use it from time to time to calculate the neighest neighbour. Like calculate for each traffic sign in traffic\\_signs what the neighrest road is in table roads.\n`MINUS` Operator: subtract table1 from table2. It is basically the opposite of `UNION` For SQL engines that don't have `UNNEST`: SELECT CASE WHEN column1 = 1 THEN table1.val1 WHEN column1 = 2 THEN table1.val2 ... END as val FROM table1 CROSS JOIN (1,2,3, ...)\nUsing a database with transactional DDL. So annoying when there's data in the prod database you weren't expecting so now your DDL migration in MySQL that worked perfectly fine on TEST now fails in PROD, but of course somewhere you weren't expecting. The down script doesn't work because the migration was only half finished and in an intermediate indeterminate state. A few monkey patches later with all the associated extra stress, and you're finally back to either where you started or crossing your fingers that the rest of the migration matches. Or you take the downtime and restore from snapshot. On the other hand if you're using MS SQL Server, Postgres, DB2, or SQLite, you may be blissfully unaware this was even a possibility in some shops. For these engines, an error just rolls back, and your database schema is never left in an undefined state. (As it should be with any sane and competent data store.)\nThis right here. I was blindly using PG for years, blissfully unaware of the full power that wrapping my multistep DDL queries in an anonymous transaction blocks had. One migration to a much more powerful distributed system later that lacks transactional DDL and wow!! Im impressed at how important this ability in PG was.\nLEAD and LAG\nUnions + window functions\nPARTITION Instead of ... oh gawd so sorry folks ... exporting to Excel and using a pivot table to see the data grouped by whichever categorical column I needed\nWindow functions. Also table functions.\nWindowing functions would have saved me so much time if Id learned about them sooner\nI use mysql mostly, so my spark jobs are also filled with sql instead of the beautiful dataframe method I have seen you people use. Sparks have much more flexibility than mysql queries, you know keyword wise, but I just find myself very comfortable using the mysql query like syntax. Using simple inner join instead of fancy \", \" On table and then putting condition in where clause. There are two things I got to know while working. 1. Using order by and then putting the column number instead of the column was a bit comfortable. 2. Using window functions like lag and lead save me from writing some embarrassing inner join which I was earlier using in my code and would have definitely taken eternity to complete.\nWindow functions in general. Back in 2019 at my first internship would have saved me A LOT of time if I just knew they existed.\nFunnily enough, CTEs like yourself. Granted I've always tried to be careful on a live database (we never had a \"data\" database till recently)... But building tables out in Big Query has allowed me to use CTEs more.\nHolding Alt+Shift and dragging your mouse up and down SQL code has been a game changer for me. Need to add commas to the end of all of these column names you pasted in? Piece of cake!\nTo level that up, end and home keys work with that too. This allows you to do tricks like: Do a multi row select, press end to move the cursor to the end of every row. Now add in a bunch of spaces. Now do a new multi row select on all that whitespace, press shift+end to highlight all the misaligned whitespace to the right of your cursor, press delete. Now you've got an aligned column padded with the correct amount of whitespace.\nPivot function is sorta hard to grasp conceptually, but so useful\ncross join lateral flatten(json_column) for flattening json arrays.\nFor row-oriented databases, I recommend everyone gain a basic understanding of how to define indexes to help optimize your queries, so that they dont have to do complete table scans.\n1. The information_schema. So much information tucked away neatly in views. It's become my first stop whenever I'm exploring a database before I speak to the Data Owners. 2. Putting 1=1 after my where clause and then appending all other filters with AND clauses. Makes debugging queries surprisingly better and it helps when you're generating SQL in python.\nEXISTS()\nThe next thing to learn: Stop using CTEs. CTEs are ***awesome*** for very specific uses, but easy to abuse. Most of my in-company reputation for speeding up inefficient queries, functions, and procedures is due to replacing CTEs with inserting into a table variable or temp table. Pretty common to say \"This report took over 3 minutes, sometimes wouldn't ever finish. Got it down to 3 seconds by replacing all of the CTEs with \"INSERT INTO \\\\@TableVar()....(SELECT \\[old CTE query\\])...SELECT \\* FROM \\\\@TableVar\" They are like Python, faster and syntactically cleaner to write, but slow memory-hogs to execute. If you use too many or they handle too much data, you chew up your RAM and then it runs like molasses, or locks up entirely, and either way you're just better off using other methods if you can.\nCTEs are zero cost in many databases systems these days.\nNot in Azure SQL Server though, which is what my org uses and 36% of the database market.\nWITH cte_name AS MATERIALIZED (  ) Now you don't need to create and populate a temp table. WITH cte_name AS NOT MATERIALIZED (  ) Merged into the parent query like a subselect. Different cases need different strategies. Luckily CTEs are flexible like that. More so than subselects.\nShould have specified we use Azure SQL Server, which doesn't have that option :(\nAh, yes. We are all both enabled and hobbled by our choice of engine. There are no perfect choices; only acceptable tradeoffs.\nLikewise in many more modern databases I've seen the opposite to be true, simply due to better query compilers that can be leveraged when you run it as one big query. If steps are instead loaded into tables, there's no room for the query compiler to merge steps together more efficiently.\nBeat me to it\nbroadcast joins for those streaming data enthusiasts:D\nMERGE in Trino. Allows you to update/add/delete records in a table without multiple queries/statements\nTable value constructor's are very useful. Typing across multiple lines at once. Need to select from 10 tables? Only gotta write the statement once. sp_help or equivalent hot keyed More difficult but understanding the nuances of and, or, exist, join, apply.\n!RemindMe 1 day\n`PARTITION BY`. Being able to effectively split one large table into multiple smaller ones (each with its own index, typically grouped by some kind of time range (per month)), while being able to query it like it's a single table is pretty nice. Kind of annoying to manually create a new partition before inserting (I wish this was automated), but the ability to just remove a month of data by dropping a partition is niiiiice.\n*scratches neck* yall got any of them mysql 5.6 tricks?\nCUBE\nWhile practicing SQL questions, I came across solutions using group by 1,2,3 etc. This is so much better than painstakingly adding all the column names to the group by statement\nReally useful for grouping stuff in time series, or event based data: Conditional_change_event Conditonal_true_event Wizardry, not used that often, but when needed it's super useful: Match_Recognize Other handy things: MAX_BY, MIN_BY e.t.c are nice. LISTAGG() IFNULL() EQUAL_NULL() LIKE ANY WIDTH_BUCKET Also in Snowflake, RESULT_SCAN is handy to access recent results and further query them.\nDatabricks: USING as an alternative to ON for join criteria. I learned this last year, the code is simpler to read when I have to use more than 3 columns. LEFT ANTI JOIN to eliminate unwanted records. GROUP BY ALL as opposed to typing columns. read_files() function to stream tables.\nJust learning to use window functions in general, new me after that moment. I would pick the [ratio\\_to\\_report](https://docs.aws.amazon.com/pt_br/redshift/latest/dg/r_WF_RATIO_TO_REPORT.html) one that I learned about in Redshift, if I had to choose a special one to my heart.\nWhat is the order of operations when using QUALIFY?\nUpvote for CTEs, but \\`distinct on\\` (Postgres) was also a win.\nSelect * except (col1, col2, etc.). I like to use this when sourcing from previous CTEs.\nLooks like EXCEPT is BigQuery specific \nRow_number() over (partition by column) To help identify complex statements where joins have duplicated records\nWe are using Json lot, for me JSON AGG and also partition by\nHash tables for storing temporary data so I don't have to write those big queries and run everything.\nRemind me! 5 days\nNot a SQL trick rather a dev tool for parsing and tracking lineages like sqlglot, sqllineage. Lineages make troubleshooting a piece of cake!!\nUNION ALL removes duplicate rows. Learned it the hard way\nU mean UNION ALL keeps duplicate rows.\ncte\nHAVING keyword to filter on aggregate methods\nCTEs are great! Another tool that's been a game-changer for me is [AI2sql](https://ai2sql.io/). It generates SQL queries from natural language descriptions, which can be a huge time-saver for complex queries or unfamiliar databases. While it's not a replacement for SQL knowledge, it's really helpful for quickly drafting queries and exploring different approaches. Might be worth checking out!\nWindow functions. I still dont know them super well, but I wish I had known they existed and probably would have been able to use them a ton in a previous role that was SQL heavy.\nmax_by and min_by\nMERGE command\nProbably QUALIFY and LISTAGG\nCTEs and Window Functions and how to effectively use them together.. Also how to teach SQL using the magic of TVC i.e Select from VALUES...to avoid physical table construction...\nCOALESCE. It can be used so many different ways and can be a life saver when working with inconsistent source data.\nPipe syntax. I found this 'work changing', to the extent that we rewrote our legacy code in pipes (only several dozen scripts, so we didn't try to use an AI). I am not sure which vendors/engines have implemented something similar. [https://research.google/pubs/sql-has-problems-we-can-fix-them-pipe-syntax-in-sql/](https://research.google/pubs/sql-has-problems-we-can-fix-them-pipe-syntax-in-sql/)\nCounterpoint. My company is switching back from this.\nCan you explain why?\nJust super annoying to maintain and anti-intuitive. Most of the problems they show above are sub queries, and should be ctes\nGotcha, thanks for answering!\nHow do you practically use this? for example my org is on Snowflake, it's not like I can install a package for pipes\nWow, I saw something about this on twitter and thought it was fake news or an April fools joke, I love pipes in R. What databases use pipe syntax?\nNone. Like prefix notation and Dvorak keyboards it has its tiny rabid fan base and everyone else hates it. Beyond small constructs it's much harder to read. You need to keep a mental stack of the entire syntax tree in your head. It's like having to use subqueries for all joins with no Cates allowed, only worse.\nMaybe I've just been doing standard SQL for too long, but pipe syntax is just so weird to read. The engineers at my current company love it, but that's because they're elixir devs and the syntax is similar(maybe the same?) to how ecto queries are structured.",
        "content_hash": "2c5463f81dc02ccdee4c58fd92c26045"
    },
    {
        "id": "1d9w4c9",
        "title": "Spark Distributed Write Patterns",
        "description": "",
        "score": 404,
        "upvotes": 404,
        "downvotes": 0,
        "tag": "Discussion",
        "num_comments": 50,
        "permalink": "/r/dataengineering/comments/1d9w4c9/spark_distributed_write_patterns/",
        "created": 1717715715.0,
        "comments": "Sharing here a diagram I've worked on to illustrate some of Spark's distributed write patterns. The idea is to show how some operations might have unexpected or undesired effects on pipeline parallelism. The scenario assumes two worker nodes.  .: The level of parallelism of read (scan) operations is determined by the sources number of partitions, and the write step is generally evenly distributed across the workers. The number of written files is a result of the distribution of write operations between worker nodes.  ..(): Similar to the above, but now the write operation will also maintain parallelism based on the number of write partitions. The number of written files is a result of the number of partitions and the distribution of write operations between worker nodes.  ..().(): Adding a () function is a common task to avoid multiple small files problems, condensing them all into fewer larger files. The number of written files is a result of the coalesce parameter. A drastic coalesce (e.g. ()), however, will also result in computation taking place on fewer nodes than expected.  ..().(): As opposed to (), which can only maintain or reduce the amount of partitions in the source DataFrame, () can reduce, maintain, or increase the original number. It will, therefore, retain parallelism in the read operation with the cost of a shuffle (exchange) step that will happen between the workers before writing. I've originally shared this content on LinkedIn - bringing it here to this sub.\nIs there a guide on when to use each of these for those new to spark?\nYou `partitionBy` if you specifically want to produce output with Hive style partitioning so that later queries that filter on the partition column can skip reading files in those partitions. If you're using the new open table formats (delta, iceberg) you might not bother with this, in favour of their own clustering methods instead. Doing a `.coalesce(1)` is for when you know you have a very low data volume, and you want to minimise the number of files produced. Instead of 10 files each with 1 row, you can get 1 file with 10 rows. It is usually to push spark away from its default mass parallelism, which spark defaults into because its whole purpose is for distributed processing of large data volumes. You can coalesce with higher values if needed, for example if a shuffle step is producing 200 partitions, you can fold that down to 10 or so. It depends on your expected data volume. A `.repartition(x)` works similarly to a `.coalesce(x)` except that it will actually reshuffle the data. If you don't give it key columns to shuffle on, it will essentially be random to produce roughly equally sized partitions. If you give it key columns to use it will effectively be a bucketing shuffle, where same values in the key columns end up in the same partitions. `.coalesce(x)` doesn't do a 'shuffle' - it combines existing partitions together. This is faster, since portions of the data don't move and there's no shuffle calculation, but it doesn't balance partitions either. This manual shuffling is also somewhat superseded by the new open table formats. You can just write to such a table at default parallelism, and then run an optimise/compact on it to combine multiple small files together. There are some niche uses for repartitioning on write, if you're pushing to something like a document store. You may have previously read or joined data based on a key value, which means the spark partitions match the document store partitions, which results in 'hot' partitions on write. A `.repartition()` will randomise that data again, so that it is equally spread across partitions in the target. You don't usually connect these systems like this, so... niche.\nThank you!\nNot sure if there is a guide, actually. I am enrolled on Zach Wilson's data engineering bootcamp (dataexpert.io) and learned a lot there. If you know where to look at the Spark UI and understand your task DAGs there, you can learn a lot, actually.\nHows the program?\nIts great! Very intense and more advanced than I expected. Definitely worth it if you are already working and looking for a more senior role in your company or outside\nCould you share which tools that you're using to draw this diagram? This diagram is informative and intuitive. I've seen such diagrams on Linkedin often, have no idea where it can be produced.\nI use [draw.io](http://draw.io) for all my diagrams, and the animation is a result of the 'animated flow' flag that you can check there on your arrows. To produce a gif I just screen record and convert with ezgif\nThank you so much! I'm also a [draw.io](http://draw.io) user, but your diagrams are much better. They look professional and intuitive. Finally, I know where to level up my drawing skills!\nWhat's one myth would you like to debunk about any of these?\nNot actually looking at any myth to debunk, to be honest. I was mostly curious about how repartition and coalesce affect parallelism and compute, as one involves a shuffle (that exchange you see in the image) step and the other doesn't. Both are used to optimize storage and IO via file compaction, and that's how I use them.\nWhich strategy do you use most often? Repartition or Coalesce? If data is skewed, are you using repartition?\nrepartition + sortWithinPartitions is great to optimize storage and leverage parquet run-length encoding compression. You probably don't need anything else.. For skewness there are two configs you can use to delegate the partition strategy to spark and optimize data distribution between partitions; `spark.sql.adaptive.enabled` and `spark.sql.adaptive.coalescePartitions.enabled` Just bear in mind, though, that you can negatively impact partitioning pretty badly by using those if you don't know your data (skewness) well. Here's more from the docs if you want to read on those; [https://spark.apache.org/docs/latest/sql-performance-tuning.html#coalescing-post-shuffle-partitions](https://spark.apache.org/docs/latest/sql-performance-tuning.html#coalescing-post-shuffle-partitions)\nIf the spark tasks show that a step is heavily skewed, it can be useful to run a `.repartition()` right before it. Sometimes you might filter on something that is correlated with a join key, and that creates skewed partitions. It may be faster to shuffle the data and process equally sized chunks, than have one partition take so much longer to process. If you do this, it is good to aim for some multiple of the number of executors you have. For example if you have 32 executors, repartition to 64/128/192. This will mean that each executor will get roughly equal portions of data, and if there's any residual skew it will be mitigated by the smaller partition sizing. Coalesce doesn't do randomised shuffling like this, it just combines partitions together, so it doesn't necessarily fix skew.\nThat coalesce(1) is more performant than repartition(1). > However, if youre doing a drastic coalesce, e.g. to numPartitions = 1, this may result in your computation taking place on fewer nodes than you like (e.g. one node in the case of numPartitions = 1). To avoid this, you can call repartition(). This will add a shuffle step, but means the current upstream partitions will be executed in parallel (per whatever the current partitioning is). https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.coalesce.html\nWhat tool did you use to make the diagram\nAlready been answered in the thread by op. Look for it\nThanks, I found it. FYI, it was not posted when I made my comment. It was posted after.\nBy animating, it removes the ability to zoom in for me (iphone+reddit app), so I cant tell what any of it says. It looks like its just animating the arrows?\nA question regarding the drastic coalesce(1), does it cause a shuffle? I've read that coalesce is repartition(shuffle=False) or something like that. But let's say I have my data being processed by 5 executors and in order to write a single output, I'm expecting it all to be collected (data shuffle) in one executor before it gets written to disk. Some clarity here would be super helpful.\nThis is great, very intuitively shows what is happening which may not make immediate sense if you just read the documentation\nWhy does repartition still only use a single writer?\nBcs 1 is used as parameter for repartition\nHey . Thanks for this . Ive heard a lot about his course and Im planning to take it once I land a job . Good luck with the rest of the course\nFantastic!\nI'd also like to add another one - df.repartition(\"DATE\").write.partitionBy(\"DATE\"). This will get you to one file per partition as in examples 3 and 4, but will write in parallel from the workers instead of all from a single one. Does require a shuffle of data prior to the writing though, so depends on where your bottlenecks are as to which approach to use.\nNo it does guarantee to get you one file per partition.\nDont do that try using rebalance before you write or repartition by a generated key to control file size.\nThis is awesome!!\nThis is really great!\nI really like this! It's super clear and informative. Are you planning on making more infographics like this?\nYes I am, I actually already shared more on my LinkedIn - will post them here eventually too\nThis is nice! May I know which tool you are using to create diagrams? Looks neat.\nIm using draw.io for all diagrams\nremove the animation please, makes it impossible for some people to read it.\nYou reading the lines?\nIt's a diagram, so yes?\nNot sure why I'm being downvoted. Unnecessary animation makes it difficult to focus, especially for people with ADHD. It's also bad practice for data visualisation. Animation draws attention towards it and in this case the animation does not add anything to the information being conveyed. Except maybe direction of data flow, but arrows provide that already.\nI totally get your point, and I'm sorry the animations made it worse for you. I've been using diagrams for quite a long time and have found that a couple of things work great when you do them right; - Knowing where and when to give emphasis; - Knowing how to give emphasis with accent colors that make sense; - Knowing where you're vehiculating your diagram and making the right use of canvas and font sizes. I've rarely made use of animations and just recently started applying them more, and I must say, they do make a difference on how quickly you can communicate directional information. They also add another dimension for you to use (you can indicate flow with moving arrows and flow without moving arrows, and give them different meaning with a legend indicating that). Hope that makes sense\nIt's such an obnoxious trend. I can't read this. I can force my eyes to look at the words, but I don't absorb anything. It's wild to me this doesn't bother other people. It was stuff like this though that tipped me off that I might have ADHD. Alas, if only there was some other way to convey the direction of a line.\nPause it? Screen shot it?\nYou know you can just print screen it, right?\nI can do a lot of things with a computer, doesn't mean it's convenient.\nDf.repartition(1).write. Or Df.write.repartition(1). Which one would you prefer and why?\nisn't both the same, afaik physical plan for both of them would be identical",
        "content_hash": "82be07a94d850b88dc349e097afa6266"
    },
    {
        "id": "whz7pw",
        "title": "Anyone read this book? It came out in 2022 so it's very modern and up to date.",
        "description": "",
        "score": 394,
        "upvotes": 394,
        "downvotes": 0,
        "tag": "Discussion",
        "num_comments": 76,
        "permalink": "/r/dataengineering/comments/whz7pw/anyone_read_this_book_it_came_out_in_2022_so_its/",
        "created": 1659822603.0,
        "comments": "Yes, i have read it through Oreilly. Decent one. Very high level but most of the modern DE concepts are covered.\nCan you please be more detailed on this: High level (while still informative) or too high level for someone that's working in DE? Its on my list, but the list is huge so i have to prioritize :D\nCan you share your top rated books you have on your list right now? Both read and want to (say next 5?)\nI ll need to sort that list first, it's just an excel sheet with courses / videos / books that I found interesting and added them. Once I sort them (preparation for my holidays reading) i ll PM you. It is both Data Science and Data engineering, I guess you are interested in the DE only ones?\nDon't PM, we are all curious!\nActually interested in both, ofc I came here for the DE ones, but keen for either, so just share whatever you see fit. Thanks a lot!\nDon't PM, interested too.\nList is huge lol when is it ever not. Seems like a never ending tdl.\nYeah I love O'Reilly books I've read many of them in my day.\nAny recommendations for more low-level intro books about DE (could be also O'Reilly)? One or two titles would be great. Starting as junior DE in mid-September, have Fundaments of DE on my reading shortlist too :). Thanks!\nIt's not low-level or even DE specific. But (another O'Reilly) 'Creating a data driven organisation' by Carl Anderson was interesting - not technical but gives you an idea of wider business need and some challenges you can expect to face. Best of luck with the new role!\nThanks so much!:)\nI would suggest breaking up the subjects of interest and read the best books amongst those. I would suggest SPARK- THE DEFINITIVE GUIDE Fluent Python SQL AIRFLOW- official docs or Astronomer blog Aws-solution architect guide Sa02 This is worth 6 months of material \nHi! Im one of the authors of this book. Happy to see our new book getting attention, and glad to answer questions here.\nWhat audience is the book written for?\nObviously current data engineers, as we feel this book comprehensively answers various questions we often get from data engineers (usually along the lines of what should I know?). The book is also oriented toward the non-DE audience who either want to learn more about the DE field, or transition into it.\nI listened to your talk on the broadcast last weekend! This is like meeting celebrity in real life haha\nHa, which broadcast? Feels like I did several last weekend. And thanks!\n>Ha, which broadcast? Feels like I did several last weekend. And thanks! I heard it from the \"Data Engineering Podcast\" on spotify! Great discussion!\nNice. Did you check out the episode where I interviewed Tobias about his podcast?\nDefinitely worth the read. Super knowledgeable authors from Ternary Data. The way the explained it on the podcasts that interviewed them on it was that they wanted to, focus less on specific tools and focus more on the concepts so it wouldnt become irrelevant in the next two years. Solid 9/10, recommend.\nTool-specific books are obviously useful and have their place, but it's certainly nice to have books about general principles anchoring a field or discipline that are targeted at being more timeless. Can buy a physical copy with the intent of serving as a long-term personal library reference without needing to worry about rapid obsolescence with new editions. Definitely a fan of books such as *The Art of Programming* series by Donald Knuth and *The C Programming Language* by Brian Kernighan and Dennis Ritchie that remain applicable.\nIm reading it. Heard about it in the Data Engineering Podcast. https://open.spotify.com/episode/3DLXk5fhetrzfYPyBomV1p?si=D2_JkJtLSN6wVM9-d5_3xQ&utm_source=copy-link Then I got to know that the authors have a podcast their own. https://open.spotify.com/episode/4knNR4ebcxkue4IGxnfIP6?si=UVN75xC5Reu09-F1kkaViQ&utm_source=copy-link\nFYI here is the link to the book to check out the table of contents. https://www.oreilly.com/library/view/fundamentals-of-data/9781098108298/\nI am now, thanks for the pointer.\nI will read it eventually - I like Joe's material and listening to him on podcasts - although I don't have an O'Reilly sub and the retail price in Australia is nearly 150 bucks, which I just can't afford right now.\nIts in libgen.is if you want to check it out for free\nBTW if you can find a school email you can probably read it for free.\nHow does this work ? I still have access to my email from my graduate program\nFaster to just check libgenesis\nTry this link and enter your school email account: https://www.oreilly.com/library-access/ Definitely works for me (studied at a Canadian one not even listed)\nMy University (Syracuse) is also not listed but unfortunately I put my email in and it gave me the error : This email is not associated with an academic account. Even though it has a .edu domain. Oh well.\nI'm an experienced data engineer myself but purchased it to read through since it's being spammed on LinkedIn the last few months. Haven't started it yet but seems like the concepts covered should be mandatory knowledge for mid level DE's and it would be great knowledge for technical and non-technical BI and data leaders to understand as well.\nThis might be an stupid question , but when reading books like this, do you do it digitally or do you buy the physical book ? I've always have troubles reading books as I struggle staying focused. I'm really interested in this one and I wanna absorb it the best way possible.\nI used to buy them physically but they end up taking up alot of space, get worn out, even lost etc. Now that I have a decent tablet I usually buy them digitally through Amazon or rent them through their kindle service.\nI bought a tablet specifically to read books like this. I have nothing on there except a note taking app / pdf reader\nI dont know this one, but I just bought another new (2022) OReilly book: Essential Math for Data Science. Hasnt arrived yet, but if its good, I might look at this one. Thanks!\nHow does this compares to Designing Data-Intensive Applications? Like is it a complementary book or something different?\nAuthor here (Im never on Reddit, so new here). DDIA is very complementary to Fundamentals of Data Engineering. I consider FODE as a prequel to DDIA. Sidenote - Martin Kleppman was one of our tech reviewers for FODE Enjoy!\nAuthor here (Im never on Reddit, so new here). DDIA is very complementary to Fundamentals of Data Engineering. I consider FODE as a prequel to DDIA. Sidenote - Martin Kleppman (author of DDIA) was one of our tech reviewers for FODE Enjoy!\nDDIA is a very technical book. This one is high level theory.\nI literally just bought this book a couple of days ago. I have only read about half of it, but so far I have already learned a lot.\nWould this be good for someone trying to get into DE?\nI think so, yes. They presume a certain level of knowledge around programming in general, and around SQL, etc. but the approach is very fundamental. They focus on the DE lifecycle as a whole, and not on any individual product like Airflow or whatever. Of course that tends to be the case with all O'Reilly books that I have read. Depending on your cloud choice, https://www.amazon.com/dp/1492079391?psc=1&ref=ppx_yo2ov_dt_b_product_details this is also an excellent book. It focuses on AWS, and goes farther into ML and AI than you probably want to as just a DE, but to get to the point of ML and AI you have to learn all the DE stuff too, so it covers that excellently.\nAny similar recommendation for Azure?\nhttps://www.oreilly.com/library/view/data-science-in/9781491917176/ That's the closest I found, although I have not actually read this one, so I don't know how good it is.\nAny similar recommendation for Azure?\nI just bought the kimbal toolkit 3rd and DDIA. I found more insight starting from chapter 3 of the toolkit.\nFinished the book last week. Overall, it's a decent book to get you exposed to the modern data engineering culture.\nNah, but based on the table of contents it looks like a good intro/reference. I bet it's meant as an intro to someone who has no idea what DE is. Like swe interested in DE would get a lot from this. May be good for beginners but may be a bit hard the first read through. I also feel like a lot of these topics I learned from cloud computing so theres that too.\nI think it caters to existing Data Engineers, Data Architects and Managers. Data engineering is completely wide and varied in scope and the book tries to define it. It won't necessarily make you a better DE, but it allows you to see the bigger picture as they explore their definition of the Data Engineering Lifecycle. A DE will only deal with a subset of what they are exploring in a given job.\nI read through the parts on data architecture and data modeling. Very accessible, mid-level stuff that covers  surprise  fundamental concepts, which can then be applied to specific technologies/tools by the reader.\nIt's on my list for September holidays. If not fully read, skim through to see what it is about.\nThere's a discord for this book, Joe Reis (one of the authors) has joined: https://discord.gg/58zjHuG8 Updated link: https://discord.gg/YVA5mgETVF\nThat link is expired according to Discord.\nAh updated, sorry\nAnyone has the pdf download link?\nIm about to read it soon actually.\nHeard about it on a DE podcast and am on Chapter 2\nWill anyone who read this book, recommend this for intermediates/new in data domain?\nI haven't read it yet\nI am now - data engineering is interesting but I don't have the skills (yet).\nVery modern and uptodate is probably the reason.\nIve followed both Matt and Joe on LinkedIn for a while leading up to their books availability, so happy for its release and I plan on checking this out!\nOne of the best books on DE I have read so far. Covers the #datalife and def worth a space on the shelf next to those dataware house bibles.",
        "content_hash": "ab5d300c94ed1bd9f366f9e714d9487e"
    },
    {
        "id": "lio4nh",
        "title": "How I feel in the coding interview when I get asked about BSTs and I damn well know all I\u2019m going to do is call apis, parse json, and copy to Redshift.",
        "description": "",
        "score": 393,
        "upvotes": 393,
        "downvotes": 0,
        "tag": null,
        "num_comments": 20,
        "permalink": "/r/dataengineering/comments/lio4nh/how_i_feel_in_the_coding_interview_when_i_get/",
        "created": 1613173730.0,
        "comments": "I google every day LOL. IDK how they expect you to have everything memorized either.\nIt's pretty stupid that they do this. They are likely passing up on people who can do the job even better, who don't care about relearning unnecessary information that is easily searchable. It's like they embrace the attitude of \"work harder, not smarter\" instead of the other way around.\nAgreed. When ive been on the interviewing side, we ask for them to solve the problem and explain their thought processes / decisions. I honestly couldnt care less if you forget the name of a method on an object if you explain I need to do x because if I dont, y and z happen. Yeah it introduces a lot more gray area, but our team has seen success in hiring people who are good at critical thinking / problem solving and arent afraid to ask for help / actively engage in pair programming.\nNot sure if this sort of comment is allowed here, but...are you guys hiring now?\nNot sure if its allowed or not but unfortunately we arent at least for the first half of 2021!\nRight?? Tech changes so rapidly it doesn't matter what you learned once. Can you learn the skills required for tomorrow?\nHow do you prove that in an interview? Maybe by demonstrating something that you already learned? Such as ... BSTs...?\nI usually go off of behavioral. I'd you have the education and a track record, it matter more to me your personality. It you won't get along with me or my team, I don't care how smart (or maybe sarcastic?) You are.\nToo many tech interviews are a game of Jeopardy. Like what is the third parameter the SQL Server backup command? Easy for devs to ask but completely useless. Can you think and reason about problems? Are you able to reflect and grow from your mistakes? Do you hold a high level theory for how things work in DE?\nyeah but it's easier to ask what a BST is than testing the broad concept of solving problems :p\nThis is why I never use trivia, require memorization, or do algorithms in interviews. I also pull interview prompts from problems that we have faced in the past or are facing now and haven't got around to fixing yet. Fake trivia problems don't give useful signal.\nthis is pretty much my current job. they grilled me hard on stuff and 90% of what i do is basic SQL more or less.\nHaha!",
        "content_hash": "0f22dd624736006cefce22a14a2d02aa"
    },
    {
        "id": "y2bl65",
        "title": "What\u2019s your process for deploying a data pipeline from a notebook, running it, and managing it in production?",
        "description": "",
        "score": 394,
        "upvotes": 394,
        "downvotes": 0,
        "tag": "Discussion",
        "num_comments": 207,
        "permalink": "/r/dataengineering/comments/y2bl65/whats_your_process_for_deploying_a_data_pipeline/",
        "created": 1665599540.0,
        "comments": "no notebooks in prod. DEs will rewrite and optimize whatever logic is needed by the DS.\n\"Throw the notebook over the fence to eng\" is the common phrase I hear. Sad life. &#x200B; What are you using to run and schedule that logic?\nSame stuff Airbnb created. Airflow. And whatever else is needed sql scripts, emr scripts, hive scripts, etc. of course making sure the pipelines fall in line with our pipeline specs, using our common de components and templates.\nNice, thanks for sharing.\nTHanks for answering here, Iron Just curious - why did you leave Airbnb?\nI cannot emphasize enough how important this approach is!! My team's policy is as follows: notebooks are for prototyping ONLY, refactored and well structured scripts are then deployed to prod.\nlike py scripts w/ spark serverless that pull from bigquery with sql=\"\"\"...\"\"\" in Intellij then run them in dataproc serverless via Airflow?\nDo you re write everytime when DS makes any changes to the notebook ?\nsame here. Not gonna happen\nhaha\nNetflix released a pretty good (very high level) blog about this a few years ago. https://netflixtechblog.com/notebook-innovation-591ee3221233 There is even a part 2 about scheduling. And several talks about notebooks at the bottom of the post. Their basic strategy is to just move the notebook onto a prod server, then use a bunch of other tools to manage them in a more \"production\" way. Honestly it seems like a ton of overhead to me unless this is the primary way that people write code at your org. Which it sounds like was the case at Netflix. But you need buy in from an entire team of people just to get all the tools running to support this type of deployment process with all the bells and whistles.\nMy first thought was also the Netflix model, as they were the first one that I had ever heard doing something like this. Personally I just \"productionalize\" the code.\nWhat is your process for \"productionalize\"-ing the code?\nGet everything in a single block. Then apply DRY principles and create functions. Then check the prod code to see if existing code already exists that can do roughly 80% of what I am trying to do. Modify as needed. Then create a py file.\nI do agree putting it in a block with DRY principles and functions so it can be tested and reused. Although I typically split it logically into multiple blocks if necessary.\nThis takes so much less time in the end than trying to figure out what is going on with a buggy notebook in production.\nThats the neat part, you dont\nBuild, test, validate and orchestrate pipelines with scripts. Doing all this with a notebook is really difficult and frustrating. Use notebooks only for interactive development, the right tool for the right job\nDo you write scripts to orchestrate other scripts or do you use a tool like Airflow or Mage to run your data pipelines?\nThe low budget/effort solution would be to schedule a cron job to execute a script or a set of scripts. It lacks a lot of features like backfilling, governance, task lineage, etc but it gets the job done. In most corporations I see people using airflow or any other in-house tool for orchestration that grants all those features i mentioned above. I the company I work for were using AWS Step-Functions since were building out our data mesh, data lake and data pipelines with AWS components.\nNice, thanks for sharing.\nAirflow\nWhy did you choose Airflow or was that already setup years ago at your company?\nNice! Thanks for sharing.\nIve seen things ranging from extracting code snippets out of a notebook and putting them in scripts, all the way to offloading it to engineering to productionize it (we havent even talked about code reviewing notebooks yet another nightmare).\nData scientist here. For many years I've written a .py file that imports a notebook and calls specific functions from within that notebook. The py file looks similar to a header file in C/C++. The reason I do this is because whenever some engineer wants to productionize my notebooks they sometimes add bugs to the code, despite it being a simple copy paste job. Worse yet, even if they somehow don't add bugs to code there could be a difference in the way the data comes in to how I'm getting it from the DB that can be causing issues. This creates major headaches because if the model isn't working correctly in production it could be the code is messed up, it could be pipeline issues, or it could be that new data coming in is different than old data. I'd rather have what I write work from the get go. No fighting with engineers all day trying to fix bugs. This way it works and if it doesn't it's very easy to diagnose what the exact issue is.\nNice, thanks for sharing. Do you still take this approach at your current company?\nThis approach sounds somewhat similar to the [nb-dev](https://nbdev.fast.ai/) workflow, where you specify markdown/hooks for production.\nThe fact that when DE productionize code ended up adding bugs is not uncommon but I think it speaks to the lack of proper habit of software testing in the data science community. The code may not even be bug free in the first place. But the love for tests tend to come later with experience and headaches.\nDS work is statistics based, not pass or fail binary based. So unit tests do not work. A model is a kind of test though so DS' work on tests more than software engineers do, in a foreign sort of way: It's not \"does the software work or not work\", but what is the software's accuracy and precision and if those metrics get worse, why?\nI disagree. Statistics need to be calculated and verified in a small setting, which unit test can catch and you also need broader monitoring tests. For example, when you calculate statistics that are bound by dates, how do you prevent the person who is maintaining your code or iterating on top of it to make an off-by-one error by accidentally changing <= to <? This is likely a specification that can be pinned down by a simple unit test. But yes, it is not the whole scope of testing data science projects. This is why packages like great expectations exist. But way too often I see data science project teams spend too much time chasing metrics and too little time writing simple tests to ensure maintainability in production, which leads to unexpected degradation and headaches when deployed. I have seen an ML model that didn't have a test/assertion to catch a possible NA from an API input that, when in production, the input API starts pumping out NAs, model degraded cost the company tens of millions of dollar. I think the total damage when tallied was almost $100M. It took the data science team 2 weeks to find out \"oh shit it's the NA\". Some things can't not be skipped.\nJokes aside the way I do it is I use a notebook to do dev work. As soon as I figure out \"steps\" , \"tasks\" I start defining them as functions or methods in a .py file and importing them in the notebook, continuing. By the time I'm done I have actual modular code. And then in Databricks specifically where notebooks are first class citizens for scheduling and pipelines I use repos and run the notebooks but all they contain are ``` import mymodule myobject = mymodule.createInstance() myobject.doTheThing() ``` And adding parameters appropriately. The neat thing is that I can add charts and basic print statements that displays in the notebook that I can then go open and look at if there's an issue. That you can go see on a per run basis.\nThat is the right solution. Thanks for sharing! Thats the approach Mage takes as well when it comes to building data pipelines.\nITT so much horror about productionizing a notebook and yet, Databricks has this out-of-the-box as a first class citizen, Netflix built a production system that does this too and AWS Glue now has notebook support that make simple jobs incredibly easy and it's just getting started. Personally I think Databricks is extremely good and it keeps getting better every quarter. What the people here are saying \"no\" to is an anti-pattern that can easily arise around code duplication and difficulty testing and that is a caution sign that everyone should take very seriously. At the same time, you can make the same mistake with shitty regular Python code. Look, I get it, it can get out of hand if you let it, just like everything else, but you can also build something easy to use that's extremely powerful if you put your mind to it. Notebooks are here to stay because of how approachable they are. I personally love them for exploration work, some prototyping and with _most of your reusable code in an external library_ they are also perfectly OK for production data pipelines. :shocked-pikachu: It's just Python code folks and with some discipline, light guard rails and smart team of engineers you can have something reliable.\nThose are all great options you shared, agree with the sentiment. &#x200B; Which process/approach/tool/etc are you utilizing at work?\nWe are just wrapping up our most recent look at how we get stuff in notebooks 'production'-ized. Our answer was not to actually run a notebook in production. Caveat: we haven't worked out all of the kinks or gotten this running at 'enterprise' scale yet. Our definition of 'production' is to support specific web applications or products users are accessing. Primarily non-real-time datasets. For us, notebooks just aren't great IDEs for production-grade anything. We looked into stuff like Papermill that 'production-izes' notebooks but there's some jankyness if you're looking for anything other than a static output. Our workflow is to write most of our core algo in dedicated modules we import into a notebook for testing, exploration, and experimentation. Separating the code driving the algo and the code and outputs used for benchtop science seemed kinda like a no brainer. We use the lab interface for Jupiter and integrate git into it to version control the module code. That repo also the repo our DE and dev teams use to build containers and deploy to stuff like Argo Workflows.\n>write most of our core algo in dedicated modules we import into a notebook for testing, exploration, and experimentation. ... We use the lab interface for Jupiter and integrate git into it to version control the module code. This is the way. Create a \"pure\" Python (i.e. non-notebook) code base to handle things like authenticating to data stores and cloud providers, helper functions for reading/writing data and tables, etc. Otherwise you end up re-inventing the wheel with each notebook. The notebooks should only focus on the algorithm, and if that gets too unwieldy consider factoring the code into functions or classes, and then moving those into the code base. I wouldn't mess around with deploying notebooks directly into production. If you REALLY want to do that, use a vendor like Databricks.\nAgree with the pure python approach. What are you using to orchestrate those python scripts?\nUsually Airflow. For less intensive tasks, another common option is deploying the code base as a Flask app. Then, use either KubeCron or APScheduler. There are other options, but we used Databricks and they have amazing task orchestrations for notebooks. We basically stopped using Airflow for anything that lives in Databricks.\nAre you orchestrating notebooks in Databricks? What data pipelines do you NOT have in Databricks that you use Airflow for?\nNot OP. I tend to use prefect or flyte. Jenkins, if needed (I'm a long-term consultant and work with what the client likes)\nNot Airflow?\n[removed]\nNot right now, Argo Workflows is handling business logic, predictions, and MLOps pipelines while the rest of the more traditional ETL stuff is handled in the relational warehouse for that application.\nAre you saying that Snowflake or BigQuery or Redshift or Databricks is running your ETL workflows?\nWe build different types of applications but have metadata schema and some pre built procs that work for most relational systems. The core ETL orchestration and workflows are set up and managed using that framework, but they can pull from other systems like S3, BQ, etc.\nWe use Dagster with the Dagstermill integration: [https://docs.dagster.io/integrations/dagstermill](https://docs.dagster.io/integrations/dagstermill) There is an integration with HEX \\[ [https://hex.tech/integrations/dagster](https://hex.tech/integrations/dagster) \\] although we are not using that as of right now.\nWe use Databricks notebooks in production. Scheduled and orchestrated by Azure Data Factory. Works well for us. We are just a couple of data engineers. The notebooks do however resemble your normal scripts with functions being imported from another notebook and running in a \"main\" function and so forth.\nThis is how we do it too. F500 company.\nGood to hear its working for you and your team.\nAre there any shortcomings to this approach? Or is it bulletproof and can handle all your use cases? What about complex workflows?\nCant really come up with any shortcomings if Spark is suitable for whatever your doing. For more complex scheduling Airflow might be better than Data Factory. We mostly just have scheduled jobs with no complex dependencies.\nDoes Data Factory come with templates for 3rd party API syncing/integration?\nNot sure what you mean precisely. You can call HTTP/REST APIs from ADF and there are a number of native connectors: https://learn.microsoft.com/en-us/azure/data-factory/connector-overview and you can run ADF pipelines using REST API calls.\nDatabricks is in this picture and is offended\nHaha I think the picture is referring to Jupyter Notebooks on peoples laptop \nThese are the steps: * discovery: heavy involvement or lead by data analysts, business analysts, and data scientists on clones of production data - Dev: DS/DA involved as smes, help define unit tests, and similar. They don't develop the code unless they are held to the developer standard - UAT: developers and business analysts only at this phase - Prod: DS looks monitoring for MLOps. Note that a DS writing code after discovery should be mostly unnecessary Don't use notebooks in prod.\nMerge parameterized Databricks notebooks and Airflow DAG to GitHub repo. Deploy to Prod environment using Jenkins. Airflow is fed parameters and executes notebooks on a schedule. We are totally new to cloud env. Been at it 6 months or so. Seems to be working? Heh\nWow thats a lot of steps. Why not use a cloud native data pipeline tool with a dev and prod environment in the cloud so you dont have to do all those steps.\nAs a Data Scientist/Engineer, the first rule is : Notebooks are simply ignored for engineering stage. We are not reviewing any notebook code. We use it like a \"sandbox\" tool to test, write code then we integrate each cell/scripts as modules into the package. Code in notebooks are not maintainable, it's fine for visualization and write some vanilla scripts, but to be honest we do not spend time to correctly split/refactor the code in cells. And jupyter notebooks may be slower than normal python script call (one reason is : cells results are written on the disk).\nIve seen teams check in their notebook code, ask people to review, and then use the notebook for some part of an online training job.\n[removed]\nDatabricks make this quite straight forward! The notebooks are exported as normal Python scripts if you e.g. commit them to a repo. The markdown is commented out (with a special bit of markdown to tell Databricks to interpret as a notebook in their platform) It's functionally the same as if you took a Jupyter notebook and exported as a .py. It's then up to you to write a notebook in a way that it would sensibly run everything you need if you just hit 'run all' from the top.\n[removed]\nMy pleasure! Databricks has improved in many areas, especially in the ease of use\nNot sure how for Databricks but here it is for Jupyter: https://stackoverflow.com/questions/50576404/importing-functions-from-another-jupyter-notebook\nNot sure why people are on here stating opinions as fact. Notebooks can be run as production pipelines, and Databricks actually makes notebooks schedulable within the notebook itself and its for this reason that many teams use it as such. Personally my preference isnt this as i prefer using a process which has better version control. but with that being said every tool has its strengths and limitations. I dont think blanket statements about never etc help anyone.. its better to give an answer as to the tradeoff.\nIve been hearing a lot of good things about Databricks and notebooks in their workflow tool.\nIt definitely has a lot of benefits, a major one for me was the seamlessness and ability to easily share the notebook with peers either as view only or edit. Honestly the only draw back for me was lack of integrated version control similar to how dbt does analytics pipelines. i am all for transparency and if i can share view access with downstream users like DS or DA, so they can better understand more technical aspects of data then all the better\nSo without version control, all your changes hits production pipelines right away?\nif you didnt have any staging tables yes, but that wasnt the major issue as there are a couple of work arounds, the major annoyance was needing to have a backup notebook with old code in-case i wanted to do a reversion.\nDatabricks recently published a lot of material on how to properly test and deploy a notebook as a production artifact. There's no escaping the need for version control, modular and tested code, and CI/CD. Here's the [blog post](https://www.databricks.com/blog/2022/06/25/software-engineering-best-practices-with-databricks-notebooks.html) describing the process, and the related [walkthrough in their docs](https://docs.databricks.com/notebooks/best-practices.html). There was also a [click through guide](https://app.getreprise.com/launch/G6Yp4zy/) that was created to take you through the process step by step.\nWhat has your experience been like doing this?\nTo be candid I haven't run this myself, and I am employed by Databricks. However, I have worked with other companies to help them implement this and the biggest hurdle is whether or not data scientists are comfortable writing modular code and unit testing it. If they are, then I think the barrier to entry for this pattern is pretty low. In general, it resonates with the people who are looking for a viable solution to put notebooks in production.\nData engineer = devops for jupyter notebooks and .py files\nThis is the clearest data Eng definition Ive heard in a while.\nWe build pipelines as a series of notebooks that pass file artifacts from one to the next, using Elyra to do the DAG composition, and then have it translate them into Kubeflow Pipelines. We used to build our automations by developing container images that were invoked by custom factory classes in Airflow. We would prototype in Jupyter and then refactor it in Pycharm. This worked, but the dev process was complicated and caused all the implementation, debug, troubleshooting, etc to fall on the shoulders of the senior DE, namely yours truly, so (for our small data team of 4) there was a constant backlog that my colleagues were perfectly capable of dealing with if not for the steep tooling and process requirements. So, I built a notebook automation starter kit repo that we all fork and a jupyterhub profile that preconfigures the elyra image with the appropriate runtime definition. We decided the risk of shortening the process in this way was acceptable in exchange for our newfound ability to, like, get shit done. It also lets us iterate and collaborate and document everything to delivery on the order of days instead of weeks (or never).\nThanks for sharing, will check out Elyra. &#x200B; Can you also share the notebook automation starter kit?\nI'd love to but I need to strip out all the private work crap before I put it out publicly. The good news is that it's nothing too fancy, just some helper functions for making file artifacts more convenient to store and retrieve between notebooks. The Elyra docs will tell you how to do all of that as well.\nDont\nJesus, just don't. While jupyter notebook is fantastic - it's so easy to have weird bugs in the code because of variable scope & state. So, if somebody gave me a notebook to productionalize - that's a complete refactor of the code. And that's my my current team partners with data scientists to do on ml pipelines. Prototyping in notebooks is fine, just got to refactor that shit.\nIs it your teams responsibility to refactor it or do you make the author/owner do it?\nIt's the data scientist's job to refactor the code - but we pair with them if they need help. Our expectation is that any code that our data scientists put into production meets the same kind of production quality rigor as anything a production engineer would deploy. That means it's easy to read & maintain & deploy, it has excellent test coverage, there's no astonishing behavior, it fails gracefully, it has excellent observability, is deployed through our CI/CD process, etc, etc.\nDo you pair with them before or after the PR is created?\nIdeally well before since it's so much easier to address issues like test code coverage before all the code is written. But sometimes we catch problems at PR time and pair after.\nIf we have a notebook we get it into AWS Glue and their new interactive notebooks feature allows you to schedule when the job runs (just executes the notebook).\nNice. How are you validating the results from running the notebook? Do you have testing for the notebook?\nParametrize and run through Papermill, I was a one man operation data team in the past and it saved me a lot of deployment/debug time\nDo you still do this at your current company?\nNo we have dedicated ETL pipelines, I do use papermill in some aspects to automate reports etc\nWhat framework/tool do you use for your dedicated ETL pipelines?\nGlue/lambda/airflow\nNice. What can glue do that airflow cant and vice versa?\nData catalogging mainly\n[removed]\nAmen\n[removed]\nHaha I think what they mean is dont deploy it and run the notebook as is directly from the author. Do some things first before you make it production ready. If they didnt mean that, thats how I interpreted it.\nIt's fine to use Jupyter Notebooks as a downstream consumer of a data pipeline. It's fine to migrate research code out of a `.ipynb` file into a suitable application code base. It's not fine to \"productionize\" actual `.ipynb` files into a data pipeline.\n[removed]\nI don't know anything about databricks. The issue with Jupyter Notebooks are they are mutable source files with hidden state, non deterministic run order, bloat that looks bad in version control software, and not designed to be imported from. All tools that work around these problems are just making notebooks behave more like modules. But, why not just use modules.\nPR to main branch, upon definition of release tag CI tool picks it up and bubbles it to testing, staging, Prod. Once merge is approved, new release is deployed. Separate workspace (this is databricks now) in prod pulls notebooks from main branch and thus the deployment is complete.\nSo you are running notebooks in prod?\nOh for sure. They're just python code with the line `#COMMAND ----------` in them here and there. They go through the same review process as everything else at the company, they are version controlled, they go through the CI checks and they are run on job clusters that lose their state between runs. The devs use branches like for everything else. I can't see a single reason why anyone who runs any python code wouldn't run it through a databricks notebook. Just because it's a notebook I mean.\nA lot of people have been talking about how great Databricks notebooks are.\nI mean, personally I would love if databricks were a bit easier to use without the notebook aspect; it's pretty tied into the product. But once a notebook has been produced, I can't see why any steps would have to be taken to \"prodify\" it before sending it to prod - not anymore than anything else that is \"prodified\".\nAs the answers here reflect, as long as your stack properly supports notebooks you can be completely fine doing this. Scheduling notebooks using projects like Databricks, nbconvert or papermill can work fine. Orchest (a data orchestration tool we've built, it's OSS) supports hybrid DAGs of scripts (.py) and notebooks (.ipynb). What I consider to be a good reason for having some steps in the DAG as notebooks is if you want some rich output (e.g. plots, summary tables) in the context of a notebook that helps that notebook tell a story of what the step is doing.\nYou can deploy pipelines from notebooks e.g. apache beam. It can be convenient for development. I don't get what is specific about a notebook you can just write the same thing in a script as well. I don't think where you write it is really the concern but how you abstract and organise your pipeline components.\nHow are your abstracting and organizing the pipeline components right now? The specific problem with notebooks is typically the way code is written in it. &#x200B; Typically the code is written in an exploratory manner. A lot of inefficient procedures, faulty logic, not DRY, not tested, not validated, etc.\nPipeline components are just containers designed to process certain steps. By abstract I mean making the components reusable for other pipelines. You are essentially creating containers that have dependencies and I/Os with one another. There are recommended ways to organise pipeline component code (e.g. kubeflow has a preferred directory structure in their documentation). A notebook can compile a given pipeline that you can then deploy. It really depends what you are doing though. Notebooks yeah can produce bad programming habits if that's what you are talking about.\nThanks for sharing!\nDont\nSend it back to the person who created it with a docker environment and guidelines on how toto use it to produce deployable code\nSomeone mentioned something funny: &#x200B; data engineer = DevOps for Jupiter notebook and .py files. What you say would be great to do... if the org supports it...\nnotebooks are like capes, NO CAPES\nI like wearing capes when I need to save DS from productionizing notebooks :-D\nI'll allow it...this time\nThank you! So if you dont use notebooks, what are you using to build your data pipelines?\nFunnily enough, that is what our team is working on right now. AWS Step Functions, Dagster and Airflow have been thrown out there.\nThis is just.... what? .... like.... why? why would anyone ever use a notebook as a prod tool?\nYoud be surprised\nJust dont.\nThere are lots of different attitudes towards this topic (as a proof, look on the comments). I'm from the party that's for notebooks in production! You can check out our [open source](https://github.com/ploomber/ploomber), it helps you overcome most of the challenges that are mentioned above me by translating from .ipynb to .py behind the scenes. We have more tools that allow work through notebooks like SQL from the executed cell, notebook profiling, and experiment tracking.\nAre you asking specifically what the process is from a notebook? Generally it's part of a *different* process. We mostly do exploratory ML & research in a notebook, that's where it really shines. But the data pipeline work gets done elsewhere. We're still figuring out the kinks but exploring and using a variety of open source tools for our data pipelines. Notebook: [Jupyter](https://github.com/jupyter/notebook) Transformation: [Mage](https://github.com/mage-ai/mage-ai), [dbt](https://github.com/dbt-labs/dbt-core) Testing: [Great Expectations](https://github.com/great-expectations/great_expectations) Orchestration: [Airflow](https://github.com/apache/airflow), [Prefect](https://github.com/PrefectHQ/prefect)\nDude, I guess its reverse.\nPlease explain more\nIt very much depends on the state of the notebook, what its purpose is, what data it impacts, how often it needs to run etc. A consistent thing for me is that the code is still usable by developers who need to use it as a notebook while it's in production. If all the code is in a notebook file then I separate out as much of the code as possible into an actual project folder and make sure that the notebook is in a usable condition. From there it requires domain knowledge of what it's actually meant to be doing and making sure that notebook owners aren't impacting any production systems. But the notebook should never be a productionised system. Hosting the notebook is reasonable but not interacting with warehouses and lakes\nShould you be the one having to separate out the reusable code or should the author/owner of the notebook do it before handing it off to be deployed.\nIdeally it should be the owner of the notebook but in my experience it ends up with the deployer to do. Usually, it comes down to an agreement between the two parties and it wildly depends on the code base. How much work is required, experience of the devs from both parties and how necessary it is\nWho is responsible for on-call/operationalizing it/responding to missed SLA or errors that cause downstream delays?\nI'd say it depends on the org but the clearest is when the deployment is a data engineer or backend engineer team and they can provide support. Usually this works because the deployer usually build webapps, refractor the code and adds things like unit tests and can become co-owners of the original notebook code. I personally would never expect a data scientist or an analyst to maintain and support. They are usually not able to from a technical perspective and they haven't written the webserver portion of the code base\nBy scrapping Jupyter and moving all the code to Airflow.\nIs someone manually moving it to Airflow?\nActually, it'd be better to write things on Airflow from scratch. Jupyter isn't for production.\nAre your data scientists writing their data pipelines and model training code in Airflow first?\nAh, different thing. We're only in charge of the pipelines. Not sure what the process of our data scientists are.\nI would simply not productionize the notebook. Tools like \"papermill\" and others that attempt to increase the scope of what can live in a Jupyter Notebooks are classic cases of \"solving the wrong problem.\" I will help you productionize your code out of the Jupyter Notebook. The logic or solution better be nontrivial enough to have justified this whole process of \"DS does notebook, engi does prod\" thing, or else I will be very secretly begrudging in helping.\nWell said my friend, well said. &#x200B; \"The logic or solution better be nontrivial enough to have justified this whole process\" &#x200B; \"secretly begrudging in helping\"\nWe use databricks notebooks and schedule them with airflow.\nWhy not use Databricks Workflows? Why Airflow? Do you use Airflow for other tasks as well?\nyes\nGot it, thanks for sharing.\nIn the end every team in our company can decide how they want to orchestrate their jobs. We (data platform engineering) providing the tools and best practice workflows for our customers.\nYou can schedule a notebook to run on a Spark pool in Azure Synpse.\nScheduling sounds great. What about orchestrating them in a series along with other steps in a typical data pipeline?\nThe place I work at uses synapse as well as synapse pipelines from Microsoft azure, you shouldn't but we do, thankfully I don't have to deal with it directly as we hired consultants to handle it\nWow. Thanks for the warning. What dont you like about it? And if you can choose another tool, which would it be?",
        "content_hash": "390195eae0fdedf05a17c4fc3e46e6ac"
    },
    {
        "id": "otwwfe",
        "title": "This nice illustration or visualization of the Data Pipeline by semantix (https://semantix.com.br/data-platform/). Hope it may some insights for new DE friends.",
        "description": "",
        "score": 383,
        "upvotes": 383,
        "downvotes": 0,
        "tag": "Discussion",
        "num_comments": 25,
        "permalink": "/r/dataengineering/comments/otwwfe/this_nice_illustration_or_visualization_of_the/",
        "created": 1627563182.0,
        "comments": "this is the type of stuff that frustrates me because more time was spent doing silly animations than writing simple descriptions. what are data loaders? is it one data loader for all data sources? is the data lake considered all six of the little icons in the middle? how are things in the data lake actually transformed? why are there two overlapping boxes drawn around the data lakes? don't be fooled into thinking visualizations like these are good because they have colors, icons, and animations. if this was any good, you'd be able to grasp what semantix data platform is, how it works, and why it is different/preferable compared to something like databricks.\nI think the illustrations in [this video](https://www.youtube.com/watch?v=qWru-b6m030) are a bit better- but they still downplay some of the biggest stuff. Like how 'ETL' (or 'Data Loaders' above) is just one little icon. My go-to explanation is to compare it to farming and food processing. Everything is 'food' but if you compare the ETL of raspberries (pick by hand, QC sort removes bad berries, packaged into plastic, stored in refrigeration) to beets (harvested with huge tractor, dried in sun, stored in open-air, further processed based on juice/sugar/canned/raw) they're completely different and you use different tools. Even aside from comparing two foods, even for a single food you would use completely different tools and methods on a small organic farm vs a huge industrial farm. And once you decide on the tools and get the process set up, you need your 'Food Engineer' for upkeep- fixing the tractor when it breaks down, ensuring there aren't bottlenecks between processes, monitoring and tracking metrics of the processes, adding new processes for new food inputs or new product outputs. Really what these visualizations miss most is how bespoke every piece of the puzzle is. All these tools and platforms are helpers at best, while the DE and others are building most of this from scratch. Even if you're in a role maintaining something an earlier DE set up, you still have to learn the specific system.\nI like this food processing analogy a lot! Thank you! Edit: You can take it all the way through to final products... Combine raspberries with sugar butter flour etc (which each have their own processing pipelines), bake, serve on a fancy platter (GUI) and voila, A raspberry pie!\nI really enjoyed that video.\nI suspect that this isn't meant to be technical but a sales pitch presentation?\nExactly. We aren't the audience for this. This is, however, a good resource for a DE who needs to introduce a non-technical person to our platform. We have a new COO starting next week, this is perfect for explaining to them where their managers reports come from.\nOn a separate note: would you consider databricks a layer or a component?\nThe graphic does resemble what Apache are doing with Delta Lake (Bronze, Silver, Gold tiers). Delta Lake also support streaming data which I don't see in this graphic (although that doesn't mean it's not in there somewhere)\nIt is indeed showing the LakeHouse unified analytics platform from Databricks\nThere is also a large segment of the population than can't focus on technical details if there are annoying animations on the screen. I literally can't understand this diagram while it is animated. to any diagram author: PLEASE DON'T ANIMATE SHIT JUST FOR THE SAKE OF IT.\nWord. This is a useless graphic of so many similar data pipelines, except with animated icons\nThe goal of such animation is to engage the audience. I see architects that bitch about animation and come with uber boring powerpoint slides with simple descriptions that nobody want to read or will quickly forget. The key success is to understand that various audiences requires various delivery methods. The impact is as (or more) important that the intent itself.\nAwesome! What is this made with?\nDoes any one know of asoftware to make such animations\nWhere can I find a link to this that doesn't reveal I'm on reddit during the workday? (Edit: https://i0.wp.com/semantix.com.br/wp-content/uploads/2021/06/smtx-data-platform-1.gif)\nBeautiful! Thanks for sharing!\nit looks very very nice and so rich, and yet there's so little actual information as all the important stuff just all sit in the middle a box, which i guess is very high level. maybe because it is depicting the what data rather than where, in what form or what is being done to it.\nYo the animations in this visual add literally nothing.... they just follow the lines which are representing flows. As if we needed moving icons to tell us that the connecting lines represent flows.... in a flow chart. This visual also describes every data platform built, ever. (not restraining the harshness here, b/c blatant free advertising grab- casually threw the fully qualified url link into the post ) I'd like to believe that as the decision makers/buyers of technology in industry get more and more technically sophisticated (and more technologists or ex-technologists move into senior management and executive leadership roles) that sales pitch artifacts (decks, emails, websites, whatever) will be forced to have more actual substance. If a sales person just sent me a link to the api docs/ the **actual** quickstart guide / actual sample data input and output they would save both of our time and i'd actually be more likely to buy the product! but instead everybody just bombards with the same pitch with the same words and the same visuals every time- and none of it describes the actual product! EVERY TIME!\nbeautiful indeed, very true\nReally helpful illustration, but as someone still wrapping their head around the data lake utility, I find the separate section for Data Lake without any flow into typical business use cases indicate there isn't much value beyond labeling and exploring the data. Is this the right interpretation? Is the section above (Raw, Trusted, Service) actually the result of the section below? Essentially a gatekeeper of what flows out from cleaning up the raw data, prior to being sent elsewhere?\nOne use case for raw data is that it can be stood up quickly. No transformations, no cleansing, this is about getting some or lots of data in the hands of someone quickly (e.g. JSON files that drive some other process or drone data for analysis).\nI saw the same on LI and it super cool to see the big PICTURE of DE.\nThis is the type of visual to show to an old-school boss who thinks analytics is just poring through a spreadsheet.\nHello, May i know, how can we do like that pls\nout of curiosity, what sofware was used to create the animation?",
        "content_hash": "12a8dbc2ba41530ec513beffdab05456"
    },
    {
        "id": "1aggfae",
        "title": "Got a flight this weekend, which do I read first?",
        "description": "I\u2019m an Analytics Engineer who is experienced doing SQL ETL\u2019s. Looking to grow my skillset. I plan to read both but is there a better one to start with?",
        "score": 379,
        "upvotes": 379,
        "downvotes": 0,
        "tag": "Discussion",
        "num_comments": 140,
        "permalink": "/r/dataengineering/comments/1aggfae/got_a_flight_this_weekend_which_do_i_read_first/",
        "created": 1706808494.0,
        "comments": "Having read only fundamentals of data engineering, i'd say that's book has a lot of words for very little content.\nMeanwhile the DW Toolkit has a loooot of words for quite a lot. I had to make a book club to help motivate myself through the first few chapters. Its quite the valuable log!\n\nJoe Reiss podcast is a lot of nobody knows how to do anything, its terrible without necessarily digging into any specifics. This kind of vague lamenting about MDS and tools and culture without specifics is the same kind of arrogant, boomer-style nobody knows how to even data model mass dismissal attitude you see a lot on data Twitter. At least, this is what I got out of his stuff in ~Fall 2023. Its possible I caught him at a bad time, but Im not going to dig more into it. I associate the name with peddling and watered down content creation\nBeing in a similar circle and knowing a lot of those people in that world I will confidently say its mostly about building a brand and selling services. None of them are actively trying to teach people objective knowledge. Not that theres anything wrong with demonstrating knowledge and perspective to advertise yourself, but its not what I would recommend as a text book.\nHi, What would you recommend instead? I am in a similar context to OP.\nGood to know other people agree that book was so so. A lot about nothing definite which is how I felt. Its more of an overview versus technical.\nsecond this. ive listened to a lot of his podcasts and i always end up feeling like wow what a waste of time. the guy is wishy washy and always complains about how todays DEs dont do things properly.\nI saw a live keynote of Reis and it was the most underwhelming technical presentation Id seen in a long long time. A bunch of disconnected cliches, software engineering, data normalization, data science he tried to seem knowledgeable about all of it but very shallow . and a few of the \"i'm not even going to start on this topic , so complex this is\". so patronizing.\nSounds a bit like Jonathan Blow, heh.\nThis, and most O'Really books are like that. Very disrespectful to the readers.\nI felt that way with the spark book to with the databricks guys (i think), remember seeing an o Reilly logo. The optimization section was crap.\nI had a similar idea. Why do you think it's like this?\nI tend to love their books. It's how I learned every technical thing I know\nHmmI found it helpful and while long some chapters where not really relevant to me but I think what I got out of it the most was thinking beyond I need to do x to accomplish y. It did a good job arguing a real data engineer can make decisions about the direction of a companys data with multiple factors to consider. If you found the content lacking you may be under selling your prowess in the field, if your brand new and coming from a analytical or similar background (like me) it helped frame my direction into the field.\nSame here. For me it's one of the best books out there for data engineers. Sure, it's still a fundamentals book, but that's what you get. A broad look into the field. And when I want to dig deeper I take their link list and go for further reading. Love it.\nThey have SO many resources from the sources in that book great point\nThe thing I got out of it was: stick to the fundamentals and dont get caught up in tech infatuation, unless proven really useful which is usually with a short time to market or when you are understaffed. Thats the only section that really made good sense.\nThat being said Ill have to take a look at DWT\nHave a few bottles of hard alcohol close by\nAgreed. Came here to say this. Ha!\nSeriously. I bought the audio book and was surprised at how long I've been listening with no meat yet.\nThis ! 1 million times. I stopped reading after part 1. People really really should learn to write consicely. Don't read that book.\nAgreed\nAny idea how I can get it summarized or anything?\nyou could MAYBE get a hold of a pdf version and feed it to an AI asking it to summarize it\nI read the left one I gave up after 1 chapter .maybe I'm those that prefer to watch YouTube videos\nFor the love of God, DWH toolkit. Its incredible how many people in our field havent read it and it makes you next to useless at your job if you have anything to do with Warehousing.\nOBT *shudders* It had a time and a place. The time was 2015. The place was Hadoop & Tableau.\n\nMy data warehouse is an excel spreadsheet\nCool - I'm looking for some practical knowledge in which I can start leveraging right away. Seems like DWH will be good for that\nIve trained folk on data warehousing for years and heres the advice I give them: Step 1) Read Kimball. It wont make much sense but youll pick up a few things. Step 2) Go make a DW for an org. Step 3) Read Kimball again and it will be an ABSOLUTE GOLDMINE, jam-packed full of invaluable nuggets. It truly is an experts book, mostly appreciated by people perfecting their craft.\nMy boss 10 years ago who was a Senior Data Architect would tell me that his boss would ask him to read the book over and over again\nSomeone recommended that I read Kimball a long time ago, like 2013. Was this the book they were talking about? I got the impression it was some theoretical stuff from the 90s.\nI'm in the exact same mind right now. I now refer to Kimball as the gospel. I will never work anywhere again where people refute the gospel.\nI find myself between step 2 - 3. Ready the whole book, spent a year building a DW I'm very proud of using SQL server, SSIS for ETL, ssas for data cube, and tableau for reporting. While applying for other roles, figured out almost no one is interested in that experience. Everyone needs to hear something you've done with a modern DW, like snowflake or bigquery. And when you say you've spent months with azure, databricks, and ADF, they are looking to hear fivetran and DBT. I'm still happy I spent the time and the fundamentals will stay with the rest of my career. I took a senior BI developer job that pays well. I plan to leverage my experience building power BI pipelines within our premium service. I also plan to be a bridge between the BI devs and DE team since I have more experience on the other side that most of them has on the other.\nWow, people can be so shallow, professionally speaking. To think your experience wouldnt generalize because their data platform is cloud based is just bonkers. You have probably been dodging bullets.\n>Read Kimball again and it will be an ABSOLUTE GOLDMINE, jam-packed full of invaluable nuggets. I read it after almost a year on a DWH project. Many things I was doing every day started to make sense...\nReading, can confirm, it is goldmine.\nYou only really need to read like the first 4-5 chapters.\nEven tho TDWT is fundamental, bear in mind it was written when compute prices werent what they were now.\nThis. My colleague started a book club so he had an excuse to have everyone in the data team read this. The abomination that's our dbt project simply exists because previous AEs knew how to write queries, but understood exactly 0 about data modeling.\nIf you go and read Kimball use this guide: [https://www.holistics.io/blog/how-to-read-data-warehouse-toolkit/](https://www.holistics.io/blog/how-to-read-data-warehouse-toolkit/) You don't have to read it cover to cover and some parts just aged badly.\nThank you for this! I got through the first couple of chapters but then lost motivation\nDoesnt that seem like most books and online courses today.\nRead this, wasnt this created by the same guys who wrote the guide of when to opt in for a DWH? (A La production db should never be used for large reads versus rights yada yada yada)\nI have no idea but that sounds interesting. Let me know when u remember what it was.\nnavigate to --> [books/setup-analytics/](https://www.holistics.io/books/setup-analytics/) Took a quick scan of the pdf it wasnt just about dwh. Maybe I just read a few parts like Ive been doing with most books recently. Depending on 2 factors related to scale, the realistic growth rate of an orgs data and again real valuable data versus garbage , few companies would have a need for a robust EDWH, and can probably handle their BI/A needs with just PG and simple pipelines.\nThanks for that. Which parts aged badly in your opinion?\nId guess any parts about normalized form in data warehouses. Columnar data formats like parquet has made the cost the scan wide tables a moot point and storage is far cheaper than compute.\nDon't remember Kimball suggesting normalized form anywhere, that's Inmon. The book only briefly touches on the storage issue, and it's in relation to fact tables, where adding a single column can increase table size by GBs. That part might not be as relevant today, but it's just a consideration and definitely can't be considered as \"aged badly\".\nI don't recall which ones i skipped myself, but i would take chapter 2 and use that as the index to pick the topics you are most interested in.\nSaving for later\nBrilliant comment! I hadnt seen this before and its a great quick read.\nLeaving this here for later\nSaved\nSaving this\nThis doesn't actually make a recommendation about which parts to skip outside of a few specific pages in chapter 2 (which the book says not to read straight through anyways...). It just says generally \"Focus on Timeless Techniques\". Someone tell me how this \"guide\" helps in any practical way\nIts good advice. I am a stickler for doing things the correct Kimball way but now I work with columnar databases I am seeing less need to completely normalise the fact tables as rigorously as I used to.\nyou will be better quickly reading the data warehouse\nAny reason why?\ni read both, i felt is more dense the Data Warehouse. I felt i could apply more knowledge after this book than the other one\nBought Fundamentals of Data Engineer few months ago, read it for a couple of weeks. It gives you an high level knowledge on this topic but I wouldnt consider it a technical guide as it does not provide any real application on technologies but rather some best practices. If you are new to DE i would say that it could be a good start on understanding basic concepts, otherwise its just useless.\nGood summary. It's a great recommendation for non-DEs to read to learn more about DE, but not a good book for DEs imo.\nKimball was required reading for my current position. I read about half of Fundamentals. If you're looking to learn stuff you can apply in real life, then Kimball for sure.\nIf I may, what's your current position?\nAre you a dentist?\nKimball. That will provide a lot of the 'why' that then makes sense of the 'how'.\nHavent read either book, but why trumps how any day. Internalize this thought. Bumped into this article on HN the other day that elaborates on this: https://www.nateliason.com/blog/infomania\nGreat Article, thank you so much :)\nGreat read, thanks!\nJeez, thanks for the read! I think I really needed to be reminded this.\nKimball for sure. Imho it should be required reading for every DE. I've read the other one as well and got a few good practices and such from it, but it was much less immediately applicable in my role than Kimball.\nI own both and have read most of each of them. Data warehouse toolkit has a lot of useful stuff in it if you are heavy in the ETL space and mostly work in the data warehouse. Id recommend it to anyone that wants to expand their knowledge in that specific area without having to reinvent the wheel. I didnt find the DE book all that helpful- Id recommend instead reading Designing Data Intensive Applications as an excellent lay of the land sort of thing\nBy Kleppmann?\nYup, had a warthog on the cover last I saw. Chapters dont need to all be read sequentially and it groups things logically (and includes cool art as a bonus at the beginning of a lot of chapters)\nThanks!\nNo problem! Im a bit of a technical book junkie so Ive got a lot of the o Reilly and manning books send help\nYes. That one. You can pretty much ignore the rest of the books after that one.\nKimball Modelling is a lost art. A little thought on the model goes a long way in reducing your reliance on super powerful machines.\nVery thorough point\nKimball, no question. You would be surprised how you will always come back to Kimball. For me, that's the real fundamental everyone working with a database should know. ( We were once again talking \"Kimball\" yesterday at work ^^ )\nAs many has answered already, Kimball. FoDE is good, it teaches you high level stuff, and focuses a lot on communicating with everyone at your job. However is creating the data for your pipes, and however is going to use it. And creating business value which is the ultimate goal :) So its more of a practical book about high level DE than actual in-depth learning of how to do things.\nKimball all the way. Tho it would take you many flights to read and absorb that amount of info!\nKimball's book is only like 5 chapters of information and several chapters of examples. I don't know how much of that you will absorb if you are not in front of a computer looking at different schema designs. Or studying for an interview. Fundamentals of Data Engineering is a lighter read and will hit you with a lot of stuff at a high level. This will be your best bet for a flight. Kleppmann's DDIA is dense and great for SWE sys design interviews, but will put you to sleep otherwise. This might be good for a flight.\nRalph kimballs! No brainer. Dude. That guy is a LEGEND.\nKimball's more challenging, but you'll probably get more out of it long term. If you start reading Kimball and are lost, read the other one.\nI wish someone gifted me these books, it is very costly in India. Edit - Typo\nJust download them for free wtf\nIn any major city's book market, you would find the DW Toolkit used copy very cheap. Recently bought it for 500 in Pune, though very hard to find I went and asked in each shop for this, after a lot of searching and almost giving up, one uncle saw the photo and said I might have it - let me look. I was lucky that day.\nIf you cant afford the books, look for content by the same authors online. Or similar content. I know there are some great YouTube vids comparing different modeling techniques. Try searching for terms like kimball vs data vault, write down the terms used but not explained, research those terms, and so on. Data engineering is a very broad field and this is a great and free way to dive right in.\nDownload a legitimate copy of Fundamentals of Data Engineering here, for free: https://go.redpanda.com/fundamentals-of-data-engineering\nThank you!\nYou can find it online.\nkimball has been the defacto architecture for a data warehouse in the last 30 years, go for it\nI would say fundamentally TDWT but neither are gona be very enjoyable reads. Heard Star Schema more updated just downloaded it. The art of teaching and making it fun is a lost art. Both are quite painful to go thru. FDE had good sections however that bordered on entertaining.\nSorry what book is the star schema one?\nIf Im not mistaken star schema the complete reference, sorry dont have my laptop with me.\nYup, thats the one. By Christopher Adamson. Have it as well and I did like it - I found it easier to grok than the kimball books at first pass so I found it useful as a companion book\nGood to know Ill present it to my book club \nDoes not matter. You need to read them few times, it's not like listed to radio. DWH Toolkit is perfect book. I have it for years and reread some topics to refresh theory. Fundamentals, you know, it's foundamental it must to be known all the time. Enjoy your trip.\nWorking my way through Kimballs now each night before bed. Theres certainly value in it!\nI have only read about 75% of the DWH toolkit and 25% of the fundamentals of DE, so you can take this for whatever it's worth. DWH toolkit by far. The value of a single chapter in the former is worth at least three of the latter\nThe DWH Toolking was first published in 1996 and there are plenty of stuff still valid today. But the main focus is on Dimensional Modeling. Fundamentals Of Data Engineering has only abstract content and goes through every data related buzzword ever existed without any depth, it's not telling you how to use kafka, but it's telling you there is such a tool with some high level explanation on what it's used for etc. i.e. DWH Toolkit is a school book that teaches you how to do stuff, the other is mainly a small talk you do in the cofee breaks with colleagues.\nKimball is the grandfather of modeling. I would at least read the first four chapters of his book. After that it gets into industry specific models, so skip to the industry youre in and read that chapter.\nI would read data warehouse toolkit only if I wanted/needed to learn about dimensional modeling. Fundamentals has wider implications.\nFundamentals first, kimball second\nThanks!\nWhy one first? You got two eyes, two hands right? Parallel\nThe data engineering one is more interesting IMO... although i must admit, ive not read the data warehouse toolkit one.\nlol\nFundamentals first and then try to build something with the Toolkit.\nIt depends how good a study you are. Both are valuable, but DW toolkit is much easier to read.\nI have read the toolkit, alot of things can be skim through tbh. Once you have fundamentals the modelling piece becomes very easy.\nthe DWT - mostly the first few chapters.\nFundamentals is a bit painful to go through for the mid chapters. The first third is not bad and the last third is better.\nAggregate them\nRead the first 2 chapters of The Data Warehouse Toolkit.\nLeft\nI think they are both fantastic. Depending how deep into the work youll be getting into, I think the toolkit will have more value.\nWell, judging by a lot of the comments on this thread, seems like Kimball is your answer. I've just purchased it. I'm a data engineer by title as of recently, but doing little engineering and mostly working on a Data Warehouse redesign, and it looks like this should be required reading to anyone in that space. Thanks!\nOn a flight? Reading the first one will help you sleep better for sure\nEasy pick , Fundamentals of Data Engineering\nread this first: your task list\nKimball is a good read. The first few chapters set a foundation but then he has a whole bunch of chapters that are examples for different fields. A tip I have is that if you are about to interview for a new DE job read the chapters related to the field the company is in. His solutions might not be ideal but it gets you thinking about how to store data for that niche and help you be and sound more informed about their domain. You dont want the first time you thought about how to design a DW for a hospital or a grocery store to be during the interview!\nIf not either of these, what's a good book to read for intermediate data engineering learning?\nI still use the first edition of Kimball's Data Warehouse Toolkit all the time, it's on my desk as I type, I've had it for years. The second book I don't know, but I may need to get it, looks good!\nI am reading both simultaneously\nKimball books\nLeft for sure\nLeft is the best  for starting point but I would also recommend just learn by doing. A shit tons of small things. Best way to become that what you want. :)\nI had lunch with Joe Reis at my work. He's a cool guy. The first thing he noticed about me was my F-91W Casio. I called it out by its model and everything. He has some interesting takes on the future of data engineering, and I imagine his books embody that.\nAs an Analytics Engineer, this might even be a good, more provocative read for you: [https://uxbookstore.com/product/clean-code-a-handbook-of-agile-software-craftsmanship-1st-edition/?msclkid=4c5188f8003115fcea31ed45a2a180b2](https://uxbookstore.com/product/clean-code-a-handbook-of-agile-software-craftsmanship-1st-edition/?msclkid=4c5188f8003115fcea31ed45a2a180b2) Clean Code is a book that frames poorly written code not as an annoyance but as a deficiency that blocks progress in an organization and provides great tips on making code clean and consistent. I read about how if I have to comment on my code to explain it, it is poorly written, and it changed my perspective on how I write code. I know how to follow logical SQL CTE patterns or Python method chaining so anyone can look at my code like chapters in a book.\nKimball. I went to a few of his courses years ago and Ralph Kimball was fantastic. Ralph would tell you what you should do, why you should do it and how you should do it. His books are the same. Anyone serious about data warehousing should read all of his books.\nJust the first two chapters of the warehouse toolkit. The rest is useless\nJust went through DW Toolkit, it was boring as fuck and there were a lot of unnecessary words and intros. Not enough substance for me.",
        "content_hash": "7f37be5a5cd6e3887da37bd04a5c1eea"
    },
    {
        "id": "1fjv6kz",
        "title": "(Most) data teams are dysfunctional, and I (don\u2019t) know why",
        "description": "In the past 2 weeks, I\u2019ve interviewed 24 data engineers (the true heroes) and about 15 data analysts and scientists with one single goal: identifying their most painful problems at work.\n\nThree technical \\*challenges\\* came up over and over again:\u00a0\n\n* unexpected upstream data changes causing pipelines to break and complex backfills to make;\n* how to design better data models to save costs in queries;\n* and, of course, the good old data quality issue.\n\nEven though these technical challenges were cited by 60-80% of data engineers, the only truly emotional pain point usually came in the form of: **\u201cCan I also talk about \u2018people\u2019 problems?\u201d** Especially with more senior DEs, they had a lot of complaints on how data projects are (not) handled well. From unrealistic expectations from business stakeholders not knowing which data is available to them, a lot of technical debt being built by different DE teams without any docs, and DEs not prioritizing some tickets because either what is being asked doesn\u2019t have any tangible specs for them to build upon or they prefer to optimize a pipeline that nobody asked to be optimized but they know would cut costs but they can't articulate this to business.\n\nOverall, a huge lack of \\*communication\\* between actors in the data teams but also business stakeholders.\n\nThis is not true for everyone, though. We came across a few people in bigger companies that had either a TPM (technical program manager) to deal with project scope, expectations, etc., or at least two layers of data translators and management between the DEs and business stakeholders. In these cases, the data engineers would just complain about how to pick the tech stack and deal with trade-offs to complete the project, and didn\u2019t have any top-of-mind problems at all.\n\nFrom these interviews, I came to a conclusion that I\u2019m afraid can be premature, but I\u2019ll share so that you can discuss it with me.\n\n>Data teams are dysfunctional because of a lack of a TPM that understands their job and the business in order to break down projects into clear specifications, foster 1:1 communication between the data producers, DEs, analysts, scientists, and data consumers of a project, and enforce documentation for the sake of future projects.\n\nI\u2019d love to hear from you if, in your company, you have this person (even if the role is not as TPM, sometimes the senior DE was doing this function) or if you believe I completely missed the point and the true underlying problem is another one. I appreciate your thoughts!",
        "score": 379,
        "upvotes": 379,
        "downvotes": 0,
        "tag": "Discussion",
        "num_comments": 96,
        "permalink": "/r/dataengineering/comments/1fjv6kz/most_data_teams_are_dysfunctional_and_i_dont_know/",
        "created": 1726672238.0,
        "comments": "I think you are spot on and my guess is the reason for this is because data engineering is a red headed stepchild in most organizations. It is a cost center, not a profit center, and so does not get any sort of prioritization at the executive level. You need leaders to actually want operational excellence in this area in order for it to happen, it's not just going to crop up out of nowhere. Everything you just said is the result of a power vacuum - there is no one person directly responsible for making decisions, so they are spread out among all different teams and people, and the result is a fragmented mess. If the product a customer is using is slow, buggy or crashing, it would set off alarms. But if the internal users have the same experience with the data platform you won't get the same response, you will just hear complaints.\nGood lord is this an accurate statement....\nvery often i have to make a Data Council or some variant thereof because there are so many people who are touching one piece of the elephant, often quite senior (Data Council with 5+ SVPS would not be unusual)\nDo you ever add someone to the council, but refuse to grant them the rank of master?\nwhat's the worst that could happen\n> there is no one person directly responsible for making decisions, so they are spread out among all different teams and people, and the result is a fragmented mess. I think that's the job of the Head or VP of data, if it exists. There's a non-trivial balance to find between having a centralized data team that makes highly optimized engineering but is too far from the business to well understand the data and care about it, and having decentralized data teams in each department who know very well their data and business goals but may produce inefficient replicated data processing. In my experience, the second extreme is able to extract more value from the data, so I tend to prefer it. But it should be compensated by having some kind of core data platform team that provides the pipeline bricks and tries to avoid duplication by analyzing potential synergies between departments.\nHow can you communicate this to decision makers and leadership? That it may be viewed as a cost center but heres another way to view it and why it may be important to view it that way?\nHonestly it's a function of bureaucracy in my opinion.\n100%. Do you think its possible/worth trying to productize  the efforts of DEs in some sort of menu so senior management can weigh cost/benefit for initiatives?\nOf course it's possible but it takes good leadership and a culture that actually wants it. You need to treat your data system the same way you would treat engineering out any other product. The same way there is a chief product officer that makes architectural decisions for your product, there needs to be someone in charge that makes architectural decisions for the data platform. You know the saying a camel is a horse designed by a committee? Data platforms at a lot of organizations are camels.\nTrue\nAgreed\n>think you are spot on and my guess is the reason for this is because data engineering is a red headed stepchild in most organizations. It is a cost center, not a profit center What? DE is critical in any company that needs even a moderate amount of data for its operations to run. The job in these cases is as vital as IT\nI never said data engineering wasn't critical, I said it was a cost center. And IT is also a cost center. I'm talking literally here, when a company is doing its financials, IT does not generate any revenue for them and neither does the internal data operations. It is a cost. And profit centers are always prioritized over cost centers.\nForce multipliers.\nmmm yeah I disagree here. Data engineers literally deliver data for important business decisions and feed machine learning models that most definitely make profits, at least those in prod\n*Everyone* in the company should at least be indirectly contributing to profits, otherwise you'd just fire them. But profit centers/cost centers are accounting terms. The sales team directly generates revenue, so it is a profit center, the data team does not, so it isn't. Unless you're in a niche industry, activity of the data team is not sold to consumers. Legal, HR, accounting, IT, etc. are all critical parts of the business, and the Sales Team couldn't operate effectively without them, but those departments are cost centers. They indirectly support the profit centers, and yes unfortunately short-sighted C-levels can focus almost exclusively on profit centers.\nData engineering is infrastructure and the value of infrastructure is difficult to quantify and/or predict. Think about a road network. Excluding toll roads in this analogy, by itself it doesnt generate revenue, but it certainly costs money. What it does/can do though is enable - people to commute to jobs, - the movement of goods between sellers and buyers, - services such as public transport, e-hailing, - and a bunch of other stuff that ultimately contribute to a countrys economy. Data engineering has no value by itself. A shiny gold standard high quality dataset has no value if no one uses it. The output of data engineering has to drive business revenue. This is often not understood, which I think is in part what leads to the dysfunction. Especially when you have non-technical managers.\nHa! I almost commented with a \"roads\" analogy too. I'm a consultant and this hits hard with me. We are often asked, \"So what are you producing for this cost?\". We're enabling businesses to improve their information to make decisions on. That's it. We can build the roads and make sure the trucks can run on it, but we're not driving them or packing those trucks. The business wants a new system of freshly paved highways to drive on, but don't understand the cost and consideration that goes into building it. They see a vision of unlimited speed limits, autonomous cars, and smooth driving with all this data, without understanding: - The civil design (data architecture) - Property acquisition costs (infra/tooling) - Construction (data engineering) - Safety & Traffic Management (security/governance) - Maintenance (Seriously, why does no one fund keeping the roads in good condition?) It's just widely not understood that data engineering **enables** applications, analysis, learning, and decisions driven on improved collective information, but it doesn't build the application, analysis, learning, or make the decisions for the business with that information. We all know that's what AI does.\nHah! I literally used the roads and infrastructure analogy when I presented to my C-levels about what our team does. It really is an apt metaphor.\nI'm gonna tell my manager this next time they ask me to create a dashboard\nThis is just corporate culture. When you dont get rewarded for doing a good job (raise/promotion) there is no incentive to do a good job. 80% of people do the bare minimum or only the things they find interesting.\nI was at a very large company and the TPMs were near useless. So were the PMs though. None of them could figure out how to translate business requirements into actual instructions for devs (mostly offshore). Though communication back from devs was also subpar. So maybe less a lesson in having the right roles and more in having effective communicators and knowledge sharing.\n> translate business requirements into actual instructions for devs Business requirements to technical requirements to me is part of the dev / lead role (or a TPM - I've never had), and a good reason for having devs with some industry experience.\nSome places treat devs as code monkeys: they want to feed them tickets and get working code back with as few questions as possible. Not how coding works most of the time, but MBAs have big ideas.\nI've seen BAs writing requirements that tie the new process tightly to legacy code or to a manual SOP (not realizing that those requirements are forcing technical choices that may be unnecessary)\nIts never a technical problem, its always a people problem. Saw a presentation on this once.\nDo you know how to fix bad process? In my company we have the same problem. No collaboration, just a mess of data\nThis is poetic.\nWow, I want to get a tattoo of that last sentence.\nThe only two problems that exist are: off by one errors, DNS, and people problems.\nAt my company we're trying to enforce \"liasons\" between the data team and the departments. So, for the sales department, let's say, there is one person who all sales requests to the data team flows through. This person has a good working relationship with us and understands the environment at a high level. This helps translate sales needs to us and helps translates our challenges to them. We have this with every department that requests reports or data from the Data Warehouse. It has its challenges, and it is nobody's full time role, but it has worked well in its limited time so far.\nYeah this is really helpful. When I'm in a role where I have to deal with multiple internal stakeholder from different departments, I pretty much enforce this as far as I can. I run a bit of a scrum-lite (I know people hate that term but sorry not sorry) paradigm. I'll have a weekly session where these said stakeholders are allowed to join, but it's completely optional and you're encouraged to drop in or out - this means we can target higher level individuals from departments who may be too busy to dedicate an hour a week. Higher level is better providing they've got a grip on what it actually is they're requesting. This session serves as a very quick show and tell from all members of the data team; which is great because it gives the stakeholders insight into what is possible for us to do for them. Then an update on how the week went in general including ongoing projects and challenges, but only things relevant to the stakeholders present. Then we show our proposed list for the next week, and invite our stakeholders to put forth any concerns or reprioritisations they may have. At this point the stakeholders are much more informed on all the ongoing work and therefore conflicts in prioritisation are rare, and everyone gets their fair share done based upon understanding of how each component is adding to business value - it has worked so well for me in conveying the \"same team\" mentality that is often lost. Stakeholders submit requests at any point during the week and we address those as a data team before this session and work them into the plan based upon how we guess it should look - but assure them that we are working for them and it's just a proposition, and gives us a chance to put forward any technical reasoning we feel is a part of it. Of course, occasionally an exec (who are all informed of the meeting) shows up and prioritises their thing over everyone else's, but that's OK - as long as we're all on the same page that usually goes over fine. The key in these sessions is being open to and manage any followups that are needed to clarify details which are irrelevant to the other stakeholders, as these issues can often arise during these sessions - try not to bore them by getting overly technical, keep it high level, fast moving, and interesting. Often I find people joining just for the update even though they have nothing in the pipeline for their department, which feels great  It also has a bunch of other benefits like allowing us to track how much we are capable of over time leading to better estimates and leaving time aside for emergency or ad-hoc requests that are hard to justify making people wait a week for. I do not believe this system would work at all if ANYONE was invited though, anyone can submit a ticket sure (I think this is OK as long as the liaison is fine with it and doesn't question the request nature in our weekly meeting), but they must have a department specific delegate to dedicate their time to be the exact term you used: \"liaison\".Over time the liaisons become accustomed to our lingo and we can be more casually technical with them and this helps out so much when discussing requests - if we had whoever showing up, this would be harmed.\nIve heard this pattern called the ambassador model, for those who are interested in trying to find other discussion of it online.\nWe did that pretty successfully too. It's a smaller company though so we went with one person working for IT who was the liaison to the rest of the company. Marketing also has one working with IT on customer facing stuff. It's someone with a lot of industry knowledge and better if they have crossover technical knowledge to help the data team understand and be able to understand their POV as well. It's really dependent on the strengths of the people involved though on whether it works or not.\nThanks for this. Are these liaisons more like business analysts? Im in a similar role myself. The challenges I face are that Im not empowered to make decisions. Its a guiding/orchestration role that gets easily ignored as I have no skin in the game - Im not a stakeholder or building pipelines or predictive models. I build data flows showing end-to-end impact (upstream changes affect xxx teams later)\nIt depends. For some departments, like sales, it is an analyst but for marketing it isn't. It's just someone who management is willing to funnel the requests through and someone with the aptitude to work with us on requirements.\nMost companies run on hype created by someone in the herd. Look at the current AI hype talk and data forums filled with crappy advise. Whereas, most companies are still struggling to built a proper data model that can be scalable, efficient and accommodate new line of business data. Most companies also don't need fancy tools like databricks when they're still ingesting only GB of data and no where close to Petabytes of data yet which requires distributed processing. Lot of DE's are tired and exhausted having to keep up with crappy ass tools being shoved into the market and then take courses and certs to stay float in the job market. Companies want to rush into production without having proper naming standards and conventions, documentation. - Don't believe any of those so-called influencors on LinkedIn that talk about fancy DE tools. - Don't buy any DE bootcamps that charge like $1k or $2k or more. Rather watch YT videos and build small portfolio projects that you'll get more hands on to defend during interviews. - Don't follow the herd and compare yourself to others. People will post anything they want, maybe read but ignore them.\nI think, as a data analyst, people on the technical side need to learn how to speak business and try to influence business stakeholders on our solutions. Im on this forum because I find so many bugs upstream. I take the time to understand what my business stakeholders need, and fortunately for me they never ask for anything crazy. Why cant people earlier in the data pipeline take the time to learn what I need? Im beginning to think DE is like SWE where everyone wants to compare their nerd dick sizes and teamwork is something losers do. Everyone wants soft skills to just be there but they arent truly valued or inculcated.\nIve had the opposite experience as a DE. I find one of the easiest ways to move things forward when I cant get good requirements is to just talk directly with the key stakeholders for a half-hour. Try to deeply understand what they want and what theyre trying to accomplish. Once I get that, its pretty straightforward to engineer a solution that fits. But theres usually so many layers of people between with PMs, Managers, analysts, etc., and they just see DE as back-office ticket takers. Not business partners. Theyd rather we spend several months playing telephone and getting poor requirements, delivering a prototype that is way off-base from what the stakeholders want, and then doing it again until they get something good enough but underwhelming. IME, the people in between are usually very hesitant or outright resistant to bring DE into these discussions with stakeholders because it makes them look redundant.\nSomeone mentioned here that might you have high-output guys who usually take charge of design and architecture, and once they're gone, that output and it's understanding are also gone, and thus, everything becomes a mess. You might also have N number of high output guys in different teams who will duplicate work, deviate structure, etc. To add to this, I think a general lack of data engineering hierarchy is a huge issue. Design and documentation, and the decision-making process around this, is often overlooked or completely dismissed. Without a single pointman to make decisions and a team that is subservient to his decisions, you will get the absolute chaos of fractured teams. With documentation and design, come understanding and direction. With leadership comes unity. I think a huge issue, which extends beyond engineering teams and into software teams as well, is this idea of horizontal hierarchy. It doesn't work. You need a higher command to follow. Horizontal hierarchy is no hierarchy at all; it is anarchy. There's a reason the military enforces hierarchy as strictly as it does.\nSenior DE here. Reading the comments, I do not see any mention of the damage that is caused by low engineering standards. I've worked with a lot of people over the last 15 years. There are 3-4 who had the motivation, skill, and curiosity to elevate those around them. There majority of people are lacking in one of those areas or are actively looking to do as little as possible. I can't imagine any amount of excellence in the TPM position would overcome the issues created by a culture that does not value good work. I don't think this is in any way some revolutionary thought. But as I've been around the block and have been interviewing a lot of candidates lately. I am becoming more and more convinced that this is more rampant than one would think.\nAs a consultant dropping in and out of multiple teams across multiple companies, this rings very true. There are often tech challenges, but they're something you can figure out and resolve - even if it takes time to find the right way. People-problems just fester and remain, be it poor PMing, problematic team members or difficult clients.\nIt is really REALLY frustrating when someone raises a defect in an output then just drops off the bloody radar when it comes to getting something fixed. Most annoying ones are where you are told something doesn't work, they manually fix it somehow, then act surprised when you want examples. That, or they bitch about things still being broken when you can't get hold of them. People are by far the biggest challenge.\nI personally hate the conclusion of scapegoating on the TPM. Data pipelines are completely dependent on the source of the data, and every organization has pipelines that were built long after the data sources were created. The sources change because software constantly changes, vendors constantly make updates, new features are added, etc. Rarely does anyone in the organization own the metadata on data sources, and its nearly impossible for any human being to mentally record the ever evolving dependencies on those data sources. And when DEs find out about a problem, theyre hearing about a bad output, which usually requires a deep dive upstream which means its a research problem more often than a bug fix. Corporate culture doesnt adjust well to any aspect of this kind of problem. The data is bad  we need this fixed asap  who messed up If you want to hire one specific person to make these problems less annoying and less likely, the TPM isnt necessarily your answer you need someone who thinks like an architect but is much closer to the skills of a data scientist because most of the solutions that make these problems more tractable are statistical quality control visualizations and alerts that enter upstream and a TPM is far more concerned with the outputs than the inputs.\nI've been a DE at two different companies now for 6 yrs and was a DBA for years prior. So I at least know a few things about data/platform teams. It seems to me, companies hire DE teams because they feel they need us but don't really understand why. This leads to a weaker mandate for a team that really requires a stronger than usual mandate to effect change and succeed. It's very similar to what DS went through we're just a few years behind them. The fact so many companies hired DS teams BEFORE DE (thus having no data infrastructure for DS to productionize anything) is a classic symptom of what I'm talking about. Getting way out over your skis. Sometimes you can score on ability to see the big picture, but then fail on the follow through. This is the story of DE right now. Where I'm at now the product team recently announced (to much fanfare) they're launching a \"data platform\". I'm sitting there, hearing it for the first time in the company wide meeting, wondering if anybody knows there's been a Data Eng team for 2 years already?? I shit you not this is a true story. It's the perfect anecdote really and I picture it being delivered by Michael Scott on an episode of the Office. More commonly though does any of this sound familiar? Need product to change some logging format coming from upstream? Take a number WAY back of the line. Need help from DevOps, they're busy and definitely too busy to help DE. Data platform should be fundamental to a modern company and DE should be a cross functional team. I think companies know this on a high abstract level and this is why they brought us on, but we still often get sidelined. Perhaps the shift requires too much change for the organization. Many in our ranks rushed into DE as it became the new sexy title and so lack the skills to deliver or articulate properly to be effective. It doesn't help either that what we're building often takes a long time to deliver the big ROI. Our failure is complete once we've fallen into pure query driven modeling--building one-off siloed garbage at the behest of marketing and analytics teams. When the right company who truly \"gets it\" hires the right team with the right mix of skills and experience, DE can be magic. These stories of grandeur trickled down from the FAANGS and ninja startups. It's what we aspire to but not so much the reality at average companies who are mostly building cargo cult copies. Do your best but just don't expect this or you may be setting yourself up for disappointment. At most places unfortunately, we're a lame duck team (to some degree) and/or a mess.\npure gold \"--building one-off siloed garbage at the behest of marketing and analytics teams.\" its the same everywhere\n\"...pure query driven modeling--building one-off siloed garbage at the behest of marketing and analytics teams\". Isn't this what we are all about, aka our fate? Lmao.\nIn my experience with startups, the core problem is siloing/lack of cross-org alignment. That comes from deeper issues, such as: - Technology leaders value the data team(s) but other leaders do not. This can cause situations in which a product leader may not believe in using the data in their work but they're pressured into it by leaders outside of their chain of command. For example, this can happen when a PM doesn't believe in what the CTO is saying but half-heartedly complies with the CTO requests. - Oftentimes the stakeholders using data are using it to justify their existing roadmap or mine for success metrics for projects that have already shipped. This can stem from cultural issues within decision-making organizations, such as heavy criticism/punishment anytime plans are changed but relatively lesser praise/reward for true progress. - Siloing/misalignment happens even within engineering orgs. Some of the issues I've seen cause it: The data producers don't even know who the data consumers are, and don't have a quick way to figure it out. Data producers are being overworked and are barely able to get their systems working, leading to not enough time for quality on things like data changes and many other areas. Data producers and consumers may not even talk to each other, so there's limited awareness of what sorts of upstream changes are problematic for data engineering, and vice versa there's limited awareness with data engineering of what's easy or hard upstream. Good TPMs definitely help, though some of the underlying changes are hard for TPMs to address and those need help from leadership. These are some other efforts I've seen help: - Happy hour with DEs + data producers: It doesn't really need much structure, you just need to get people talking - Having DEs paired with individual product engineering teams - Office hours to mentor data consumers - Company-level celebration of good uses of data and/or data deep dives I'm sure there are plenty of other ways to improve data culture, those are just the ones I've seen.\nThanks for the post! I'm curious, could you share the purpose of the survey? Are you doing customer discovery?\nYes\nAs well, would be interested in the research\nWhat kind of product are you trying to build?\nThis is a tricky one. Sometimes you have product managers assigned to data teams -- but the scope of what DE covers doesn't map to a \"product\" definition, and often the product managers aren't technical enough to truly add a ton of value. A TPM is a good idea, hopefully one that has some actual technical acumen. Otherwise I look at the tech lead for the team to provide a lot of leadership. It also matters who's on the stakeholder side. The more technical they are the better. I like to have analysts as the interface to the actual business/operations folks and then engineering to support analysts, but sometimes you need everyone in a room, and you have to have GOOD analysts who won't fingerpoint and who won't (always) try to cowboy up their own private infra.\nUnpopular opinion maybe since this is the focus of this subreddit, but I think DE is a good place to start out or dabble in, and then move on. You can learn a lot in DE, dealing hands on with business data. It provides a great opportunity to move into something more analytics / application aligned / business aligned. I often found it treated like electricity / internet / water. Management expects it to just work, the upside for it working well is limited while the downside of outages / slow onboarding / etc is much larger. The path to senior and opportunities there are somewhat limited and capped. I often found seniors just get expected to do more \"story points\" or tickets rather than being given leeway for deeper project work. Most orgs would love to outsource DE as much as possible to a vendor solution or hire a small team of engineers to automate as much as possible while leaving mostly configuration/DQ/QA/support to a 5-10x larger team of junior DEs.\nI love the electricity analogy\nWhat areas do you think people should move on to from DE?\nWhat are your internal users doing with the data, learn about that, and \"move up the stack\". Look for the power users of your DE platform and understand more of their day to day. edit: note when I say \"move up the stack\" its not my own opinion about data teams, as I've spent most of my career doing it. However it is many of our observation that this is how management looks at data teams.\nI agree but don't at the same time. DE is a dev role that pays WAY more 99% of the time. It's also true there are those 1% Analysts out there talking right into the ears of CEOs at huge companies, probably making millions. It's bloody hard to land in a role like that though. While you need the kind of business savvy you're referring to, it also a requires a huge degree of connections, luck and tenacity. The same thing goes for DE roles too, and your electricity analogy is great. Bottom line though is DEs (and DBAs, BI Devs) are still paid more than 99% Analysts anywhere and everywhere I've ever worked in 20 years. Many analysts in our company ask me often about moving to our team, it's never the other way, because the pay. Electricity is a commodity sure but Electrical Engineers still make more than general, pencil pushers at average companies. Business services, analytics and paper shuffling is an even more plentiful commodity. What you're saying is like hockey is a better career than X since you can be in the NHL. Easy. No problemo! :)\nBut its usually the other way around - people from analytics/business wants to go in DE\nBecause it pays more. But having done both, I dont think it should.\nThe best pay is knowing how to do both, with domain knowledge\nAgreed\nWell more incorrect then unpopular. I take what you mean and see it too. People in business doing more interesting work, more interesting connections having CEO exposure, heck sometimes even their code is more complex. Why is everyone is asking me how to join development and data department in IT? Money. Line workers in IT make more then even some managers and directors in business. Completely illogical, but that's how it is. On the other hand they tried to hire me into business, but the salary demands where completely out of bounds for them. So you should elaborate what roles you mean. Apart from being a director in business, the only other jobs with similar salary are in other IT departments\nHow were you able to talk to so many people?\nCold outreach on LinkedIn. Then it is a numbers game, the curious will schedule a call.\nThis is a feat in and of itself. To go back to your post, I think a lack of a TPM is a symptom though, not the root cause. The reality is that data/Data is an afterthought, even for data driven companies. Its somewhat similar to an architect and no company really hires one as a best practice; its usually when things are broken. In the case of Data, the barrier to entry is fairly low, so companies can go pretty far without hiring dedicated Data folks. By the time they do, the ratios are so bad and Data teams are already underwater. Do you want the team to fix broken business definitions, performance tune, fulfill requests by non-technical stakeholders or plan for the future? You needed all that done six months ago because you havent been able to hire? Another problem is that each team starts hiring their own people and now you have folks doing things in silo. Anyway, I dont have a solution, but Im not sure a TPM is the right answer\nJust curious, what was your message?\nPretty much you can wrap all the problem related to the job with \"business stakeholders\", also non technical management is pretty bad in general: I'll list you a couple of problems they cause: **1) This new tool, is fantastic, it'll become our company's standard!** They decided to save money to introduce 5 new tools to distribute the \"weight\" of the pipelines from a tool we were mainly using to the new set. Now I do agree that the decision have many pros from the business perspective, still, we did not cemented the previous tool and they indtroduced, not 1, not 2, but 5 new ones which have to be learned and no one on the team has experience with (moreover they're cheap tools which probably will have tons of bugs) **2) Idiotic requests, I'll make an example for the DEs willing to read :)** A business stakeholder asked me to pivot a table with 5 columns and more than 30.000 rows making it with 30.000 columns. Now that would be fun as it is, but his idea was to make the data governance team (2 people) write a description for each of the 30.000 columns so other stakeholders could be aware of the meaning of the fields. \\***That's the first time ever I refused to do something at work**\\* (I didn't even refuse to bring untested stuff in production, but this was just too much). **3) Unrealistic deadlines too!** \"This task will take at most a couple of weeks\" --> Due to several problems that arose I completed that task 2 months later with a lot of pressure and lack of understanding and support by both stakeholders and my manager (which by the way was aware of the problems in advance, but when you can work 16hr/day who cares, right?) **4)** **Stupid solution to simple problems** We were getting data from an external company through APIs, now since the APIs had issues, they decided to transfer the data via a shared space. This is no actual problem, apart from the fact that the two datasets, while maintaining the same information were different. Now that genius of a manager decided to merge columnwise the two datasets, with a subset of columns overlapping and others containing the same information, but since the format is different there's a ton of duplicates columns. After he asked me to update that shit we argued so much that he had to call a colleague of mine riding a train and totally unaware of the conversation to demonstrate me he was right.\nI have always been in dysfunctional data teams, Im in one now, and Im the problem to. **You are spot on**, and Ive been thinking about this for a long time. Conclusion: **Data teams will always be like this.** **Reason:** * **there is strong preference for one-man supermans, hotshots,** and much less actual teamwork then it might seem * Any kind data work, be it new report, pipeline, ML model, Data Warehouse, tool selection, will always be better performed by someone with 160IQ > 140IQ > 120IQ etc. * This is both from the perspective of the DEV, but the customer too. . 1x 160IQ will outperform 2x 120IQ on any data task of complexity, from bug solving to enterprise design * Hence these supermans are hired. A lot also come from one-man stints, which doesnt help, but its not the core issue * It happens in legacy warehouse just as in supermodern DBT whatever * **I guess it happens in similar lines of work, where peak performance is crucial, not collaboration** (lead detective, lead surgeon, pilot etc...) * I realized when watching detective stories, where real pressure inside the detective teams is explored We have all of these disfunctions all the time * everyone has wants to be architect. * each time we assigned one, rest of team grouped against him. now we have no architect to make decisions, we are all architects/analysts/developers and reiterate every decision in 20 ppl endless technical meetings * everyone wants to be the project manager and lead a a microteam for his project. but no one wants to actually do the grunt work * everyone wants to be the contact person for the business and share the good ideas and be in spotlight * always creating parallel lines of communications, chaos and confusion * its kind of people who are both smart, ambitious - not nerds - actally good in communication when they want. Which makes it so much more visible - when talking to customer - angels, - when talking to colleagues 200% aholes * no one reads any documentation or patterns * reinventing the wheel at reach release, doing fully duplicate work * \"stealing\" tickets, stories, users.... * badmouthing and criticizing everyone day and night Each time a real submissive junior comes, its a blessing. Finally a person who will just do what he is told and dont compete for everything with everyone. I dont deny Im this person as well, always annoyed of \"slower\" colleagues performing worse then I could, always doing any kind of crap to everyone *OFC the gold pack at end of months makes us all endure and continue*\nThis is very real. I can totally relate to this comment both from a managerial perspective to an engineer perspective since I've been in both. It takes maturity to put things aside and follow a hierarchy. Then again, if companies never enforce a hierarchy and continue with a disjointed horizontal hierarchy model (non-hierarchy) , then this will, in fact, forever be a problem.\nI think it doesn't help that modern management tends to follow the idea that teams can self-organize themselves out of these kinds of dysfunctional behavior. I believe that a good team manager should have a grasp of the work that needs to be done and the types of people (motivation, experience, etc.) make a good team. It doesn't have to be the team manager, nor does it have to be one person, as long as the person responsible for hiring keeps an eye on the big picture.\noh god, you mentioned the self-managed team keyword. Gonna go puke for real. Yes, this is our mantra as well. In practice in means the most noisy people calling the shots, regardless on how wrong, misguided or outright sly they might be.\nI would so happily be the simpleminded gruntwork person on a data team.\nregarding TPM, we have these: * chapter manager - before we also had the business relationship problems mentioned elsewhere, and he did manage to reform this and remove this problem - big success, essentially preventing the disbanding of the team. * he has no vision, no architecture vision, low technical and business understanding - his mantra is based on agile the business makes decisions * any conflict in the team is resolved by group technical meeting - \"solve it among yourselves\" leading to the situation above * \"product owner\" * completely incompetent person promoted god knows from what - almost zero business and both technical knowledge * on plus side can be easily manipulated by us to \"demand\" anything we tell him too. he fails his business clients more then us * most of the time he spends calculating the costs - calculation no one asks for, that are confusing, wrong and useless as he has no good data for it (the irony!) * duo of solution architects * immediately sabotaged and boycotted by rest of the team, the idea quietly cancelled. But your humble storyteller continues in this role anyway, and in reality the are few user stories that are both successful and without his touch, polishing and lubrication\nI would say the reasons are varied across companies, but mostly from what I have seen is a lack of planning at the enterprise level and coordination among the groups. Everyone's priority is for their application to do the thing it's supposed to do and to the extent that other groups have built dependencies off of that then that's their problem. It's often not really a PM problem, but a management and processes problem. You see stuff like enterprise governance, enterprise architecture and data contracts trying to solve those types of problems typically caused by siloed management approaches.\nGood leadership matters, of course. But most individual contributor data engineers that I see are passive in the face of organizational or process challenges. They seem to want to wait around for someone to fix their problem while working in their merry way, solving an arcane technical problem and adding more technical debt. They should be like software engineers did decades ago take ownership of the process that they work within. Customers will always ask for too much, not understand the complexity of your data and you will always get some shit data. stop waiting around for some magic leader to fix your problems and start owning your work processes yourself .. dont ship code without tests, use version control, refractor, create tickets yourself, call postmortems when you see a problem. The whiny passivity of data engineers is gotta end . Stop waiting for your TPM in shiny armor to come save you  take ownership of how you work as a team\n+/- 3%, 90% of everything is bullshit, to quote i dont know who. or, to quote george carlin (i think): think of how stupid the average person is, then think how half the people are stupider than that. this not to despair, but rather to adjust your expectations about whats realistically acceptable functioning. from a reasonable expectation, you can actually achieve improvement.\nThat's basically my experience. My organization has mismanaged our data lakehouse project so bad, that I actually put in my 2 weeks last week. They fired my team lead a few weeks ago and it's just been chaos. Im one of two people at my org that actually know how to be a data engineer, and how to operate our warehouse that our customers use directly. and we're both quitting at the same time. multi-million dollar project, just down the drain.\nI have product owners who argue times should be in their time zone and not UTC. Mention the date dimension they think I am talking twilight zone stuff. Its so frustrating.\nProduct Manager who works with Federal Agencies here. Good Data governance really requires organizations to have their shit together. The more people in an organization there are the harder it gets to set clear goals and make good decisions. Well maintained, well governed, well defined, well administered, discoverable, secure, tested, and quality enforced data is really expensive and the cost of cutting corners is very rarely paid right here right now ... so standards slide and before long all the data is partitioned in tons of silos. Then the silos breed, and before long you have dense layers of systems all doing the same stuff with the same data in terribly inefficient ways. I can't tell you how many times we have had to back track a runtime data problem 4 or 5 system upstream 'cause very basic documentation just didn't happen along the chain. If you work in a time and materials industry like federal contracting there are perverse incentives to let your client tell you to do dumb shit so they pay you to fix it later. And the one thing the folks love to do is waste loads of time on bespoke documentation or just not bothering and wasting weeks of time on re-work. And since investing in data is very much a pay later thing ... It's amazing just how many contractors get away with blaming others for their own incompetence and making bank spending time cleaning up their own messes really slowly.\nAlways a people problem. Mostly people trying to justify a job. Too many people involved whore normally non-technical trying to explain data modelling from the aspect of an excel pivot. Plus, people always think their role is the most important in process, I see this with DEs as much as everyone else. Data is no good in a box. Reporting is no good without data. Business is no good without reporting. In my experience you either need good people around you, or you need to become a jack of all trades. I am regularly expected to be the DE, BI dev, analyst and SME on reporting projects because its just easier to understand everyones requirements if I can do their job. Not ideal in any situation though.\nDoesn't sound too different what many SW teams are saying though.\nData PM here: this is my job\nI used to work as a \"data engineer\" that functioned as a business analyst. While I still hold the position that both sides should be a little more well-rounded so there's no need for a translation layer I definitely agree that it helps I think on top of a translation layer, an individual who can play the corporate game on behalf of their more technical counterparts who might not want to is helpful But, one thing that I struggled with a lot was a manager who I felt like could not do my job and therefore just could not wrap their head around why things took so long. Being managed as a technical person by an arguably non-technical person is insufferable Not only because you can't learn anything from them, but because they really can't wrap their head around what you do all day and you're constantly justifying the time you spend on things\nYou just have to work for a company that has an inherent data culture. Avoid businesses with high touch sales or marketing cultures, ideally for a company where most data is generated without much effort.\nData is political. Information is power and people in large orgs hoard power to maintain influence and control. Data engineers are one of the few capabilities that collect data from all the systems in the company and put them together. Any conflict, disagreement, bad relationship between departments, misunderstanding and dysfunction will aggregate upwards into the analytics team and its for them to resolve it all.\nThis is so important. For DEs who are not used to the way business ppl think, consider *why* the business wants to mine its data: -To measure success -To allocate budget Any manager of any department is *strongly* motivated (to the point that their job could depend on it) to sabotage any data initiative that could provide hard evidence that something they tried didnt work. A universal data platform is almost guaranteed to find something like this for any manager (after all, no manager is perfect). So unless there is *strong* buy-in right from the top, who are willing to keep the priority of data projects high & provide enforcement to protect them from managers who are worried it will show them up, these projects are highly likely to fail completely.\nManagement wants quick fixes and doesnt want to invest the time to do things properly... Found this song that reasonated [https://youtu.be/MSPrykMKNlo](https://youtu.be/MSPrykMKNlo)\nI agree with your assessment here- data teams are not diverse enough. They are either too technical with no sense of the business and their use cases, or not technical enough and create a jumbled mess with not enough consideration for edge cases. I think there needs to be a balance to every data team and separation of roles with Analytics/BI Engineers (or Data Analysts) with responsibilities to act as the\"voice of the customer\", and leave the design choices, modeling, and strategy to a more technical set of users that can have more complex but efficient design. This isn't a new concept with companies previously having Business Intelligence Analysts and Business Intelligence Engineers, but modern tooling has made it easier to be less technical and move a tremendous amount of data around.\nI think having a BA that works with the customer to describe their needs is a valuable thing. Sometimes you get people who can do both, but it's rare. I think having that person who properly scopes out the problem, and also can scope out why any workarounds work is pretty valuable in finding solutions to data issues. Never been a big fan of telling non-technical customers to design a solution when they don't necessarily understand their data model.\nAgreed. Balanced DE's that can fill into either role in a project are the most valuable. I am certainly not that so I really appreciate anyone on my team that doesn't get annoyed by scope creep or space out in planning meetings.\nThats a lot of interviews! Im curious whats it for? Writing a blog?\n> unexpected upstream data changes causing pipelines to break and complex backfills to make; how to design better data models to save costs in queries; and, of course, the good old data quality issue. Yup. I just came here to say that your analysis is on-point!\nIf you make changes to your table and want to altruistically see the downstream impact, what tools to use? This seems like such a common / obvious / pure technical problem, and yet I have not seen a great solution for it. Am I missing something? (Asking as a data scientist in big tech observing my DE colleagues struggling with this).",
        "content_hash": "0e1cb75ac1bafc2a585e142d5a2d3375"
    },
    {
        "id": "1767l40",
        "title": "Just do a quick 30min to 1hr take home test. \ud83e\udd21 \ud83e\udd21",
        "description": "This is UMortgage interview assessment. Reads to me like free work more than skills assessment.",
        "score": 376,
        "upvotes": 376,
        "downvotes": 0,
        "tag": "Interview",
        "num_comments": 101,
        "permalink": "/r/dataengineering/comments/1767l40/just_do_a_quick_30min_to_1hr_take_home_test/",
        "created": 1697118958.0,
        "comments": "I remember doing something like that from (hmda website) for them them when I applied for data analyst role years back. Took me 2-3 hours. They have never even get back to me lol. Dont waste time\nMaybe theyre still considering you. /s\nCan you recommend a video tutorial on how you started Data engineering if you don't mind me asking\n1) Get a solid understanding of Python and SQL. 2) Improve your Python scripting skills. Learn to work with APIs, deploy scripts that will scrape data or perform an action (can be simple). Just get lots of experience creating scripts. 3) Dive into pythons data wrangling frameworks like pandas and pyspark. Learn about sparks tungsten and learn about the inner workings of pyspark + spark. 4) learn about storage solutions in the cloud. I like AWS, so RDS, redshift, Dynamo, and S3. 5) Create a small platform that scrapes data, dumps it into S3, processes data and stores it in a DB, create an API that allows people to extract the data, and create a dashboard to visualize the data (doesnt have to be super elegant but elegant enough). While doing this project learn about data lakes and data warehouse. Maybe dive into lake houses. Also learn about ETLs and ELTs. 6) Learn about data streams and build a local project that allows you to stream data. Stock market data is popular but may come at a cost. Learn about strategies for data streams. However, Ive found far more jobs that dont deal with data streams that jobs that do. Your mileage may vary. 7) Continue building projects, and start applying to jobs. Make sure your resume showcases each project. Write a blog (or a very detailed readme) about each project and why you built it the day you did. What challenges and opportunities did you face? Just a side note. Most teams believe they deal with big data but true big data is not super common. Example, I used to ingest about a quarter petabyte per month. Now Im in a project that has an entire database that is no larger than 70GB and they think that they have big data challenges that can only be addressed with pyspark. Edit: I found video tutorials to not be as effective as hands on stuff. You can use video tutorials at various stages, but I dont believe that a 5 hour video tutorial will give you the knowledge of tackling data issues. Its a start or a supplement but its not the magic key that unlocks everything. Its a lot of trial and error unfortunately.\nI started a project scraping streaming data and with kafka and dataflow pushing into BigQuery. Then visualize via Looker. %80 is completed. Can you think that when I complete this project, my chances increase on the Data Engineer job listing?\n..you lost me at \"Get\"..\nThats probably why. This is more than 3 hours work to do well\nI see theyve removed the section on proving the abraxas conjecture. Going soft these days.\nAbraxas, starring Jesse Ventura?\nUnexpected Foundation\nNo way this is .5 to 1 hr assignment, whats going on yo?\nTheyre trying to prove there arent enough qualified data engineers to hire so they either need: A. Visa allocations B. Waivers that let them get tax incentives without meeting employment conditions C. To hire the CFOs cousins nephews uncles cousins son even though hes 19, still in school, has never held a job before, and will be paid $200k annual D. Collecting resumes for some other nefarious plot\nNothing that complicated. This is probably just a product of clueless HR and out of touch non-technical hiring managers. Very common combination in the finance industry, especially for low-tier companies like this.\nI dunno, I've only ever seen take home tests designed by hiring team\nThere is always the potential that the hiring team who made it never learned that its impolite to intellectually haze strangers. There is a strong element of superiority complex in the dev world combined with a solid foundation of Dunning-Kruger.\nThis is absolutely it\nThey cry when we quiet quit. Lol\nHadnt even considered these angles!\nPretty good assignment but they should be honest with the time estimates. This will take more than 4 hours for sure. There will be people willing to do it.\nThis is another level\nhaving a take home assessment that take too long is frustrating for an interview, makes me want to try other job openings instead...waste too much time\nI think companies are just trolling for ideas nowadays. \"How would you design this system architecture?\" great thanks for the free expertise, i'ma gonna forward this to Pakistan for implementation.\n\nMove on to the next.\nWhat caching strategies are they referring to? There is no dedicated section on PostgreSQL documentation related to caching.\nWell, to be fair you could still cache data in an in-memory DB like redis, or \"cache\" data by storing the results of commonly run queries in a materialized view (doesn't seem like data would change frequently, so pre-calculated metrics and such seems viable). Though, I think this is much more common in application development-- haven't heard of people doing this as much in BI.\nI dont think this company knows what it needs. Prolly hired a data scientist and they said \"yo we really need a 2 data engineers, devops person and a data modeler/solution architect to be able to do all you are asking of me\" And then they hire overpriced consultants and a single data engineer to keep the lights on.\nStar Schema. When do I start?\nSo is this a way to get requirements for free or what? Big brain move.\nYeah, then have ChatGPT merge 7-8 of the better submissions and it's probably be pretty complete.\nHah, something similar happened to me. They quoted me a time estimate, and I gave them a basic  but functional  API that fulfilled all of their requirements in the allotted time, because fuck them. They then got pissy because I only did the bare minimum. Uh? Those were your requirements, you twats. If the bare minimum isnt enough, raise the minimum bar. Assholes. Oh, and I didnt get the job.\nThat's why it's not worth doing assignments. I'm happy to consult or do a contract project first, but beyond maybe 3 hours for in person interviews, my time isn't free.\n37 pieces of flair\nStick to the time and lose out to someonedesperate without many options who spends 4x the time on it. The company will then moan about not being able to recruit decent talent.\nThis is... so close and yet so far from being reasonable. If it were a toy example with a simple dimensional model a good sr should be able to do that in 1 hr . Given that data model... yikes.\nI did this exact project in a previous commercial mortgage job as an analyst, which was a great opportunity for me to do some hands on modeling. Honestly it took me a few weeks, (partially because I had to figure out the ELT for it as well) but in hindsight its just about the most typical star schema model. They have pretty a easy to understand data dictionary online and lenders have regulatory obligations to provide complete and clear data, so the cleaning is more just applying what you see in the documentation/creating accurate dimensions. Im a DA, not DE, so I have no comment on whether or not this is a reasonable ask for an interview, but for others, if youre interested in modeling/ELT/visualization this is a legitimately good project for professional experience!\nWhere do you actually get this data? Is mortgage data available through some api or portal you can pay for or is it published by the government? Would be pretty interesting to do granular analysis on mortgage loans. Edit: Oh I see now, it's a gov website that does this. Pretty sweet! https://ffiec.cfpb.gov/data-publication/modified-lar/2022 would be pretty cool to get this for commercial mortgages as well\nYep! All publicly available. It was a cool analysis, we used it to look at pricing and competitor volume by metropolitan statistical area, compare LTV, etc. one interesting takeaway is how different 2 $5 Billion in originations companies can be. Some were super high volume of relatively lower loan amounts, where others had fewer loans in super hot and expensive markets.\nIt looks like written by gpt.\nI'd tell them Ok, my fee is $2000 per hour for this.\nExactly. I'd just let them know my consulting rate. Not quite $2000/hour, but my time is extremely valuable so I'm not doing bullshit assignments because their team doesn't know how to interview candidates.\nIt's not horrible since it's actually pretty vague, you could probably chat-gpt or Bard like 90% of this just to get ideas down on paper on how you'd want to analyze the mortgage data, but I'd wonder what round this is for an interview, tbh. Actually writing out a proper data model sounds pretty tedious depending on the fields.\nIf you go the first link, you will see a giant data model without ability to see all fields/tables in a condensed view. Just doing that will take an hour.\nThat sounds cursed\nAt minimum they're asking you to build an entire project requirements outline for them. Hard pass from me lol\nSeems a bit heavier on the transformations and bi piece than actual DE but a nice assessment overall. I'd imagine it would take 2-3 hours though.\nAgreed. All the folks crying that this is trying to get free work this seems exactly what an analytics engineering team would consider bread and butter. I would literally ask a third round candidate to walk through this solution design in an hour-long case study interview. And thats with time at start for intros and end for questions.\nThe WORST part of this is they give absolutely no goals or objectives. Actually, they do, they just aren't telling you, you would be judged by some invisible criteria. Just...procure, analyze, clean, transform, and develop this data for any and all purposes lol no matter what you do they can make criticisms because they can decide after the fact you should have included this or that. Oh, you didn't include risk data? Oh, you didn't provide data on under served markets? You are expected to be a mind reader, because no way they are fairly evaluating people's choices even if it's reasonable.\nAs a hiring manager giving this kind of interview (in person, not take home, but same style) I dont care about hidden criteria. I care more about you making reasonable assumptions for what would likely produce business value, and then describe how that impacts the solution. Its easy to find scores of candidates who have the tech skills to code something that is specs out for them in detail. Id rather narrow down to the fewer candidates who have some intrinsic understanding of what value they could deliver. This case study approach helps with that.\nAny manager that has the poor judgement to hand out this monster of an assignment I would expect to have poor judgement in evaluating someones work on it. It's completely unreasonable. And there's the very real possibility of someone being much smarter than you, and you wouldn't be able to judge the free range work of that person. Give an assessment with goals and discrete, common steps so you can compare candidate to candidate. You can evaluate reasoning and analytical process even within a specific scenario.\n>Actually, they do, they just aren't telling you, you would be judged by some invisible criteria. I've had this before. Any solution that wasn't exactly what they had in mind is wrong.\nAnything that requires me to \"research\" data of an external entity/organization is a *sign* of a red flag to me (e.g., free labor). One alternative to design such tests is to create a hypothetical situation of sort (or something that does not require me to research/crawl data of another real organization).\nHere the deal though, they should already know you can do all these things becuase presumably you ve held a role where that was the job before and that experience is why your interviewing with them. If your junior you probably don't have the data modeling architecture experience to crank this out.\nTotally agree on the ability to go through business documentation and transform business requirements into data models/architecture designs.\nLast line \"This documennt should serve as a comprehensive guide for implementing the data model\" seems like they just want some free work done. The assignment sounds fun, but ideally the goal should be to have a conversation about the applicants findings. Not to provide a 'complete and comprehensive guide'. That's just bullshit. 'Have a look at this data and these questions, we want to hear what you think/your findings during the interview.' would be fine.\nIf the job is for a technical writer, sure. For a data engineer position this seems excessive\n> this assignment sounds like a pretty fun exercise to me What part of a job search do you think \"Hey I'd like to do a pretty fun exercise for a random company that isn't paying me!\"? Your interview and conversation with the person should tell you all you need to know unless you need hyper-specific domain knowledge. Even if you need hyper-specific stuff, there's probably a mandatory training class they'll need to take because your company uses it differently than some other company. Imagine this was a dating app, but before agreeing to date you, someone wanted to \"test date\" you for a week where you had to send them flowers and compliments but they never returned the favor. It's preposterous. If you want me to do a 10 minute quiz on Excel/SQL/Python, great, I understand the base level of CYA. If you send me something like this, I'm either not responding or I'm running it through ChatGPT and hoping you're as big of an idiot as I think you are. \nThat was my thought too. This type of exercise seems pretty reasonable in terms of figuring out whether someone has the core skills needed for a DE job. If they're doing the old \"get the interview candidate to do free work for us\" trick, that's super shitty, but if that's not the case, I don't see what's so egregious about this.\nNow imagine doing this for multiple companies you are applying to and getting ghosted by them. It's rarely if ever worth the time as an applicant.\nThis is quite a nice assessment !!!\n[removed]\nYou dont build data models for your BI team?\nVery cool assessment. Youd be surprised at how many people will fail this.\n>Very cool assessment. Youd be surprised at how many people will fail this. elaborate! Please\nI work as a Data Platform Engineer and I'm applying for more Senior Data Engineer roles at the moment. I really don't mind them as long as I'm learning something new, or revisiting something from a long time ago like SQL, or new python frameworks. My last 4 have all been quite different and have taken 10,3,4 and 11 hours respectively. I imagine it gets old pretty quick once they all get a bit samey or there's nothing new or interesting to gain from them. GPT4 is a godsend though\nWhere are you having time doing free 10 and 11 hours take home projects? Without even a guarantee that the role might fill up, that they might already have someone in mind internally?\nFor swe roles it's not uncommon to get 3-5+ hr long assignments. I usually do them anyway because I usually get the job.\nUnpopular opinion probably, but this doesnt sound that bad. Maybe for a try-hard who _wants_ to do free work. But competent DEs could write an outline that hits many of those bullet points with off-the-shelf techniques in 30 minutes. 30 for inspecting the data and forming the plan in the first place. Another 30 for whatever. Its not gonna be the best roadmap for actual implementation but I wouldnt do that level of detail for any company anyway. And you can always expand in the follow up discussion. The red flag is that it looks like domain-appropriate data. So the odds of this being free work are higher. If I were designing a take-home Id create a prompt thats outside of the companys domain so that assignments couldnt be used as free work. Just to help assuage suspicions. Ive also done take homes that pay for serious submissions. Id do that too.\nI had a company task me with building an entire end to end pipeline. I did about 40% of it, and im currently working there now. Love it. You don't always have to finish the take home assessments. Just need to prove that you are capable\nWhat's wrong with this? If you are a good DE this should be really easy. If not, it's doing its job..\nTo be honest this is kinda alright. Although it's deffo not a 1 hour job. I had one recently that asked me to create a simple website, sign in to some tool online and create an api to create a user on my website. Then they asked me to demonstrate how I would push to a github repository. Deliverables was a series of videos explaining what I had did and how I did it. As you can imagine, I said fuck that. First of all when has a data Engineer been asked to create a website from scratch???\nFeed it to chatgpt and see how things go. Just copy paste whatever chatgpt says. It will be a fun experiment.\nUpload this doc to chatgpt and let it do the work\nI did the same Monday\nThat's not even a Data Engineering task, that is something more related to analytics or ETL developers. A data engineer is an specialized software engineer\neh this looks par for the course in today's hiring environment. You either pass because you have better things to do or you do it and just ignore that they're probably going to steal it rather than hire someone.\nEven if this was designed by the hiring team (which it probably was), that speaks to a lack of communication across teams. And in my experience bad cross-team communication usually means as a data engineer youll be asked to do tons of extra work without compensation or consideration of your other projects.\nI dOn'T sEe ThE pRoBlEm HeRe\n..you lost me at \"Data Engineer\"..but plus hopes your way!..\nThat's a red flag for me. Once they used me \"test work\" as their own actual product and I am never falling for it again.",
        "content_hash": "0bb4d3b00b053b24861681a466ac8279"
    },
    {
        "id": "10kl6lg",
        "title": "Finally got a job",
        "description": "I did it! After 8 months of working as a budtender for minimum wage post-graduation, more than 400 job applications, and 12 interviews with different companies I finally landed a role as a data engineer. I still couldn't believe it till my first day, which was yesterday. Just got my laptop, fob, and ID card, still feels so unreal. Learned a lot from this sub and I'm forever grateful for you guys.",
        "score": 379,
        "upvotes": 379,
        "downvotes": 0,
        "tag": "Career",
        "num_comments": 99,
        "permalink": "/r/dataengineering/comments/10kl6lg/finally_got_a_job/",
        "created": 1674606710.0,
        "comments": "Thank you. I have a B.Tech in IT and 2 years of experience as a project manager / data lead at a start up back in India. Moved to Canada for Post Grad in Big Data Solutions Architecture and this is my first full time permanent job over here. And yes I do have a portfolio.\nCould you please share more information about your post-grad? I have recently completed my bachelor in CS and looking for masters program in data engineering.\nI graduated with a canadian bachelors degree in engineering in april 2022 and started working as a data eng in september. Tbh Master's is not necessary to become a data eng, projects/internships will get you further.\nI wouldnt say masters is required, I used it as an opportunity to come to Canada.\nCould you share resources to courses or projects you found most useful when you have the time?\nMostly YouTube, Reddit. Books - Designing Data-Intensive Applications, Kimballs theory summary.\nI'm not sure if he's a student right out of college but I'd appreciate the insight on the portfolio too. Edit : welp, it's there in his post history if you wanna check it out\nWell yes but no. But you get the picture lol.\nI do and also, you deserve this fam. have a good day!\nSo is a bud tender someone who sells weed or grows it or both\nIts a retail / cashier position at Canadian Legal Cannabis dispensaries.\nIt might be the name of someone who sells weed at a weed bar / lounge. So the equivalent of a bartender for weed. No idea if budtenders have growth duties however.\nNo growth duties, we just sell products off the shelf.\nIts a retail job at a cannabis dispensary\nCongratulations!\nThank you. \nCongrats and best of luck! Share anything new you learn!\nOf course I will, looks like Im gunna be taking up a lot of responsibilities in here which is very exciting.\nNice man welcome to the club, good news is the hardest part is getting that foot in the door. Now youll have recruiters calling you and can even work remote anywhere in the world !\nlol sounds good, hopefully in the future.. for now its a hybrid role.\nFor sure Im talking about the industry as a whole, remote jobs are by far easier to get in this industry\nGotcha. Thats the plan.\nrare wholesome post lol gj\nThank you. \nCongrats! For me, it was waiting tables, pizza places, whatever to make a buck. Landing that first job was a game changer.\nIt does, I literally feel a burden off of me.\nCongratulations! This is so inspiring. For someone who feels stuck in square one forever, this has given me hope. Thank you for sharing!!\nTrust me I know how it feels. I basically took big leap of faith moving to a new country in hopes of building a career here. And I basically went into a depression spiral a few months back. But hey you only lose when you give up.\nNice! The first time is exciting, but after the 4th or 5th it starts to wear off, lol\nHopefully Id start something of my own by then.\nCongrats man! Nice to hear this\nThank you!\nWhat was the job interview like for this company?\nVery standard 3 round process. Technical 1, technical 2, manager. Took about 3 months, a bit slow but that was expected since its a government organization.\nNice to hear this with all the bad news daily  Congrats you made it!\nThank you \nItd be great if you can share something that helped you!? Like Im a fresher pursuing Masters, graduating this June and looking for full time no previous exp. And what would you recommend for beginners in DE? Practice resources or projects or what??\nSure, I think youve already heard the python, SQL, leetcode stuff. Other than that these are the few things you can do: Skim through Kimballs book. The first 6 chapters are good enough IMO but finishing it never hurts. Choose one cloud environment and specialize in Its tools. AWS has the bulk of market share which also means they have huge competition, I went with Azure since my old company was already using it. Start picking interesting Data sets from Kaggle and play with them in cloud and make a few projects. Show them off in your portfolio, GitHub offers free hosting upto 2 gb so I used that for my website. But most importantly network, network, network!! Use meetup.com or other free event sites to join in person data events, youll come across a lot of interesting people there and make new connections as well.\nAppreciate it. Thanks for that!\nWoo! Congrats!!\nThanks \nCongratulations  All the very best for your journey ahead!\nThank you \nTC or GTFO\n82k CAD + healthcare + dental + wellness\nThats great! The fist job is the hardest to find, going forward you add 50% easily to tc. Best of luck!\nThank you!!\nCongratulations! Love seeing posts like this. Hope you come back and pass the knowledge on to other seekers in this sub.\nOf course\nCongratulations, don't forget to continue practicing your gratitude punches and you might gain full enlightenment\nHaha on my way to 100 type Guanyin Bodhisattva.\nCongrats!!! I know how that feels, completely unreal, like a dream for a day or two.\nExactly lol\nIf you don't mind me asking, what is your major? Is it undergrad or graduate degree?\nIt is an Under Grad, Universities in India have a different system from over here, we dont have much flexibility when it comes to our electives. Our whole class had to choose our collective electives which will be same for everyone in that class, so I do not know how to answer that question. I majored in Information Technology I guess. Followed by a Post Graduate Certification in Big Data Solutions Architecture in Canada.\nThat helps a lot, thanks for answering, congrats!\nCongratulations! Its really life changing.\nThanks!!\nCongratulations!\nThank you!\nWhats your favorite strain?\nToo many to choose from. Dancehall for anything creative Red Congo or MK ultra for Lifting/ Hiking. Strawnana or Crunch Berry to relax / Chill.\nPerseverance pays off\nIt really does!\nCongratulations  Im so happy for you  keep it up and do not give up \nThank you!\nCongratulations! Super happy for you.\nThank you!!\nGood for you. The first job is the hardest by far to get.\nIt definitely is. Thank you!!\nCongratulations and welcome in DE!!!\nThank you!!\n>Thank you!! You're welcome!\nHey man, I'm looking for a second course right now. Do you mind sharing which college offers 'Big Data Solutions Architecture'? Thanks\nI did my post grad at Conestoga but honestly I didnt learn anything that I cant learn from YouTube or over here.\nYep, definitely feels the same here at Georgian. Did you do your second course too? What would you suggest after Big Data as the first course?\nI don't really suggest a second course, to be honest.\nInteresting. But am I not taking a risk that way? Because if I don't get a job in time, I might not make the PR points.\nCongratulations\nThanks!!\nCongrats!!! Now the real work begins!\nThank you!\nAll the best!\nHappy for you, man!!\nThank you!\nCongrats! And never stop learning and questioning\nNever will. \nCongrats!\nThank you.\nCongratulations, here's to more success and fulfillment \nThank you\nCongratulations!! Love to see posts like this\nThank you!!\nCongrats. Seems to me people who struggle to get the job work the hardest once theyve got it. Good luck to you\nCan confirm 100%\nAny tips on landing a job as a budtender? Im a data analyst who was trying to get into data engineering, but budtending is sounding more and more appealing .\nLearn the basics about difference between sativa and Indica; most common terpenes and their effects; the difference between the THC and THCA and how it goes from 1 state to another; how companies mention thc dosage in the packaging. Thats pretty much it.",
        "content_hash": "2572adf1efa53a539500a2872d5f2015"
    },
    {
        "id": "rr7paf",
        "title": "I'm Leaving FAANG After Only 4 Months",
        "description": "I apologize for the clickbaity title, but I wanted to make a post that hopefully provides some insight for anyone looking to become a DE in a FAANG-like company. I know for many people that's the dream, and for good reason. Meta was a fantastic company to work for; it just wasn't for me. I've attempted to explain why below.\n\n## It's Just Metrics\nI'm a person that really enjoys working with data early in its lifecycle, closer to the collection, processing, and storage phases. However, DEs at Meta (and from what I've heard all FAANG-like companies) are involved much later in that lifecycle, in the analysis and visualization stages. In my opinion, DEs at FAANG are actually Analytics Engineers, and a lot of the work you'll do will involve building dashboards, tweaking metrics, and maintaining pipelines that have already been built. Because the company's data infra is so mature, there's not a lot of pioneering work to be done, so if you're looking to _build_ something, you might have better luck at a smaller company.\n\n## It's All Tables\nA lot of the data at Meta is generated in-house, by the products that they've developed. This means that any data generated or collected is made available through the logs, which are then parsed and stored in tables. There are no APIs to connect to, CSVs to ingest, or tools that need to be connected so they can share data. It's just tables. The pipelines that parse the logs have, for the most part, already been built, and thus your job as a DE is to work with the tables that are created every night. I found this incredibly boring because I get more joy/satisfaction out of working with really dirty, raw data. That's where I feel I can add value. But data at Meta is already pretty clean just due to the nature of how it's generated and collected. If your joy/satisfaction comes from helping Data Scientists make the most of the data that's available, then FAANG is definitely for you. But if you get your satisfaction from making unusable data usable, then this likely isn't what you're looking for.\n\n## It's the Wrong Kind of Scale\nI think one of the appeals to working as a DE in FAANG is that there is just so much data! The idea of working with petabytes of data brings thoughts of how to work at such a large scale, and it all sounds really exciting. That was certainly the case for me. The problem, though, is that this has all pretty much been solved in FAANG, and it's being solved by SWEs, not DEs. Distributed computing, hyper-efficient query engines, load balancing, etc are all implemented by SWEs, and so \"working at scale\" means implementing basic common sense in your SQL queries so that you're not going over the 5GB memory limit on any given node. I much prefer \"breadth\" over \"depth\" when it comes to scale. I'd much rather work with a large variety of data types, solving a large variety of problems. FAANG doesn't provide this. At least not in my experience.\n\n## I Can't Feel the Impact\nA lot of the work you do as a Data Engineer is related to metrics and dashboards with the goal of helping the Data Scientists use the data more effectively. For me, this resulted in all of my impact being along the lines of \"I put a number on a dashboard to facilitate tracking of the metric\". This doesn't resonate with me. It doesn't motivate me. I can certainly understand how some people would enjoy that, and it's definitely important work. It's just not what gets me out of bed in the morning, and as a result I was struggling to stay focused or get tasks done. \n\nIn the end, Meta (and I imagine all of FAANG) was a great company to work at, with a lot of really important and interesting work being done. But for me, as a Data Engineer, it just wasn't my thing. I wanted to put this all out there for those who might be considering pursuing a role in FAANG so that they can make a more informed decision. I think it's also helpful to provide some contrast to all of the hype around FAANG and acknowledge that it's not for everyone and that's okay. \n\n## tl;dr\nI thought being a DE in FAANG would be the ultimate data experience, but it was far too analytical for my taste, and I wasn't able to feel the impact I was making. So I left.",
        "score": 379,
        "upvotes": 379,
        "downvotes": 0,
        "tag": "Career",
        "num_comments": 122,
        "permalink": "/r/dataengineering/comments/rr7paf/im_leaving_faang_after_only_4_months/",
        "created": 1640782928.0,
        "comments": "Edit: For some reason I feel compelled to add a disclaimer that this is my n=1 anecdote. Goog headcount is close to 150,000? now I think? So if this contradicts your own experience, or a googler you know than well yea- every team is different and everyone has a different experience. \\shrug This was my experience at Google. All dashboards and SQL. In some way I felt it actually set me back in my career staying there for two years. There's a lot of tools and tech stacks that I don't have experience with because everything was internal tooling only. I started after grad school at a startup and didn't realize how jarring and frustrating the transition to a mature blue chip company would be and to sit in a team where I didn't control the whole pipeline. FAANG at this point is basically the bureaucracy of Oracle or IBM with better food and branding. If you want to sleep walk through a 9:30 to 4:30 job fine, but it wasn't for me.\n> In some way I felt it actually set me back in my career staying there for two years. There's a lot of tools and tech stacks that I don't have experience with because everything was internal tooling only. This is a great point. Meta has obviously open-sourced some of their internal tools (React, Presto, GraphQL, etc), and others are directly comparable like Airflow and Dataswarm, but ultimately when everything is built in-house, you fall behind with the tools. I definitely miss being involved in the opens-source scene and needing to hack together solutions because we didn't have an outstanding tool already made for us.\n100%. I think a lot of folks don't realize that \"breadth not depth\" folks like me (and it sounds like you too) really struggle with such a narrow scope of work at large mature orgs like faang. It really bites you at perf time. Some SRE team you never knew existed didn't have the bandwidth to do X,Y,Z- so now your dashboard doesn't go into production and at perf time it's a \"failure to launch\" despite the fact that you can't control how another org works. So many little anecdotes like that...\nThis is interesting, can i ask what product you worked on at Meta? I do think the role differs across the FAANG companies and one thing i liked about it is the ability to grab chances to work across the stack with little friction.\nI was told a lot of DE is automating data pipelines and if you like automating things and working w data then its the perfect job for you. Is this not correct?\n\nYoure mad broski\nIts not primarily dashboard development in industry, Ill tell ya that.\nThe truth that us DEs don't like.\nmeanwhile, i'm over here running 12 hours of queries on our single on premises sql server...\nYou must have dozens of terabytes of data to go through? Or are you still storing your database on spinning rust?\nSpinning rust \nIt's a very old term, back from when platters were coated with iron oxide or were in fact made of iron. I think they stopped using iron in the 1980s. Modern drives use aluminum (or glass) platters with cobalt alloys as the magnetic medium. Once solid state storage became the hot thing about 15 years ago (once prices dropped enough to be affordable for databases), spinning rust became the derogatory industry term for the old tech. Databases were one of the first use-cases for solid state storage because much of their performance is determined by IOPS latency and IOPS capacity. Of course spinning rust still has its place in bulk online or streaming storage. It's a good place to keep those seven year old Facebook photos that get looked at once a year, or to keep old kafka logs or whatever. As areal density increases, hard drive throughout has gone up, and for streaming access patterns, modern drives can exceed 250 MB/s. 50 TB drives will be out in a few years, so the tech is keeping ahead of solid state for bulk storage costs. Spinning rust is starting to lose its shine for streaming storage, as a modern CPU core can often process well over 250 MB/s depending on the algorithm, making hard drive linear read speed a bottleneck in modern high core count CPUs.\nI actually thought I responded to this post yesterday and came back because I thought it was interesting lol. At first I was trying to understand why you're response was so long and just thought you liked explaining things. But now I realize in reality you're just showing the phrase \"spinning rust\" is not just a joke but a valid term used to describe the hardware. I also mentioned in the response that I thought I sent... Lol that I would come back to your response since it was interesting and because I could learn a bit from it. Either way thanks for the explanation it's pretty good to know in my opinion .\nWhy does it take 12 hours? How many rows of data are you looking at?\nIn my experience, the 12 hours is the cumulative time taken by a series of queries. Since it's sequential, it takes that much time. It can definitely be optimized. But many companies don't want to rebuild anything. They have visual basic code from decades back still in production. It's cheaper for them i guess. I was maintaining such a codebase for 500USD a month salary in India. Rebuilding requires much better skilled and much better paid devs. Why would they spend the extra millions?\nIn my experience, cutting query times to a 1/10th what they were dramatically increases productivity and eases stress. If the job takes 12 hours to run, what happens if it fails? I have had employees begging processes not to fail so they don't have to work the weekend after it's rerun. Also, during a rewrite I find so many data issues. This led to an entire team being sacked at one company for failing to input data for 4 years. They may not want to rewrite the queries but every day people re-evaluate how they are doing things to find improvements and weed out bad practices. Acting like technology is different is sticking their head in the sand.\n> This led to an entire team being sacked at one company for failing to input data for 4 years. As in, a 4 year gap in the data? Or the team was unable to reliably ingest data for 4 years?\nTo each their own. Your experience is valid and I hope you find what you're looking for in your next role. There have been times, people, and specific projects that made me feel the way you do. However, as a Data Engineer at Meta (who you know) who has worked with a wide variety of people, systems, and projects at multiple levels over a longer period of time, I'd like to respectfully offer an opposing perspective. I'm not trying to invalidate your experience, but instead I'm offering my own experience in addition to yours. >a lot of the work you'll do will involve building dashboards, tweaking metrics, and maintaining pipelines that have already been built In my experience, this is only true for contractors, lower-level ICs, and non-senior new hires. At those levels, managers first want to make sure you demonstrate a very solid foundation in Python, SQL, and internal tooling before they start throwing large, ambiguous end-to-end projects at you. Maintenance is a fact of life due to entropy, but if you learn/follow/create best practices with the pipelines/processes/platforms you own, you shouldn't need to spend too much time on maintenance. Maintaining code that other people wrote is a great way to learn efficient, robust designs which you can then apply to your own projects. >there's not a lot of pioneering work to be done, so if you're looking to build something, you might have better luck at a smaller company. As someone who worked at and helped build a tiny company before joining Meta (then-Facebook), I disagree. While you're correct that smaller companies have a ton of stuff that needs to be built, there are plenty of teams at Meta with \"green fields,\" meaning they're just getting started and you can design and build totally new parts of the codebase for them. The advantage of doing that work at Meta is that there's MASSIVE investment in tooling to allow DEs to focus on solving novel data problems instead of re-inventing the infra wheel. On a more mature team, there's still plenty to build, it's just at a different level. Sure, you have some metrics, but what do they actually mean? Why are they moving that way? Are they they best possible metrics for your team? Etc. If building Data Infra is what you want to do, I can think of a few DE's by name who spend basically all their time on infrastructure, working primarily alongside Software and Front End Engineers, with just a little bit of DS and UX Researcher partnerships thrown in. Granted, they're on a team focused on that type of work. At a smaller company a DE might find themselves swamped with DBA work (like managing a high-availability database cluster) or monotonous ETL with management that resists efforts to scale. That can leave very little time transforming and refining raw data into new information/insights, creating new metrics, supporting Data Scientists designing/running experiments, or working with SWEs to pipe high quality training data into machine learning models and measure/compare results. >There are no APIs to connect to, CSVs to ingest, or tools that need to be connected so they can share data. It's just tables. I totally disagree. Plenty of SWE teams have their own APIs. Learning them greatly expands the type data you can synthesize. DE guidance even calls out an \"Integrator\" archetype that specifically focuses on this kind of work. My most successful projects at Meta have been made possible by integrating with internal and external APIs. Case in point, building helper functions or, better yet, adapter classes to make it easy for others to integrate with an existing API is a great way to demonstrate technical ability and leadership. Regarding CSVs, I don't understand how ingesting a CSV or other flat file is preferable to ingesting data from a logger output table. In both cases, the data is not always perfect and there's nothing stopping a DE from improving existing loggers or just building their own. >...if you get your satisfaction from making unusable data usable, then this likely isn't what you're looking for. Fair point. No one at a world-class company like Meta is shipping code that creates unusable data. It's not always perfect, but it's always usable. At a smaller company that ingests information from outside, you will absolutely need to handle messy data. I've done plenty of that in the past and am tired of it, but I can appreciate your interest in wanting to explore that space. &#x200B; >Distributed computing, hyper-efficient query engines, load balancing, etc are all implemented by SWEs... Mostly, sure. However, a lot of long-term changes currently in-progress to make the whole company's data processes more efficient are initiatives driven by high-level Data Engineers, supported by Director-level DE management and above. There's nothing preventing a DE from making changes to Presto or any other piece of data infra at Meta. Since DE's are the primary users, we're often the ones finding issues or coming up with ideas for improvement. Those can be implemented by SWEs whose primary role is to build these tools, but just today I combed through another DE's code change that fixes an infra issue, so my experience does not match yours. > ...so \"working at scale\" means implementing basic common sense in your SQL queries so that you're not going over the 5GB memory limit on any given node. Working at scale means designing or improving processes to help people beyond yourself and your immediate team. At a small company that ideally means your work scales to the entire company or all of your clients. At Meta, the ideal case is that your work scales to help the entire world. Writing efficient queries is probably the most basic DE responsibility no matter where you work, since computational power is never free. You can't skip the fundamentals and go directly to working on complex systems. &#x200B; >...large variety of data types, solving a large variety of problems. FAANG doesn't provide this. At least not in my experience. For all the reasons above, I disagree. I think your perspective is heavily focused on the limited work that people are exposed to when they start DE work at a huge company and/or working with too many other DEs who are still new to the role/company. &#x200B; >I Can't Feel the Impact How often is huge impact achieved after 4 months on a team and in a new role? &#x200B; >A lot of the work you do as a Data Engineer is related to metrics and dashboards with the goal of helping the Data Scientists use the data more effectively. For me, this resulted in all of my impact being along the lines of \"I put a number on a dashboard to facilitate tracking of the metric\". This doesn't resonate with me. It doesn't motivate me. Me neither and thankfully I've never had to write something like that in my self-reviews. Measurement, metrics, and dashboards are foundational but there's a lot more for DE's to do than that. It's unfortunate that this was your entire experience, but it is what it is. >In the end, Meta (and I imagine all of FAANG) was a great company to work at, with a lot of really important and interesting work being done. Agreed. >I think it's also helpful to provide some contrast to all of the hype around FAANG and acknowledge that it's not for everyone and that's okay. Agreed, with the following caveat: Data Engineers can focus on analytics, machine learning, traditional software engineering, and anything else that involves data. The role is broad and ambiguous so no matter where you go, you have to actively find the opportunities that excite you and create your own path to pursue them. Managers, mentors, and peers can help but the bulk of that work will always fall on your shoulders. Good luck in your next adventure!\nThank you for the feedback. Is your interest in building pipelines something you discovered at Meta or something that you already knew? When doing the interview, did they explain the role properly? I'm sure Meta still has \"data pipeline engineers\".\nAssigning a formal name of \"building pipelines\" was something I discovered at Meta. I had never heard of orchestration before that, and used Windows Scheduler like once in my career to run a script at a specified time. But the idea of writing scripts that pull data together was something I'd been doing for quite a while in my work. So I had a general idea about the types of work I liked doing, and Meta helped put some formalization into that. To Meta's credit, they absolutely explained the role properly. In fact, I can't applaud their DE interview process enough as literally every question that was asked across all 5 interviews (1 phone screen and 4 on site) was relevant to the work I'd be doing. Not understanding that being a DE at Meta would be heavy on the analytics falls entirely on me. I think I was just blinded by the fact that I wanted to be a DE, so I didn't take time to sit back and ask myself if it was the right move. Ultimately, I don't regret my decision, as being a DE is definitely what I want to be doing. It's just unfortunate that it didn't work out there. To be clear, the \"data pipeline engineers\" are the data engineers. There is lots of work to be done with regard to pipelines, and as a DE you'll work with them every day. A coworker of mine actually did some pipeline work that saved the company a TON of money every year by optimizing some pipelines. So the work is there, but stuff like that is pretty rare from what I saw. A lot of it memory optimizations so pipelines stop failing and adding metrics to a dashboard.\nYeah it's probably a question of ratio. Nowadays, they probably need a lot more dashboards and analytics than new pipelines or processes. It's good to see that their interview process was clear though. I always regret to see roles marketed as \"Data Engineer\" when they're actually more \"Analytics engineer\" positions. Thanks again for the feedback.\nAre you able to go into more depth about their onsite interviews? What were they like and what was asked by the team? Thank you for writing this excellent post!\nI wrote about this here: https://tibblesnbits.com/posts/de-interview-faang\nThis is a great comparison for me to read for where I'm at in my career. I recently started a Sr. SWE position focused heavily on data at a fortune 50. It's not a tech focused company, although the tech leadership would have you believe they're the next FAANG. After interviewing I expected that after onboarding I would find clean pipelines with ML driving automated marketing campaigns, and I would be working on building new delivery mechanisms. As it turns out their data pipelines seem to be held together with duct tape and super-glue. When I joined the company, I had to manually initialize a process in Jenkins to update my own SSO permissions. There's tons of technical debt built in where it looks like third parties were hired to set up a specific process, and they just hack & slashed their way to a working solution. It's a complete mess. This means there's a HUGE opportunity to improve things, but first I have to convince leadership there's something wrong with how things are currently running.\nAs a DS I normally hack and slash a pipeline together just to demonstrate an informative correlation in the data. Management then runs with it for a few years until it becomes a requirement for ops and something goes wrong. Then they want devops to formalize it and they hate having to redo everything. While I cant waste time formalizing every exploration into the data. I would like to know what things you would like to see when you get to a company. I could try to at least set up the transition for success.\nThis answer probably isn't what you want to hear, but documentation goes a long way toward something feeling like an architected solution vs. a hack. I've been at my current position for about 3 months, and feel like I'm slowly tracing back each thread of a massive knot of yarn. Part of one of the projects I was working on this week, for instance: A chunk of our customer behavioral data is uploaded to our marketing platform via a daily file drop to an SFTP server by a third party vendor. This has been a daily process in place for over 3 years, and there is no documentation on it. Some of the data are used in a daily communication, and marketing thinks there's other data there that can be used for short targeted campaign. No one I've talked to at my company can tell me what even half of the data points mean. Our account rep at the vendor is on holiday, so I can't find out the source of the data... Never mind that at the very start this data should be feeding to our customer data platform before it ever hits the marketing platform, or that there's no audit trail, error catching, etc. etc.\nThanks for the insight. Personally I comment my code excessively. I would rather scroll past boring words than try to decrypt code from 3 yrs ago. On the external documentation, that is an organizational problem I see everywhere. Some mid level manager 3 yrs ago got a presentation that sold them on the concept. After the project was implemented the Department was reorganized and No one has access to the archived files for the old department name. My solution has been to place all the info in an internal wiki site for company ip and add the link in a comment. Other than that you can find someone that actually worked in the shop during the implementation. If they are a data hoarder then you might get everything you need. Good luck.\nSo are you trying to contrast your experience in essence saying \"it's nice to know I'm at the company that I'm at despite all the problems... since I can look to solve these difficult problems. And so I shouldn't be strongly desiring a job at FAANG so much\" ? I'm just trying to relate your comment to the post.\nExactly. I'm sitting at my company going, \"This isn't what I expected, I should keep trying to get a job at a FAANG\", but actually the mess at my company is an opportunity. If I get a job at a FAANG I'll just be maintaining nicely organized systems that someone else designed at built.\nThat's a good point .\nWell that sounds like you don't like to be a cog in the wheel and you are a builder not an optimizer (in terms of product development). Builders are usually happier in smaller companies or in research departments than in very operational roles in large companies.\nI really like this line of thinking, builder vs optimizer. I'll have to ponder that a bit and think abotu what it means in terms of how I approach projects.\nMy impression of FAANG DE positions has been the same, its more DBA and analytics work than any sort of DE work, which is why I just go for their SWE jobs when I apply to them. This is the only case Ive seen (outside of small body shops that dont know what they want) where the difference between a DE and SWE is more than one being internal and the other being on product.\nDumb question, what kind of education do you look for in those roles, respectively?\nLeft AMZN for similar reasons, little impact and more DS-style work than SWE. Also in Europe the pay is pretty shit (no better than a lot of other companies really).\n> Also in Europe the pay is pretty shit (no better than a lot of other companies really) I think this is really pertinent. I doubt this post would get nearly as much traction (if any at all) if FAANG didn't pay so well in the US because it would be this whole \"so what\" scenario where no one cares that you're leaving. For some, working at one of the leading tech companies is certainly alluring, but I think for most it's just a monetary thing.\nThanks for sharing! I'm a Meta DE myself (just passed my second year) so it is interesting to see other perspectives. You're absolutely spot on when you talk about the DE role at Meta being very different to other companies and actually being closer to an Analytics Engineer. I also feel like what people outside Meta would call a DE is what we would refer to as a \"Data Infrastructure Engineer\". I personally love what I do as I have always considered myself to be more on the DS end of a spectrum you can imagine for DE, where SWE sits at one end and DS sits at the other. I've found that the best (and most interesting!) way to place myself is somewhere between a PM, consultant, and a traditional DE: * **Think Like a PM:** Work backwards from what may be abstract or loosely defined product goals. Plan out the different data assets (Pipelines, datasets, dashboards, metrics) that will help the team get there. * **Execute Like a DE:** Build data assets in a scaleable manner; use best practices set by other DEs/your team, or define your own. * **Support Like a Consultant:** You're one DE to what could potentially be a team of up to 20 engineers and other partners at various degrees of data literacy. You can't possibly support all of them with every data question they might possibly have. Empower your team to get the most out of your data assets by holding show and tells, presenting at team meetings, being transparent with regards to your future plans, and maintaining good documentation. There's definitely less to being a DE at Meta than there is at other companies insofar as the *execution* goes, but a whole lot more to it in terms of the other aspects of the role. Sadly, this isn't for everyone, so I actually have seen quite a few DEs in my time either leave the company altogether or struggle to maintain the performance ratings they'd like because the route to the best impact isn't compatible with their core interests and/or skillset. It's great that you're recognised this in such a short period of time - sounds like continuing would be a surefire route to burnout. Hope you find what you need from your future venture!\nThis is a fantastic alternative perspective of what being a DE at Meta is like, and helps shine a light on the aspects of the role that would make it enjoyable for someone. Hopefully people see this comment and use it as a comparison against the post to determine if Meta is right for them. Thanks for sharing!\nMy only advice to anyone starting a job in a big company is to try to place themselves in a core platform level team where your deliverable is code and not direct business value. Maybe it is obvious that there is more interesting work in that type of teams.. however what is not obvious is that people think you have to be experts to move up the stack, there is some truth to this at senior levels but these platform teams also hire lots of entry level or non-experts. Most of the time they are short handed and asking for a team change is all you need to be placed in one.\nEven working at a non-tech monolithic company youve gotta network with other teams and always be looking for other opportunities. OP giving FB a few months and writing them off is BS. There are certainly departments and teams within those departments that hire positions exactly what OP is talking about. *someone*has to design and build the ETL\nA couple points: 1) I didn't write Meta off after a few months. I've been there for 2 years, but have only recently been working as a DE. So it's not like I'm unfamiliar with the different teams that were available. 2) \"There are certainly departments and teams within those departments that hire positions exactly what OP is talking about\". Not necessarily. Meta is an incredibly mature organization that has had very intelligent people working there for a long time. It's not unrealistic to imagine that a lot of the building has been done and that a lot of what needs to be done is maintenance. 3) \"Someone has to design and build the ETL\". Correct. If you're on a team that's rolling out a new product, then you're lucky enough to get to do some data modeling and ETL design from scratch. But the majority of your work is performing maintenance on existing pipelines, or making tweaks to some tables. If not that, then it's building dashboards or figuring out where the data is for a metric that a DS wants to look at. Your comment does highlight the primary reason I wanted to make this post though. The idea that FAANG is the end all be all for any role perpetuates throughout this industry. There seems to be this resistance to the idea that FAANG might not be the best place ever to work. But the number of people who responded with \"I left FAANG as well\" or \"I've had concerns about these same topics\" is clear evidence that not everyone is content with working somewhere just because of the paycheck. I'd never slight anyone for choosing that route, but to assume that it's the best just isn't accurate for at least a subset of the population.\n>...core platform level team where your deliverable is code and not direct business value. I strongly disagree with this. The point of hiring any engineer, and indeed any employee, is to deliver business value! Engineering is nearly always a *cost* \\- not a profit - centre. If you aren't able to correlate what you do with business benefit, then what the heck are you even doing?\nWell i said direct business value.. how do you measure the business value of the apache beam team at google? Does it bring business value because more people use dataflow absolutely, can you draw a direct line from a new feature to a dollar amount not so easy\nWhat do you mean with dashboard to help DS use the data more efficiently? I cant grasp what kind of dashboard they are looking at. You experience make sense, imho FAANG's are pioneers and mature companies, they wouldn't be where they are if they are still struggling with their data\nDashboards that help DS use the data more efficiently are any dashboard that reduces the amount of code that a DS needs to write in a Jupyter notebook. For any given metric that reports on an aggregated set of data, a DS will be interested in viewing that metric from multiple perspectives. Take car sales for example. The metric, at the top level, might be all car sales in the country. But this can be broken down by make, model, region, lead source, etc, all of which would require the DS to write some code to grab and slice the data. And similar code would need to be written for any given metric a DS wishes to investigate. A dashboard can make this more efficient with some simple dropdown widgets that allow them to slice and dice all they want.\ni got it, its for data exploration\nI think it has always been quite known that jobs at FAANG are less challenging and relatively little impact, but they do make it up with stability and high pay. So I guess to each of his own. I interviewed at FAANG companies a few times and even during the interview it was clear things are a bit boring and people dont really care about you, whereas the startups I interviewed with always had much more interesting questions and challenging problems and seem to genuinely care about me as a person. I ended up withdraw each time I applied for FAANGs. But then again, I always took relatively lower cash pay and those stocks dont even have a price. Mostly people probably consider this stupid, unless you are like me and dont believe in the concept of money. so its really a personal preference thing.\nHey! ex-meta as well. I will eventually put out a video about this. But I did have a slightly different experience. I think the one thing that did resonate is that Meta does have a more mature infra set-up. I almost find it comical that the job descriptions say \"should understand distributed computing\" All of the distributed computing is abstracted away. You will write a lot of SQL and airflow like data pipelines. I do think this makes sense in some regard in terms of creating efficient workflows. However, like the OC of this post said, a lot of DEs get bored quickly because they are used to doing a lot more SWE like work.\nInteresting post OP it does confirm some assumptions I had personally that Most complex problems in DE that can't just be solved with better data pipelines are actually software problems. Also even within that software at the kind of scale it's unlikely you'd be solving these problems on your own they normally encompass several layers of networking and software and disks. Personally I'd normally lean to more software heavy data engineering/backend roles to solve the kind of problems you seem interested In. Also if you look at companies at scale up phases you'll likely have a wider exposure than 1) not enough data to require anything non trivial 2) so much maturity that you're working on a very narrow problem space on .001% optimisations. There are a significant amount of companies that deal with huge data problems and complexity that are not as high profile as FAANG and they will pay money for expertise.\nAside: How tough was it to leave the $? How was the KT experience/discussions etc it being a tier 1 company? (assumption: highest $ == top tier folks, generally)\nLeaving the money is always a hard thing to do, which is what FAANG banks on. It's why they pay you so much. I'm thankful that the new role I'm taking pays well, albeit not as well as FAANG, so it's not as big of a decline as it could have been, but it's still hard. I spent a long time thinking about this decision, weighing the pros and cons of staying somewhere where I felt I wasn't making the best use of my skills but making enough money to set myself up for life. In the end, I decided it was worth it because while I might set my retirement back a few years I would be doing work I enjoy more so working a few more years wouldn't be so bad. I don't know what \"KT\" means, so I can't answer your second question.\nwholeheartedly agree, i left a quant job for a startup job and the regrets often resurface - but it also reinforces everyday what my priority really was. why i wanted to leave everyday etc. Thanks for sharing!\nIf KT means knowledge transfer, then I can say that it's exactly what you expect. The level of intelligence you're face-to-face with on any given day is honestly surreal. One of the things I geuinely loved about working at Meta was that everyone was there to \\_work\\_! I've had a few jobs in my lifetime, and being on a team where people are just there to grind out the hours until 5 so they can leave is frustrating. At Meta, everyone is driven, intelligent, and happy to help you out. The random conversations you have and the incredible people you meet happen at a cadence that you just can't really find anywhere else.\nI think KT means knowledge transfer\nI'm in the process of applying to Meta. How much do they pay? I have couple years experience now and feel like it's time for a payday to fund a small business I'm trying to start. I feel level.fyu is not informative for DE.\nThis is really good info to know, thank you for writing it. A DS friend of mine recently went to Meta and was trying to convince me to come over. I was not intending to, and this makes me feel a little better about it just likely not being the right path for me either.\nLike I said, it all depends on what you like doing. If analyzing data is what you enjoy, I honestly can't think of a better place to work because there are so many different things to work on. But it's definitely not for everyone, and that's okay.\nthis has been my experience working in large corps as well. I was actually trying for faang hoping to do exciting work in DE. but looks like that's not the case there.\nIn my particular case I work in consulting as a DE/DArch and this is basically why I stick to it. The money is nowhere near good enough compared to \"final client\" or something like FAANG. But the good part is that every time we start a project is bascailly starting from scratch. Like my current client, I've been with them for like 6 months. When I got in, they were like \"yeah we use Azure\" then they gave me access and had like 2 subscriptions and no resources on them. They had nothing, not even a storage account. So it's fun because we're now building a data platform and dealing with the entire ingestion layer, including stuff like networking and VPNs, etc. All the data pipelines and making all the data available. Which is a lot of fun. On the other hand, working in consulting, the money is not that good so I keep bouncing between finding something else that pays better but is probably not as fun, and staying where I am\nThere is an inherent catch 22 here which is that for any company where data is the critical factor to success (e.g., FAANG), they will certainly already have a very mature data organization. For any company where data is more ancillary, typically that also means they wont pay as much for perfect data (not always the case, but often). I think this is what youre seeing. The best places to work would be startups where data is the critical factor (or new divisions of existing companies) - but I would actually think these would be substantially more competitive because the architect has significantly more influence than the operator/optimizer\nThere are more gradients there. There a lot of mature companies that are figuring out how important data is and how the landscpa has changed. A lot of consulting is making bank due to the whole \"digital transformation\" and that also gives me a lot of opportunities to implement to people that are in neither band. They are learning now thya data is critical. Is not \"0 bullshit\" but there is definitely a pie to carve there I do agree that startups is where \"the fun\" is seen. But honestly, I don't want to have 0 WLB, shitty pay unless the startup succeeds (statistics is not on our side here) and the volatility of it failing and being out of a job overnight\nThis is such an important post. Thanks for sharing!\nApplause for this insightful and well-written post. Not all DE jobs are created the same way!\nI left Facebook after 18 months of a DE for similar reasons actually. DEs at Facebook are closer to analytics engineers at other companies!\nThank you for sharing your experience OP. If this is generally the case, I find it very ironic that FAANG interviews often test you on harder DS&A leetcode-style questions than other companies that actually need DE's who do serious software engineering.. What was your interview like? Did they test you on a lot of things that you found out later you would hardly use on your day to day job?\nI've been thinking about this quite a bit lately. Not so much from the interview perspective, but more from the day-to-day perspective. Companies like Meta, Google, and Amazon seem like they'd be more beneficial for junior DEs who have maybe 1-2 years of experience. And then once they've grown a bit those DEs would go on to work at places that need deeper knowledge of the data tech stack and more of a SWE skillset. But because of the fact that these companies can pay so much, they attract the top talent, and that top talent creates a higher bar by which everyone is compared.\n>...FAANG interviews often test you on harder DS&A leetcode-style questions... Where did you hear this? The technical questions at Meta aren't too difficult at all. It's more about product sense and behaviour for a DE, and ensuring that you have basic SWE skills in order to execute quickly using the tooling. During my interview I actually performed poorly on the Python side of things but I had enough product sense and general problem solving skills that I made it through.\n>I'm a person that really enjoys working with data early in its lifecycle, closer to the collection, processing, and storage phases. However, DEs at Meta (and from what I've heard all FAANG-like companies) are involved much later in that lifecycle, in the analysis and visualization stages. This is really disappointing. I thought you are going to work on writing systems and maybe even writing tools for DEs. I guess it's because most of the frameworks has already been done and those people arise to director/managers so they don't need new people to write?\nDeveloping systems and tools for DEs is definitely available at Meta. It's just that in order to do that kind of work, you have to be a SWE, not a DE. As others have pointed out, even though the frameworks have built, that doesn't mean they were built right, or that they'll be right forever. Meta (and likely all of FAANG) definitely have opportunities for you to work on that stuff, but like I said you have to be a SWE.\nThanks for the clarification, thought DE should have that piece of job, or part of the piece :/\nAppreciate the candor. I'm currently an analyst at Meta looking to move into DE. May I ask what level you were at and what the salary and RSU grant was?\nThe new Meta name is so terrible\nNow its MAANG lmao\nThe grass is always greener on the other side. Thank you for the write up, OP. As someone who is not at faang, but who presently enjoys the (sometimes frustrating) hands-on design and implementation of data pipelines, the question of If I transitioned to faang, I wonder if my work would continue to be fulfilling and meaningful? has been on my mind a lot lately.\n> and I wasn't able to feel the impact I was making. That's probably the best part at working for Facebook.\nI have thought about being in this position a lot of times, and the first thing that comes to mind is: does it really matter if what you're doing is not 100% what you like but you're getting compensated well enough? Like sometimes I see my job as a means to get the things you want and not to prove to yourself or anyone that you can do more. Good on you for following your heart over money but I think my mind would explode with this conflict.\nSee, I'm the exact opposite. Work, for me, is a chance to learn really cool things and do important work. The money is just there so I can pay for food, shelter, and entertainment. As long as I make enough to live the life I want to live, I'll take a pay cut to work on projects I love.\nThanks for sharing. I was lied into a \"data scientist\" position that turned out to be just writing simple scripts. Now I'm trying to pivot to data engineering but the scale and the requirements aren't challenging enough in the team to actually grow and learn more tooling. After reading this I think what I actually want to do might be to get back to software development instead.\nFAANG is almost never the answer. Those companies are successful but the intellectual challenges and the financial gains of what made them successful have already been shared among the early joiners. It's good for a new grad because it provides validation and some good but practices. The fact that people don't stick around is well documented by the turnover rates. The difficult part is to identify *today* what is most likely to become the next big thing within 5 years if you want to make an impact and make money.\nThis is certainly possible. In fact, I went through a similar transition to become a DE by transitioning over from a Security Engineer within the company. And I definitely considered transitioning over to SWE. There were a couple of things that stopped me though: 1) The SWE interviews at Meta are pretty hard, and I dont have a good enough understanding of data structures and algorithms to do well. I could obviously spend time practicing, but ive got a lot of other things I want to focus my time on learning that are more directly related to DE. 2) Even if I got the SWE role, there's no guarantee that there would be open headcount on a team that does data infrastructure work, so its possible I'd be spending even more time working on stuff I don't like. Meta requires that you stay on any given team/role for 1 year before transitioning, so this would mean 2 year minimum (1 as DE, 1 as SWE on a team I dont like) doing work I don't want to be doing. 3) I was offered a role that allowed me to do the work I want to be doing, in an industry that I think is important, with a salary that is still really good. But the possibility to transition is definitely there.\n>I thought being a DE in FAANG would be the ultimate data experience, but it was far too analytical for my taste, and I wasn't able to feel the impact I was making. So I left. Im trying to get away from my CRMS dev job and get into DE or DS, could this be said to be a big difference between a DE and DS mindset? Ive done some data preparation and some data table transforms as a researcher before my dev job, and I guess I enjoyed both the analytics and the scraping together of different data-sources from APIs.\nI would absolutely consider this a result of a DE vs DS mindset. As I've said in the comments (that's not meant to imply you should have read them all), if you're into analytics, then working at FAANG is likely a great opportunity. During the DE bootcamp, Meta shows you this spectrum that they believe DEs fall on, ranging from DS on the left to Data Infra on the right, stating that as a full-stack DE you can end up anywhere on that spectrum. I agree with them that that spectrum exists, but im not so sure I agree that there's room for DEs to operate at the far right-hand side. Either way, my point is that DEs can absolutely stay close to the analytics if that's what they like, and Meta has a bunch of opportunities for that. It's just not my jam.\nHave you beat.. I left my faang job after 2 months lol\nDid you leave for similar reasons?\nYou made some really great points. For me, it just wasnt a season in my life where I wanted to work 12 hours a day. I didnt want to miss out on my kid growing up so that I could make an extra 40k a year.\nI was at Meta for just over a year before leaving and if I wasn't at the risk of losing on various benefits, I probably would have left sooner. Like you said, being a DE there didn't match my expectations of being a DE overall - the technologies and the day-to-day work specifically. Combine that with their toxic PSC culture and I just wasn't satisfied. I did feel like I lost out on a year of DE innovation during that time too and now have to play catchup. Don't get me wrong, there are some aspects of Meta I miss (the people were great and the benefits were top-notch). But, at the end of the day, my new role gives me the personal growth I seek, despite it driving me crazy from time to time. Best of luck going forward!\nI think this assessment of Meta is pretty good, but my experience at Amazon as a DE was the complete opposite so I wouldnt generalize across FAANG. Metas DE role is more similar to Amazons non-technical BI engineer role. The work I did as a DE at Amazon is done by SWEs at Meta.\nOh, that's really interesting! As I mentioned, I can only speak about Meta and speculate about the rest of FAANG. Can you describe what your day-to-day was like when you were at Amazon? I think it would be helpful for others in the thread to see what kind of tasks you were doing.\nThe DE role at Amazon is really inconsistent, the work you do is totally dependent on the team. While I was there, there was a company-wide initiative that basically required every team to spin up their own data warehouse, so that's what I did for a few different teams. Most of my work was building ETL orchestration using AWS tooling because Amazon doesn't let you use any external framework so you have to do everything from scratch. I almost exclusively used Python or other scripting languages, I hadn't used SQL in at least 3 years before I went to Meta. But there are also DEs at Amazon that do very similar work to Meta. But Amazon makes it very easy to change teams and doesn't require you to wait a certain length of time, so you can pretty much just find a team that does the work you want to do. Meta is almost the exact opposite - the DE role is very consistent across teams, but because of that there's not a lot of variety available. I have a lot of opinions about this but I will say that there is recognition among DE leadership that the DE role lacks technical depth and there are efforts to expand that. You might have noticed that some of the framework work has been moved to DEs, but those roles are pretty informal at this point. But I do think the DE role will grow more technical over time, especially because Meta is uniquely positioned to push data engineering into the object-oriented/API world. I would not be surprised if the role split like Amazon's eventually because it will be hard to expect someone to do point-and-click dashboarding while also expecting them to understand and use OOP. That being said, I think the big advantage of Meta is that it's very easy to change job roles. In particular it's super easy and straightforward to move from DE into SWE, which is very unique among FAANG companies. In my experience FAANGs and even many startups won't bother considering a DE for a SWE role, so it's an incredible opportunity at Meta to be able to become a FAANG SWE without going through the external hiring process. At Amazon there's no official process for moving to SWE from DE and at least when I was there, I tried but it didn't really seem possible. Personally I think that if you want more technical depth it makes more sense to move to a SWE role, instead of just a \"more technical\" DE role at the same company, because companies will look at your resume and just see DE and draw their own conclusions.\nOP, are there any software heavy DE roles at Meta that you were aware of? I heard roles in Infra are less to do with analytics\nI can't speak from experience, as I never worked with the infra team, nor reached out for conversations. But my understanding is that if you're a SWE on the Infra team, then you're doing the software heavy work. If you're a DE, it's still analytics, looking at how people are using the tools, capacity planning, figuring out where namespaces are full or over quota, helping calculate what future data center needs are, etc.\nThanks a lot, that makes sense! I was approached by a recruiter for a DE role at Meta and he was upfront about the type of work which is analytics heavy. Although it is not something that I enjoy doing, FAANG brand name and the fact that Im in Canada where companies pay way less than US based companies is making me consider it.\nThere are certainly reasons to consider taking the role, the most obvious of which is of course the money. And getting Meta on your resume will certainly open some doors. Depending on your scenario, working there for a couple of years could be worth it. It's not an easy decision, that's for sure, and that's why I wanted to write this post, so that people could make a more informed decision.\n...\nYouve thrown away the opportunity to learn about the future of DE and build a very nice foundation for the future. The days of resume driven development in the DE space are quickly getting numbered.\nWhat I understood from the post, is that, if you want to learn and implement about the future of DE you need to be a SWE not a DE. As the DE role just makes use of the tools atleast at Meta. Also what do you mean by > The days of resume driven development in the DE space are quickly getting numbered\nI worked at Meta for 2 years prior to transitioning to a Data Engineer position within the company, and consistently performed well in my performance reviews. So if your belief is that I was not able to keep up with the culture, intelligence of my coworkers, or fast-paced work, then I think I have enough evidence to prove that that's not the case. I've also been working with data for the past 10 years, so if your belief is that I was not able to figure out my role or find a way to make impact, then I can assure you that you're wrong. Ultimately, your comment makes it sound like you believe just sticking it out when you're not happy is the most important thing because walking away from something means you failed, and that doesn't sound like a healthy mindset. That can lead to burn out, unhappiness, etc. Life isn't about \"keeping up with the others\". I like to spend my time doing work that matters and that I enjoy. Plain and simple.\nThat sounds nothing like that to me. Why do you think that or are you just being negative for no reason?\nWhat is an SWE?\nSoftware Engineer\nThank you for the insightful profile of your day to day work! Keeping under 5 G memory on a single node is a great benchmark, btw. I have seen a lot of bad jobs get pushed to prod by just upping the memory.\nMAANG\nYou mean MANGA.\nwhat happens when all the f500 companies level up their infrastructure to that of FAANG?\nI retire and do something else, haha!\nin all honesty, ive heard the same complaints from SWE @ FAANG as well. ridiculous interview process and you spend most of your time making small changes to an existing codebase and you dont really get to code that much. i work at a bankk now and we have to do alot of ETL on text/csv files and using on premise servers..in all honesty i learned alot about being more efficient and optimizing things and am hoping this will translate into a better job in the future but now i am wary of FAANG.\nVery interesting, thanks for sharing. You mentioned everything in terms of data infra was very mature, did you find you got exposure to the best practices they used to build their pipelines? I'm considering whether I want to make a move to FAANG, have built a couple of platforms in small/medium companies from scratch end to end including infra, pipelines and dashboards/transforms and they work fine from an SLA and maintenance perspetice. Wanted to learn more, so thinking that being in a place that's so mature will teach some best practices around those for future experience? Was that the case for you/if you stayed longer?\ndo you have another spot lined up? Otherwise, our team could use you.\nI appreciate the consideration, but I do have another gig lined up.\nHi, thanks for letting me know your thoughts. I am an incoming senior student who just offered an DE position at Faang. I am happy but would want to consider exit opportunities back to swe after this as well. Do you think having Faang in your resume as a recent grads worth spending a couple of years doing 90% analytics, 10% swe? Any thoughts on keeping up with distributed system, infrastructure knowledge in this role? Thank you\nI think its definitely worth having on your resume. Meta is a fantastic place to work, and you'll do some really cool things. And as a new grad, it'll expose you to a lot concepts really quickly.\nWhat skills would you recommend to be hired there?\nI've written about what my background looked like, and what the interview process was like here: https://tibblesnbits.com/posts/de-interview-faang\nThanks for posting your insights. What's your roadmap to become a DE ?\nWhat happens when all the DEs on the left end of the spectrum (the data infra side) leaves? I'm approaching my two years on my team; I'm now the most 'senior' IC on my team, and I find myself 'teaching' basic programming and debugging skills to more product-oriented DE. There is no room to grow on the technical side any more because the org is full of people who can't write (decent) code. I can't find a technical mentor, and my peers are not up-leveling me on the technical end. In my mind you only go into the product /analytics side when you can't hack it as a software engineer. You will get a snowball effect where all your DEs have the same skill sets as your data analysts/data scientists, and anyone who can code will leave as fast as they can.",
        "content_hash": "d24326617aac3e23c7149f0b217732d5"
    },
    {
        "id": "yyh6l9",
        "title": "What are your favourite GitHub repos that shows how data engineering should be done?",
        "description": "Looking to level up my skills and want to know what repos out there follow good data engineering practice.\n\nWhat accounts/repos stood out to you?\n\nWhich repos do you find yourself peeking at from time to time?\n\nWhich ones taught you something that you didn't already know?",
        "score": 375,
        "upvotes": 375,
        "downvotes": 0,
        "tag": "Discussion",
        "num_comments": 40,
        "permalink": "/r/dataengineering/comments/yyh6l9/what_are_your_favourite_github_repos_that_shows/",
        "created": 1668768618.0,
        "comments": "GitLab's is the best I can think of: https://gitlab.com/gitlab-data/analytics\nBut how do I LEARN to code like that. Is reading it multiple times through the recommended way?\nI collected a list of [Open-Source DE Projects](https://brain.sspaeti.com/open-source-data-engineering-projects) including two of myself, [Airbyte Monitoring with dbt and Metabase (a little bit Rill Data)](https://airbyte.com/blog/airbyte-monitoring-with-dbt-and-metabase) released yesterday, and an all-in-one [Building a Data Engineering Project in 20 Minutes](https://sspaeti.com/blog/data-engineering-project-in-twenty-minutes/) from last year (more for advanced users).\nNice try, Elon.\nHuh?\nI think its a joke implying youre Elon musk and asking for help on how to rebuild twitter. If youre out of the loop - in 3 weeks he fired half of the company and another ~half of those remaining quit And hes asking devs to print out code so he can review it  https://twitter.com/caseynewton/status/1593651264419684359 Edit: spelling\nhttps://github.com/awslabs/deequ This is such a well written codebase. It'll teach you how to write modular code, enterprise level projects, test driven code and much more.\nYup I remember reading this when looking for a great expectations competitor. I bookmarked it as such.\nTry these two. [https://github.com/smpetersgithub/AdvancedSQLPuzzles](https://github.com/smpetersgithub/AdvancedSQLPuzzles) [https://github.com/databricks-academy](https://github.com/databricks-academy)\nhttps://github.com/damklis/DataEngineeringProject\nRemindme! 2 days\nCan someone link this if they find it? Was scrolling forever and searched and couldnt find it.\nI think this one https://www.reddit.com/r/dataengineering/comments/ygieh8/data\\_engineering\\_projects\\_with\\_template\\_airflow/\nhttps://www.reddit.com/r/dataengineering/comments/yve7sf/masters_thesis_finished_thank_you/ you'll have to ask OP for the github link though.\nRemindeMe! 2 days\nGreat question!\nRemindMe 7 days.\ngreat question",
        "content_hash": "883605a81a83eb1aa61d82b0bb82cd64"
    },
    {
        "id": "1g2vk24",
        "title": "Good book for technical and domain-specific challenges for  building reliable and scalable financial data infrastructures. I had read couple of chapter. \n\n\n",
        "description": "",
        "score": 372,
        "upvotes": 372,
        "downvotes": 0,
        "tag": "Discussion",
        "num_comments": 30,
        "permalink": "/r/dataengineering/comments/1g2vk24/good_book_for_technical_and_domainspecific/",
        "created": 1728842203.0,
        "comments": "Thanks, Tamer\nThats actually hilarious, its almost certain that OP is the author which is fine but he really should just be up front about it!\nOP's history shows a lot of book recommendations from various authors. Personally, I'd rate even \"weak\" O'Reilly books a 7.5 or 8 out of 10, so it may be worth checking out if you work in that industry.\nThanks, Tamer\nI agree, the book actually looks interesting. All of you should go check it out right now.\nCurious if this book is related more to trading or accountancy? In the UK the Finance department of a company is usually just accountancy\nThis book serves mostly individuals working at institutions such as banks, investment firms, financial data providers, asset management companies, security exchanges, regulatory bodies, financial software vendors etc\nBuy it, bro. Why would it be free? What's wrong with you?\nHow applicable do you feel this would be to other domains? I'm currently in an enginering firm and I've often found value in looking to how other industries have approached and solved problems. For some things, regulatory for example, might be massively overkill for me but is there enough relevant informaiton for other domains?\nData engineering principles are mostly similar but this book explain some nuanced understanding of financial services industry and it's challenges. Fundamental of data engineering remains same. And definitely you can apply those principles to other domain as well. Definitely give it a try.\nThanks, I get an oreilly sub through work so I'll add it to my reading list.\nI see o reilly book cover I upvote\nThanks for the recommendation. Does it cover data governance, audit and retention ?\nThis is the index of book : I. Foundations of Financial Data Engineering 1. Financial Data Engineering Clarified 2. Financial Data Ecosystem 3. Financial Identification Systems 4. Financial Entity Systems 5. Financial Data Governance II. The Financial Data Engineering Lifecycle 6. Overview of the Financial Data Engineering Lifecycle 7. Data Ingestion Layer 8. Data Storage Layer 9. Data Transformation and Delivery Layer 10. The Monitoring Layer 11. Financial Data Workflows 12. Hands-On Projects The Path Forward: Trends Shaping Financial Markets I havent read entire book but it does cover data governance, audit and retention. Its really indepth and comprehensive book Recently launched and available to read online ( if you want to read for free then dm me i can give you orielly access )\nLooks interesting. And currently discounted at the platform I use. I'll skim it on Friday while taking a shot everytime \"financial data\" comes up.\nYoull be dead before getting through the table of contents.\nThanks, my company provides us O'Reilly access. Very good stuff there compared to Pluralsight.\nHi! Would you mind giving me access as well? I didnt know Orielly had something like that\nHow is this supposed to differ from regular data engineering? There's already an O'Reilly book on data engineering.\nIt has some finance stuff.\nThis one has the word \"Financial\" in the title. Which might be some clue.\nAnyone else read this? It's a domain i have a project in\nThis is not released in physical format. Only online ebook\nAny recs for healthcare data?\ni havent read much on healthcare data but i had found one book which is useful for this \"Hands-On Healthcare Data by Andrew Nguyen\"\nThis book cannot be bought from Amazon as it is only on pre-order. Release date as next month. How can OP say good book if one cannot buy it?\ncuz i read it online- i have access to pre release books for example a book to be released in decemeber 2025 is avaialbe ( partly) on orielly online subscription.",
        "content_hash": "7486f63aa2856f307c2b5536a577ff97"
    },
    {
        "id": "zdj8y1",
        "title": "Data engineering with ChatGPT",
        "description": "",
        "score": 368,
        "upvotes": 368,
        "downvotes": 0,
        "tag": "Discussion",
        "num_comments": 51,
        "permalink": "/r/dataengineering/comments/zdj8y1/data_engineering_with_chatgpt/",
        "created": 1670273491.0,
        "comments": "If you haven't had a chance to check it out yet, ChatGPT is \\*extremely\\* impressive. I've also gotten it to write pyspark routines, describe the purpose of arbitrary code, write comments, refactor to rename variables and adjust from snake\\_case to CamelCase, and write exhaustive jest tests for a random function. If it made a mistake I could easily point it out and it would correct it, remembering and modifying the previous answer as needed. It even gave me a short essay on why it would expect the h3 package to outperform the Haversine formula for finding lat-long points inside a given radius when I asked it to write two different implementations of the same function.\nAgreed. I think its good enough for boiler plate especially for unit testing\nAgreed - it's a bit like landing on the right generic doc page for the use case you are after.\nWhich is awesome. Half the time programming is bending an example into your particular use case; great libraries are the ones with a documentation with lots of examples... Having a tool that auto-generates examples for you is very useful to say the least.\nYeah, and it fills the gaps in the documentation! I am sure many support teams are rushing to ChatGPT to answer user questions.\nYes, Ive asked for explanations of complex algorithms and it usually gives me a good rundown where the meaningful bits get a bit vague. Then if you ask it for references or papers it straight makes shit up, title, author, link and all.\nYea it is wrong half the time for me too But dam it's on its way to replace half the workforce\nso do we all start looking for a new field? but seriously, this code is really bad. Yes, it does something, but it is extremely inefficient. I think we are safe. But I can see how this tech is used more and more in IDEs and editors like \"replace ABC with XYZ in all files and sub directories but only files that has py extension\"\nI mean, it's unsettling imo. These models are the canary in the coal mine for me. Github copilot has reported that \\~26% of code generations are accepted unedited. What happens when it gets good enough for 40,50, 60% to be used unedited? The dev market has had such high pay for so long due to an undersupply of developers. But if tools are released in the short term that can make one dev 2-3x more efficient, the job market will certainly restrict.\nI personally see this as a productivity tool, yes. But not as a replacement of an engineer. I've been around for quite some time and you won't believe but even back in late 90s early people were pitching tech to replace programmers (that was the word). Another example is all these talks about self-driving trucks and replacing professional drivers - they were scary close to make it real a couple years ago before this idea died. If you are doing low quality, tedious work, I would be concerned. Writing code to support analytics needs requires some human brain and a good one. At least I like to think this way :)\nJust wanted to add that the self driving idea hasnt died at all. Its still progressing each month with multiple cities now offering driverless rides to people. Its just been at a slower rate than predicted in 2017.\nwho will write the code they use to train the models? one thing i dont see pointed out enough is that *all* these models are 100% derivative of actual people's work (who are of course totally uncredited). i feel like the entire idea of generative AI is to keep people from noticing the human element. these models are not creative or insightful... merely fulfilling their function statistically given their training data. what we should be scared of is how our work is being used for free to power ai products receiving billions of vc - not that theyll replace our jobs.\nYeah but the work required to train a model nowhere near replaces the work it eliminates. An model like this might take, what, a team of 50? 100? 500? How many developers are there? If this cuts an average developer's workload even by 20%, assuming companies then cut their workforce by 20%, how many people does that cut out? Definitely more than the number of people it took to develop the model.\nThe hard part about software engineering isnt coding. It is understanding the problem domain and then coming up with a solution. We have been building tools/frameworks/languages to make the coding part as easy as possible for years and this is just another one. I welcome any and all improvements to my day to day, but it doesnt even come close to pulling business knowledge out of a MBAs head so we can actually build things.\n> What happens when it gets good enough for 40,50, 60% to be used unedited? Its not an inevitability. If it does get there, there will be bigger societal upheavals that will make worrying about our jobs less important than everything else. In my experience using chatGPT and other similar tools, as well as knowing fairly well how the sausage is made (Ive been engineering in deep learning teams for the last 5 years of my career), we dont have much to worry about right now. Ive been using it for Advent of Code and the results are horrendous and usually dont even work.\nAs if the job of a software engineer is writing code mainly.\nIt's supposed to be a conversation, so you can easily make the code better and more efficient by pointing things out to it. Think of it as an intern who came back to you with their first draft after you told them to figure something out on their own. I've definitely had junior DEs come to me with worse looking code than this thing is putting out, and they took much longer to write it. I don't think we're all out of a job yet, but I can already see how I can use this to take care of a few annoying tasks that we haven't had the bandwidth to address because we have too much work and not enough people despite hiring as fast as we can. One of those tasks is cleaning up and documenting the extremely messy spaghetti code of a former DS who decided to leave with no notice and pursue a love of sheep farming.\n>pursue a love of sheep farming I would love to hear that story lol. But if the input is a messy code, I think generated docs will also be messy? Thanks for sharing your example btw, I want to play now myself. It is all over the news and different subs with people imagining how their life will change. Actually, one of the coolest working AI examples I've seen and can actually understand the application.\nLike the time I told it to debug the broken code it wrote and it switched some comments around.\nI have read that people have used it to add comments to code, I didn't see the example so idk how good it is, but seems useful if it works well enough, even if just as a starting point.\nBruh can I github this? Looks incredible!\nAll the AI artists joke about getting a bachelor's in prompts but I think I'm going to start trying this for 5 minutes before stack overflow.\nok now feed it the 50k line legacy codebase with minimal unit tests and ask it to implement that same feature in the code base without breaking any other code. And don't tell it any of the requirements because the people who aren't data engineers don't understand those at all. Just tell it to \"create a job to get business process x to run\" and see how it handles that request, because that's the level of granularity I get from my business partners.\nDo not piss off the bots\nIf you ask that is going to give you some generic answers and future deadlines while getting busy interviewing to become a chat bot in some bank.\nit will tell you to fuck off\nI'm using this right now to work on a project. Whether the code is optimized or not matters very little. My dataset is only 10million records. I'm running single-node spark in a VM with 64gb ram and 32 cores. That handles it just fine. And importantly I am head down speeding towards the output of my analysis. What I find interesting is that so much effort in SWE best practices is based around modularity, reusability, minimizing redundancy. The ideal state is that for every line of code you write you are maximizing new and unique value. Reducing boilerplate code. Everyone hates writing boilerplate code. It's like an invisible procrastination block. ChatGPT and CoPilot absolutely remove a lot the inertia. You absolutely have to still understand code really well. I don't think it reduces the demand on the knowledge of the programmer at this point. However, it won't be unreasonable to think that at some point large language models can also refactor code. And be based on passing unit tests. It's weird to think but we could be moving towards a much, *much* more declarative programming paradigm. Edit: also it should be noted that it does occasionally output things that don't exist or don't work but even then I found it useful. e.g. I asked it to write a python function to simulate sampling from a specific obscure type of distribution and it gave me code for a submodule in numpy.random that doesn't exist but that did lead me down a productive path. Although in fairness what I asked it to do was hardly boilerplate in that scenario.\nMissing from the screenshot, does it actually run/do what's supposed to do? The way the prompt is written, it looks like you asked it to read from a datasource, ensure that all records in that datasource have ID beginning with XYZ (like a DQ check), and insert the latest record for each ID (most recent timestamp?) into another table. The screenshot is truncated, but the only function you see just reads a list of Airflow tasks with a certain name and returns the ones that start with XYZ, which doesn't seem like what you want.\nFWIW ChatGPT has been banned from SO already due to being straight up wrong in most cases\nSO?\nStack Overflow.\nGotcha thx\nThe Printing Press didn't make writers obsolete. Just have to buy yourself a printing press to stay ahead of the game.\nScribes, however... Maybe this will separate the program scribes from the authors\nIm using ChatGPT right now for generate airflow DAG tests and Im spending more time debugging them than if I had actually written them, But the good side is it generates the boilerplate code\noh wow. it is truly impressive! From my experience, it can be buggy and doesn't always follow a specific common logic like humans, but it is dependently a good start! I asked it for tools to manage data, it captured 4 technologies that are commonly used: 1 - Hadoop 2 - Amazon S3 3 - Apache Spark 4 - lakeFS All in all, the last one made me raise an eyebrow. I work full time on lakeFS, and it might be biased and follows me somehow? Or is it just the case?\nIsnt this what powers github copilot? why not just use that instead?\nDo you have a link to that playground? Playing around with this exact prompt as well as couple of easier ones has not at all resulted in code as detailed or comprehensive as this. Most times the generation stopped mid DAG, i.e. the first tasks were covered but not all. Also PythonOperator tasks were not fully scoped, for example row filtering was just not done at all. Based on these results I'm quite sceptical of this screenshot. Edit: What kind of model were you using? Was this output on first prompt or on several prompts? The results are so non-deterministic, and so much context is lacking, I find it really hard to take this at face value.\nHah, I was doing this yesterday with Airflow DAGs too. It gave me a decent enough template to start with; saved me a bit of googling and typing.\nHard to see from screenshot the solution * I wouldn't even consider using XCOM * You need to upgrade Airflow or check your logs. `python_operator` is deprecated.\nLife is exciting again...I am thinking things..and they are happening!\nVery interesting! Is there a URL/sandbox through which we can try this thing?\nDoes that mean my job is going to be taken over? Kidding, relax.\nHas anybody tried out how ChatGPT performs for converting various SQL queries from one engine to another? For example, I've got some T-SQL queries that I would like to convert to SparkSQL - and would want to avoid any hustle related to syntax differences, especially datetimes etc. We've got a use case at work as we work on 2 different engines (+ various data sources, incl. SQL databases) and it adds additional layers of tedium in QA process. I would try to build some simple CLI tool, or tkinter Python app for this if I manage to find time for it.",
        "content_hash": "aa63315a85df829e7a6fc47d2a2ec071"
    },
    {
        "id": "1h1s0md",
        "title": "I\u2019ve taught over 2,000 students Data Engineering \u2013 AMA!",
        "description": "Hey everyone, Andreas here. I'm in Data Engineering since 2012. Build a Hadoop, Spark, Kafka platform for predictive analytics of machine data at Bosch.\n\nStarted coaching people Data Engineering on the side and liked it a lot. Build my own Data Engineering Academy at [https://learndataengineering.com](https://learndataengineering.com) and in 2021 I quit my job to do this full time. Since then I created over 30 trainings from fundamentals to full hands-on projects.\n\nI also have over 400 videos about Data Engineering on my YouTube channel that I created in 2019.\n\nAsk me anything :)\n\nhttps://preview.redd.it/k8antmiuzl3e1.png?width=4032&format=png&auto=webp&s=d6156f36ab532ab6a81e3f0135239d164bdf4639",
        "score": 366,
        "upvotes": 366,
        "downvotes": 0,
        "tag": "Discussion",
        "num_comments": 168,
        "permalink": "/r/dataengineering/comments/1h1s0md/ive_taught_over_2000_students_data_engineering_ama/",
        "created": 1732784888.0,
        "comments": "Just came to say nice work I came across your site many times when I was getting in touch with a new tech for my stack Thanks!\nThanks! That's one of the reasons I do this. Not just for beginners, but for people who want to learn something new, but don't want to spend hours to get started. Not standing still is super important. It's the one thing I would do differently if I had the chance.\nI somehow was in that situation, when moving from BI-onprem to the cloud... and then's where your content was useful for a head start :) Thanks again!!\nNo questions. Have just enjoyed the livestreams lately. Thanks for your long term contributions to the community\nI'm doing one today. 10am EST. Just haven't created the event in YouTube. We've been uploading a lot of reactionary videos lately, because I like them. They help me get new ideas and people get a second opinion\nWhats the YouTube channel\nThis one: [https://www.youtube.com/c/andreaskayy](https://www.youtube.com/c/andreaskayy)\nWhen I started as a data engineer, there wasn't that much noise about spark, streaming and distributed computing. However, in recent times, I have seen those things as basic requirements for a job posting which engineers who are more seasoned on relational databases, find difficult to crack. I have tried to upskill myself but these technologies keep changing every quarter and it is hard to keep up to speed with the junior folks who have been exposed to these technologies from the start of their career. What is your advice for people like me?\nI come from a typical CS background with SW development, relational databases and computer networking. The shift for me started when I was working at my old job and realized that a relational database was not able to handle all that data. Not for storing and neither to extract all that data to building ML models by the Scientist that I was working with all the time. For distributed processing you have to move away a bit from the relational db topic. To be able to process large amounts of data you need parallel processing. Either with a NoSQL storage that supports this or with simple files in a data lake. This way the workers can access and also store files in parallel. How I teach this to my students is with setting Spark up through a docker container and then using simple files as data source and destination. It's most of the time anyway what you are going to do in real life. The data might then also flow into some analytics store like Snowflake, but for learning you don't really need this. I haven't tried it myself, but one of my guys is currently preparing a workshop where students learn spark through Google Colab.\nWho would you recommend getting into the DE field, Im currently a data analyst. But I feel overwhelmed whenever I start looking at the basic of DE\nHere's an example roadmap that you could do: [https://www.linkedin.com/posts/andreas-kretz\\_roadmap-making-the-jump-from-analyst-to-activity-7259953868267032577-xLfO?utm\\_source=share&utm\\_medium=member\\_desktop](https://www.linkedin.com/posts/andreas-kretz_roadmap-making-the-jump-from-analyst-to-activity-7259953868267032577-xLfO?utm_source=share&utm_medium=member_desktop) I don't know what resources you've used to try to get into DE. Just looking around on the internet will most likely not get you very far as it's not targeted. That's why I started my Academy in 2021. To give people a better way of doing this\nHey Andreas, just curious whether solely specializing in snowflake and dbt will open up ample opportunities for me in the future(next 5-10 years) ? Currently a 4 YOE and currently have not yet got any realtime exposure to Spark, neither DataBricks, only working on the warehousing side\nDo i have to be a Math genius / intelligent to have a career in Data engineering?\nI've never really been good at math. Always hated it, in school, in university as well. As an engineer you don't really need huge math skills. You can calculate percentages and averages? You're hired :D\nIf it's any consolation, the vast majority of mathematicians are also not good at math, that's why they study it.\nthis is one of the most puzzling comments ever.\nIt was a joke: nobody is good at maths, there are just people who love it enough to study it.\nNo you don't. Source : I'm a data engineer with marketing/communication background.\nAgree. Been doing data for a couple of decades and my educational background is philosophy. Data engineering and visualisation is an eclectic bunch; I have worked with ex-historians, ex-policeman, ex-musicians and ex-chemical engineers who all found they had a knack for handling sets and uncovering the message in the data. An active interest, willingness to learn, aptitude for methodical thinking and good communications skills (listening and speaking) are solid foundations.\nYou need good modeling/abstract thinking skills which may sometimes come as a bundle with math skills.\nI have studied fundamental physics, and none of the math is useful to me in DE. High school math is more than enough. However, being used to handle abstract concepts will help learn the plethora of concepts that don't stop popping up in data engineering.\n> Math genius Define \"math genius\", since that feels like a relative term. I bet you can survive on Algebra, and Relational Algebra (if you know SQL you're 90% there), unless you need to implement something more complex. Algebra can be learned by learning to program, though knowing it beforehand makes life easier, and Relational Algebra is the foundation of SQL - if you know about tuples and sets (knowing Set Theory here is a boon), you're 50% there. > intelligent Average IQ (100), should cover you there. If you're <80 IQ, then you may find yourself struggling way more. It's not impossible, but oh man is it going to be hard.\nHow are data engineering roles expected to evolve with the rise of AI, and what skills should one focus on developing now to stay prepared?\nPeople always talk like AI is going to replace us engineers shortly. That's not true at all. We're doing so much more than just creating code snippets. A good engineer is going to use AI for being more productive. Creating documentations, understanding documentations, cataloging data sources and integrating them in the data pipeline. Using AI to help with easy development help. So for simple things it's going to be super helpful, but for complex problems that need a lot of communication, understanding, flexibility and so on, it's not that useful. And not even talking about monitoring, bug fixing and so on. So, more a helper of help doing the annoying/boring work\nHi I'm a data engineer with 3 months of experience in india and would like to pursue more about data engineering. I just want to discuss the fee structure for the course.\nMy Academy? It's actually super simple. It's one payment for everything. You can choose 12 months access or unlimited access. Start with 12, because we have discounts for upgrades. Right now is the right time. We give 30% discount for black friday\nHow can I figure out if data engineering is the right path for me? Is there some kind of test I can take or a way to try it out to see if it suits me?\nI once had this test on the website. Try it out and let me know if it helped you: [https://forms.gle/ayxEFPcnBFAmAchZA](https://forms.gle/ayxEFPcnBFAmAchZA)\nWhat's your recommended tech stack for a small to medium-sized company?\nThat's very difficult to answer because it depends on what you are trying to achieve. On AWS, Lambda, RDS and Redshift, S3 and maybe API Gateway is a good start. You can get dangerous with this simple setup\nAs an fyi the link doesnt appear to be working!\nThanks for the info! Teachable recently changed something with the DNS. I might need to fix something. Added the full url\nHj Andreas your videos and lives on YouTube helped me when I was trying to migrate to DE. I was able to do it in the past few years and now I work somewhere that I never thought possible. Thanks for all you do.\nThat is so awesome to hear!!! This is the reason why I do all this :) Always believe in yourself and with discipline you can achieve unthinkable things \nHello Andreas, I want to start saying thank you for your contributions... I follow you for a long time. Besides the gratitude I want to ask you, what is a good book/resource about ML but for a DE that doesn't know too much about ML/AI?\nPuh, difficult. I'm not a book nerd. Let's look at it the other way around: What's your goal with this book. Understanding and implementing the process or being able to do ML (e.g. train models) yourself?\nUnderstanding and implementing a ML process. Being able to talk with a Data Scientist without understanding the math behind their data products. Maybe something more related to MLOps? (It could be any kind of resource, not only books) I hope I explained correctly... Thanks again!\nTake a look into Regression vs classification. You'll need this to understand what Scientists do with ML. Still 90+% is that: [https://docs.google.com/spreadsheets/d/16xxBKNd-OYIcAvgdTBx8Skpr\\_GMSi-\\_upkxp4Vaoap0/edit?resourcekey=&gid=1902589212#gid=1902589212](https://docs.google.com/spreadsheets/d/16xxBKNd-OYIcAvgdTBx8Skpr_GMSi-_upkxp4Vaoap0/edit?resourcekey=&gid=1902589212#gid=1902589212) For the general workflow I have something in my Data Engineering Cookbook: [https://github.com/andkret/Cookbook/blob/master/sections/02-BasicSkills.md#Data-Scientists-and-Machine-Learning](https://github.com/andkret/Cookbook/blob/master/sections/02-BasicSkills.md#Data-Scientists-and-Machine-Learning) For MLOps directly.. I need to do a bit of research\nHey Andreas, Hello from Malaysia. Data modeler with about a year of experience here. Will be moving into DE next year due to a re-org happening. Curious to know if you have any tips or specific strategies to land remote job / get a job overseas? Thank you!\nDo you already have the DE job because of the re-organization or are you being let go / do you have to quit?\nWill be moving laterally into the DE job in the same company, not being let go.\nI love your reactions video . Would love to invite you to my podcast when I get the time to set it up\nWhat's the link to your podcast?\nhey, i worked at bosch too, great company. question is: do i have to work with data analytics first in order to get some experience to work as a data engineer? i know they are different fields, but one of them is easier to get a starting position, then climbing the stairs.\nThat always depends on your skills. If you don't have computer science skills right now (like coding) Analyst is a good job to jump into the data area. Here's what I would do: \\- get a job as data analyst in an industry / domain that I like (and that needs engineering as well) \\- become good at the domain \\- Educate myself and move towards Data Engineering skills \\- Search a data engineering job in that domain People underestimate the importance of being able to understand the data, the propose and the typical problems that people have. Especially in the beginning is this important. If you have 10+ years of experience then it's not that important anymore, because with experience switching the domain will get easier\nI am currently working in power platform domain. Would love to switch as a DE have covered certain topics( Hadoop, SQL, Python). Would you offerings be fine for someone like me\nNot sure what power platform domain means. What kind of skills do you have? Can you code and work with SQL?\nIt basically comprises of(SharePoint framework, PowerApps, PowerAutomate) so basically Microsoft offerings. Yes I know SQL part of my work once included writing stored procedures for business use-cases automations.\nThat's already very good. We have Python courses in the Academy, but it would make sense to get a bit comfortable before starting with our stuff. Take a $10 course somewhere. Either udemy or codeacademy or somewhere else. This will make your life easier breaking into data engineering.\nIve been on a new team at my company for about 3-4 months now, working as BI Engineer after coming from a different team where I was just a BI Data Analyst. I feel my new team is definitely forcing me into a DE role with a BI specialty, but what advice would you have for a junior engineer looking to become a better contributor at work and their own career growth? Especially in terms of habits/routines/work mindset.\nI'm curious, is \"forcing you into a DE role\" good or bad in your view? In terms of growth: A good engineer is a problem solver. Embrace it and focus on helping solve problems. When you do that take ownership of the problem / the process / the product. Especially in larger companies you'll see that people shy away from this, which is annoying. One of my habits is to start every day with a list of topics / tasks I want to achieve. This helps a lot of getting more focused. It also makes sense to do this for a longer term. Think about your goals and how to get there.\nInitially I did have a bad view because I interviewed for just a BI Developer role and was a bit frustrated when I realized after some time I wont be doing the same work as my prior role. I had to adjust to more of a BI DE role now, along with an RTO policy and much longer work hours. Im stuck at my current company and role for now, but after doing some self-reflecting I try to focus on the positives of the skills and growth Ill have over the next year to help myself and my team/project better. So I would say I have more of a positive view now, which makes my work and life more manageable.\nHey Andreas, great content. I am also a DE with 4 YoE and I also happen to like to teach.\nGreat! Help some people! Do you have a blog or something where you share your expertise?\nI have started teaching within the company I work for. I am planning to start writing blogs also!\nDo that and share them on LinkedIn. Tag me on the post.\nSure. Thanks\nWell done. I've done lots of courses in related topics and promptly forgotten 99% of the content  use it or lose it I guess. Now I just produce reports using Power BI...\nYep, use it or lose it.. That's why it's important to know where you want to go. Otherwise you'll just look into this and that and forget about it later. What's your biggest problem when producing these reports?\nI come from a non technical background in business and work in finance. Is it possible for me to self learn the skills and tools necessary to become a data engineer? What should I be aware of and focus on that people with a more technical background dont have to?\nPeople underestimate how important domain experience is. So as long as you stay in that financial segment you should be able to switch jobs sooner or later. Start with SQL and learn how to query data and build a relational data base. Then do a bit of research which tools people use in that sector and focus on learning these + the basics that you come across\nI am working with boomi, axway and tibco. Will this experience help me to become a DE?\nData integration is a big part of what we data engineers do. Now, this is only the beginning of the data journey. Think about from your work experience where does this data goes next. What is done with it next? Which tools are used? Then add that part to your knowledge. You can also add this here and we'll discuss more of it ;)\nOk so beside these tools what else should I learn to add in my resume. I am btech grad so have a sound fundamental. Along with that intermediate knowledge of Power BI. Will taking certification of DP- 900 help me.\nDo DP-900 and DP-203 if you want to get into Azure. Then build a small end-to-end project with this knowledge. Maybe start with an ETL job with Data Factory, extracting data from an external API, writing it into a relational database and visualize the results with PowerBI. Add synapse and blob storage and please document the project in a GitHub repo!\nI have a DS background and work experience, currently I am doing a lot of data engineering tasks in my job and my side project. I am creating and ingesting data into postgresql I created, and also making ETL pipelines, and the data ingestion and ETL will be local, while hosting the final database in the cloud. Also working on automating scripts with a task scheduler. I may eventually swap out the local components to cloud eventually with snowflake and DBT labs. It may be a vague question, but should I be learning or focusing to further my skills in data engineering?\nSeems like you already do a great job!! Yes, moving everything to the cloud makes sense. Focus on that, building those skills is very important. Do some research though: \\- Postgres is a transactional database, Snowflake is an analytics store. Research OLTP vs OLAP \\- dbt is used for ELT jobs where you transform the data after it's stored. You might not need that If this works as it is right now just lift everything up into the cloud using the cloud services. Then iterate from there.\nI am doing graduation in AI/ML, how woud you say and if being good in data engg. helps AI scientists or ML engineers as that is my ultimate goal, but I have an interest in DE and want to start my career in the same and then move up the ladders.\nLook, think about the responsibilities Scientists / ML Engineers - creating good analytics results. Now, somehow that data needs to get from the source to them, and it needs to have a good data quality. That's where the engineering comes into play. Setting up the platform, building the pipelines, automating everything, monitoring, bug fixing. Making the data available for the people who work with it. Don't worry about moving up the ladder. At this point your goal is to get into the game. Everything else will naturally happen if you do a good job.\nHi Andreas! Just wanted to share that I love your style of videos. Calm, thoughtful, and without fluff. Ive been enjoying your reaction videos lately, great content!\nThank you very much! Feedback like this makes me happy :)\nDo we have to buy the academy package to have access to the discord and get mentorship?\nThere's a free portion of the discord. Check the main page, there's a link towards the bottom of the page. But that's more for community and you are not going to have access to the Academy or coaching channels\nWith AI coming into play, how do you think the data engineering roles are going to evolve? Because what I'm seeing is that corporates or big companies don't allow using AI in DE work, but DEs at startups do use AI to optimize their work.\nFor now they might not allow it, but what these big companies are doing is hosting pre-trained models (or often even training their own ones) within their data platform. This way they are not sharing their IP with big tech. AI is here and it's not going away. You already said it, people are using it for optimizing the work / for boosting productivity.\nI recently noticed that many course creators are using large language models (LLMs) alongside their course videos. This allows students to interact with the LLM as if they're having a conversation with the instructor. Its a great way for students to get answers to their questions without having to wait for the instructor. Besides that, It also helps course creators avoid answering the same basic questions repeatedly. Feel free to reach out, If youre interested in having a trained LLM for your own video content (fully owned by you)! Ive built several LLMs for similar use cases and would be happy to help:)\nSnake oil.\nI'm brand new and know nothing, moving into DE soon and will hope to learn from your material. Thanks\nLet me know if you have questions! We're here for you\nStumbled across your yt page today and love the content, energy, and approach Ive seen so far. Will definitely check it out!\nthanks man! Sometimes it's difficult as a non native english speaker, but doing my best :)\nJust wanted to say hi Andreas, you helped get me my first job during COVID. I have a friend who just signed up for the course about 6 weeks ago. He's in looking at career transition like I was. Just want to say Andreas talked me out of doing highest price product, he thought I should do the self directed academy and that saved me some money. This guy really does want to help students out.\nOh man, I'm happy to hear that I was able to help you get a job! Big congrats :) Support your friend and keep them accountable to do something every week. They'll make it too.\nA non CS student who has 4+yrs of data Analytics experience, good understanding of sql, python , how do I switch to data engineering ( currently working on building a data pipeline that is at a relatively larger scale that my usual work using databricks)\nJust keep doing what you are doing. You don't necessarily have to switch to data engineering. Make yourself more useful like you are doing now. Being able to build larger pipelines instead of just analyzing data on databricks is the perfect direction. Take courses, read books, do research on topics that you don't understand.\nHi Andreas, thanks for your offer. Technical Project Lead here for software components in a complex product , already 44. Looking for a career change to keep my sanity. Electrical Engineeing background, MBA, not too much hands on experience (can use git, wrote Python scripts in the past (though for sure not proper for CS/SWD/architecture standards, toyed a bit around with sql if excel wasnt sufficient for data amount, tons of architecture and sw issue discussions). In general I would like to do more technical work and DE/DS looks quite interesting. For my situation it also looks more manageable to achieve, than becoming full SWD (also no interest from my end). I am not 100% sure yet, which of the roles DE/DS is more interesting for me, and what would be a better fit for me to catch up. Wouldnt mind going full stack, but where to start best?! I am in a big, german, corporate so plan would be to get some more profound knowledge and then find an internal job/team which would be willing to support that change. Anyhow I am not sure if all of that is feasible at all at my age (also considering younger competition and having job opportunities in case of company lay offs). Some thoughts would be appreciated. Thanks\nWhat you should do is to get some DE/DS experience that will enable you to lead these kind of projects. Not make a switch into development yourself necessarily. Use your experience and add data topics on top. Stay at the company where you are right now where you already have connections with people. My coaching program can help you a lot for this. We'll figure out a plan together to get you focused. We can talk every week about important topic, you'll also hear other people's problems / questions and you can get direct feedback on your journey for 6 months. Ask your manager if you can get reimbursed through the company from the employee training budget.\nJust thinking aloud... This approach sounds more rational for sure. Still torn to get things running on my own, hands on. In any case a topic change would be good. I am really slow on changing positions, as I also lost kind of interest in the series development we do and doing similar projects just for another sub product did not catch my interest at all. So staying where I am was good enough. Then you have everyday life (family etc) and are happy enough if your job is running smoothly (line manege, colleagues, pay...all great...). Doing something new also means invest. That being said, I am at a point now where I really know I don't want to do my current job for another 20 years and kids are at least a bit more grown up now (12/16). Then I was searching a bit more actively and DE/DS catched me. Focus is a good point. Let me think how to start 1. Get a better understanding of the actual DE/DS work. Maybe until end of the year. Idea is still quite new and I just stumbled over this sub during collecting infos. 2. Get in touch with departments doing DE/DS work and get an idea if and how they would see me fit. There are actually some (internal) positions - this sometimes has the benefit that chances are better for non subject matter experts. 3. If there is any path to continue consolidate how to get up to speed with trainings&co. Bit eager to press the training button already (Black Friday ) but involving company sounds also more rational.\nWe'll, you need to figure out if doing something different is a good idea yourself. Make a decision and then act. One thing I can promise is that whatever you'll focus on will be beneficial. Just don't half-ass it. Start a bit differently. 1. Combine your step one and 2 together. Talk with people doing that work to understand better what they do and who the'll need. Don't talk about you for now. 2. Use their needs as a learning plan for you. 3. Select resources to strategically learn these topics. 4. Pitch yourself to them by asking if you would fit. 5. Forget about Black Friday sales for now. You need a plan first. If you find out that we can help you send me an email (link is on the homepage) we'll work something out. If you can find in step one someone who you can talk to and who's interested in helping you along that would be awesome. Just don't be pushy. See who you \"click\" with\nHi Andreas. Came across your page a few weeks ago. Cool stuff. Just a general question: Im currently a college student and still got a couple of years, though I love data engineering and can see myself in the field in the future. Do you have any general pieces of advice moving forward for a college student like me? Also, is data engineering a field that one goes into after a few years in some other field (data analyst, backend engineer, etc.) or can anyone hop into the field immediately?\nData Engineering doesn't require you to do data analyst or backend engineer work first. I highly recommend that you use your free time now at college to learn to code and gain CS fundamentals. These topics will be always useful for you.\nGot it. Thank you!\nhi Andreas, cool to have come across your content and AMA, what would it be your suggestion(s) to somebody wanting to freelance as data engineer? thanks a lot for putting valuable content out there, keep up the great work!\nSo, I guess that you have experience with Engineering. Yes, create good content that helps people and put it on your LinkedIn profile. That's where people are looking if they want to hire you. Create a portfolio of end-to-end projects that you can showcase. Get yourself a website where people can learn more. The more information out there the better. Then start reaching out to people. Getting that first job is the most important, so don't go too hard on the price.\nthis is pure gold, the guidance is really appreciated \n\nHow do you reconcile your lack of working at a FANG company with selling a course to work at a FANG company? If thats not disingenuous, please explain why.\nI never said that my Academy is specifically for landing a job at FAANG. Wo gives a shit about that? As if the only place you can be successful and do interesting work is at these companies. People try to get into these jobs for two reasons: trying to make big bucks and having the company names in their resume. (I always have to lough when people put \"ex Google\" in their bio). I just can't understand why boasting about being a cog in the big machine is the big goal. In many companies people have the freedom to start from a green field and actually being able to make a big difference. Generally, I rather teach helpful topics, than making promises where you'll get a job. Hell, I don't even give guarantees that you'll get a job. That's the most fake marketing scam ever. Especially in the current economy.\nI'm curious if you or any others here know of a similar course offering/structure for ML/AI/RAG work. I have a good skillset in DE but am a bit lost on getting started in the AI space alongside it. Love your work, have used it plenty to stay up to date and refresh on platforms I haven't used in a while.\nThe only guy I know is Andrew Jones with Data Science Infinity. I can't attest to how good the program is, but I have talked with him a few times, also on one of the live streams, and he's a good guy. I also have the module \"Data Preparation & Cleaning for Machine Learning\" in my Academy, so I trust him.\nWhat do you think about the saying \"those that can't do, teach\"?\nHahaha. That's actually a problem I'm fighting, because the longer you are out of working at a job the more difficult is it to know what people need and how things work. I try to solve this by: \\- Listening to what people need on LinkedIn and other social media \\- Going through job descriptions to to look for requirements \\- Having people work with me on courses and the coaching who are actually at a job doing engineering \\- Listening to the input (problems and goals) from coaching students to stay on the pulse of time \\- I'm also currently in the process to talk with people about recruiting. Not just for placement of people, but for better understanding which people companies need and what their responsibilities are Bad teachers will not do this and then the saying becomes true.\nHello, I am in analytics engineering for abt 1 year. I want to be good at optimising spark SQL. What are the things I should know and what is the best way to learn how to read the spark physical plan to know where are the bottlenecks in my spark SQL script?\nIf you are looking for Spark optimization then I recommend you get yourself a book like this: [https://www.oreilly.com/library/view/high-performance-spark/9781098145842/](https://www.oreilly.com/library/view/high-performance-spark/9781098145842/) You are in a perfect position, because you can actually analyze your queries that you do at work. Doing this in a synthetic example is quite difficult\nHi sir. Is it possible to transition from Data Analytics to Data Engibeering. I am very good in python and its data analysis frameworks, am good in sql and databases, and also am familiar with AWS.\nYes, 100% I have many people like you in my Academy and the Coaching program. Without knowing much about you, the main thing for you should be looking for is working towards being able to bill end-to-end pipelines on AWS. Basically getting the data towards the Analytics that you are working on already\nI had got a full stack dev role 4 years to pay college fees I didn't love it. I personally Don want to work in Javascript, typescript and react or any frontend framework so I quit my job and I had 2 questions do I will need to use js or react in de role as I dont want to use that. I worked in 1 project where I used airbyte to ingest data from jira and used cube js as semantic layer and provided rest api to frontend dev can I count this project as related to DE?\nMost of engineers are using Python nowadays, but js can help a lot especially with API creation (although there are good solutions for Python). That project sounds a bit weak on the transformation and data storage part. Look into Spark and a NoSQL database for instance that could be a great start. Extract the data from an API, store it somewhere (AWS S3 or locally) then use Spark to process the data, put it into MongoDB and use an API to query the data... Think that's a cool project for beginners. You can also run this completely without the cloud\nAre you saying we'll evolve into DE roles like Pokemon Next you'll be dreaming in Hadoop not color\nYou'll be dreaming of AWS, Azure or GCP. That much I can promise \nIm having a db thats getting to big, the data wharehouse lakehouse stuff is confusing how can i easily get started on my k8s cluster? (Db has tables with users sightings (x,y,z coord, date, gear_ids) in a video game ~4000000000 records, 1Tb and one for highscore data, ~9 million new records each day, 30 day retention, 80 columns int size. Currently data comes in with kafka, worker container puts it in the DB.\nDepends totally on what your goals are with this. Is this pure analytics or is this for some kind of transaction / game mechanic relevant? If it's purely analytics then it can totally make sense to just drop it into files and then query them with a query engine like AWS Athena / Presto. Research OLAP + OLTP\nMy goal is to create reports, such as a heatmap of bots we detect, and train our classifiers to detect bots with ofc some feature engineering. The limitations are that we are an opensource project woth no barely any funding so we are looking at opensource solutions that work on our small k8s cluster\nWhy do the germans prefer language proficiency over technical proficiency for jobs that are in Denglish anyway?\nThis is a difficult topic. Yes, most communication, documentation and code is in English anyways. Back when people worked in the office the problem was always that the Germans are going to speak German with each other. So if you can't understand it you are isolated from everyone else in the office. I imagine it's the same in any other European country though. I also remember times though when people were annoyed that they have to speak a second language in a meeting with 10 people, just because one person only understands english.\nHello I'm currently intern at Bosch Viet Nam my role is Low Code Developer. So my question is about how my role at present can be useful at Bosch as a Data Engineer. I really wanted to be a Data Engineer and what specific tech i can study about it and the project i can practice. Thanks you so much\nThis is great. Will save this and will come back for questions!\nFor fun and what we always ask on interviews - SQL or python ?\nBoth super important. You need to know this. Maybe practice with leetcode a bit..\nExactly its literally how we find out if the person has just read a book or is answering based on experience\nYeah, you can very easily find out if someone has really coded or not. I can pinpoint this without 2 minutes\nJust want to say $264 for 12 months is incredibly affordable compared to literally anything else I've seen in the space. Props to you, man.\nGood education should never be super expensive ;)\nHi, i just want to step into de. Are the some fundamental things i need to know?\nPython bascis and SQL bascis. You don't need more to start. People make a big thing about requirements, but it's not terrible.\nSAP BW ETL engineer with 14 years of experience data engineering recommended or not to pivot ??\nAshish, I answered both questions on the live stream. Just go back and scroll a bit through the timeline. The questions are displayed in the bubbles. I definitely took my time with this. Look at timestamps 43:45\nLoved your answer !!! Thanks , wish you god speed ![gif](giphy|9PyhoXey73EpW)\nWhat are the platforms used by the data teams at big tech or SMEs? Or do they build softwares in-house? Thanks\nWell if you're talking about Microsoft, AWS and GCP. Of course they are going to build on top of their platforms. Would be stupid for them not to as it's basically free for them. Most\nwhat about smaller companies?\nDifficult to say what they use. Some of them are very hyped about using GCP as it's very simple and has great services. Market share wise AWS is still strongest. You can't go wrong with AWS, then Azure, then GCP\nWhat is the story behind GitHub cookbook, did you develop the cookbook and academy in parallel?\nI actually started with the cookbook. I wanted to create a resource that has everything in it that someone needs to get started. I still keep it updated. just added two updates this week. Unfortunately I don't have enough time anymore to work on it every day. But teaching through a book is always difficult, especially if you want to show stuff to people. So, because I already did YouTube and coached people in DE, building my Academy was the next logical step.\nGreat to know, thanks!\nAbout to graduate this fall with a B.A. in MIS (Management Information Systems) degree. The program was a hybrid of business and some database classes, but more so towards business. I would like to get into data engineering, but to ensure I can get an entry level role, what are some more skills to develop. Would SQL be enough at the start? Do you have a video on entry level roles?\nRemindme! 12 hours\nI will be messaging you in 12 hours on [**2024-11-29 10:08:15 UTC**](http://www.wolframalpha.com/input/?i=2024-11-29%2010:08:15%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/dataengineering/comments/1h1s0md/ive_taught_over_2000_students_data_engineering_ama/lzh0wy5/?context=3) [**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fdataengineering%2Fcomments%2F1h1s0md%2Five_taught_over_2000_students_data_engineering_ama%2Flzh0wy5%2F%5D%0A%0ARemindMe%21%202024-11-29%2010%3A08%3A15%20UTC) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201h1s0md) ***** |[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)| |-|-|-|-|\nRemindme! 48 hours\nI am pretty new to it field although I have experience of working aa a software QA and have been laid off 1.5 years back . Will this course help in landing a job ?\nOne of the goals of the Academy is to get people job ready. Content wise there's more that you'll need to land a job in the Academy. The problem is that you will have to actually go through it and do the work. That's where most people are lacking. Lack of effort. Are you applying to jobs? In what frequency? For which jobs are you applying? Are you getting invited to interviews? You might want to optimize your CV and change something in your strategy. Get the Academy and take a look at the content. We give 14 days full refund. Start with the Basics, Python for Data Engineers and Docker, then do the Module to Platform & pipeline design. Focus on Data Modeling (we have 3 courses there that will help you) after that start getting into one of the platforms. Maybe Spark + Databricks and try to apply your QA knowledge and processes to these. That will help you a lot\nI'm a DE Intern at a Major Tractor company and a grad student who is graduating in May 2025. I'm currently pursuing a masters degree in Information Management. I have a total of 9 months of work ex as DE where I didn't do much work other than some monitoring stuff and building a POC. In my current internship, I have built over 10+ end to end pipeline on AWS, also I'm a AWS SAA certified. 1. Do you have any advice for me in navigating the current DE Job Market? 2. Also, should I give the Snowflake / Databricks Certification to improve my visibility? (I can't think of myself working in any other role other than DE.)\nI would keep focusing on AWS. Are there tools that you have not worked with, like Redshift? Glue? Try to use that knowledge that you now have from the internship and build a personal project that you can actually show online with these tools. Building a portfolio is always important. It also enables you to talk often about topics that you would not be able to otherwise. In short -- double down on AWS. If you have the time and you are actually enjoying it then look into Databricks and Snowflake certs as well. They never hurt\nI am working primarily on Azure portal using services like ADF and Synapse but I wanna explore more about Big Data processing using PySpark but its kinda boring to learn theoretical concepts. Can you suggest what kind of projects should I make so I can learn as well as I have something to show while applying for a job?\nBuild this pipeline: 1. Extract data from an external API with Data Factory 2. Store it into Blob storage 3. Use spark to process that data on a schedule 4. Put the results into a Synapse You can also just start with learning spark and processing data files you find on Kaggle. Always try to build small end-to-end projects\nThoughts on someone that started in the hardware side of things (two years at Intel), and now is preparing to become a data engineer full time in 6 months, is it possible? What would you advice for interviews?\nAwesome\nDo you have a course on Udemy I can buy?\nI only have my Academy & Coaching on [learndataengineering.com](http://learndataengineering.com) Don't do Udemy courses\nWhat's your first job in the data field have you ever taken ? Did you get a Data Engineering job at first or another role in the data field ?\n\\- My fist IT job was as a support guy running around the company helping over 200 people with all kinds of computer / software problems \\- While studying I worked as a computer network technician helping to set up + renew the networking infrastructure for company locations \\- As 6 months thesis I worked on creating the design and proof of concept for a condition monitoring system for machines. \\- After university I worked 1 year as a SAP consultant (mainly development & customization) that was terrible \\- Then I got into my old thesis topic. Turned out to be a huge project with a lot of data that \"normal\" systems weren't able to handle. Also just monitoring the conditions turned into predictive analytics of when something will stop working. So I had to move towards Hadoop, Kafka & Spark. Was super fun, because it was like a little startup until the corporate guys came in. I then jumped off and did other things.\nHi Andreas, I come from a background in robotics and industrial automation (PLC programming, instrumentation, and control) with 3 years of experience. Additionally, I have learned intermediate skills in Python(Pandas, Numpy, Scikit-learn, Matplotlib) and SQL, PowerBI, and some backend experience with Django (tutorial) and FastAPI on my own, outside of work. Additionally, I have some experience in analytical projects. Do you think your program can help me become a data engineer given my background? And what types of projects could I create or focus based on my experience? Thank you!\nHi Andreas, Glad I stumbled across your post and your course. I've had a look at all of the subject teached in LDE's Catalog. My current position is a BI Developer using Microsoft stack such as SSIS, SSAS, SQL Server based Data warehouse, PowerBI, Multidimensional Olap Cubes. Our company is in the very early phase of adopting Cloud based technologies. There is a dedicated Cloud team responsible for identifying the solution that we will be adopting. They've gone back and forth between GCP, Azure. From what I've heard is AWS and Databricks will be the solution moving forward with details/components to be finalised. There will also be a plan for my team (Data warehouse) to leverage of the chosen technology to move into a modern data warehouse using AWS. In your opinion, will signing up for LDE help me with my current workplace situation. Will the lessons I learnt be enough to help the Cloud team and also my DW team?\nHow can I break into Analytics engineering as a data analyst?\nHow do you keep yourself up to date after you switched to full time teaching?\nDo you plan on going into Iceberg and open data tables?\nI' guessing you mean iceberg and delta tables. Yes we're currently looking into that. Iceberg tables would fit very well to our Snowflake training and delta tables to our Databricks one. Don't have an eta right now, because we're working on Terraform with Azure, Platform & pipeline design and other topics first\nPage Doesnt Exuist currently\nThink Teachable messed something up. I created a ticket. Thanks!!\nThank you for your work \nHello I'm currently intern at Bosch Viet Nam my role is Low Code Developer. So my question is about how my role at present can be useful at Bosch as a Data Engineer. I really wanted to be a Data Engineer and what specific tech i can study about it and the project i can practice. Thanks you so much\nWithin larger companies it's always a good idea to check the internal job portal. You might find something there. Depending on your definition of low code developer your next step might be more of a software developer role though. Maybe take a python course to get started. Then relational databases.. Take that step and then move to another role. Either way a good idea is to get started on the cloud. Try to do the beginner certifications for AWS or Azure.\nThanks you. Sorry I have one more question. I am planning to do my masters in Europe and when I get there Bosch Europe is where I want to work. Can you give me some advice or experience when I apply in there.\nI think studying here in Germany is quite simple for foreigners. A lot of people do it. I'm not sure about the process, but Google should be your friend there. Keeping your job through this might be more difficult, but very important. Maybe ask HR if there are possibilities to do this without quitting. These large companies should be able to help you there.\nHey Andreas, nice to meet you. I was just wondering if there will be remote opportunities in the Data Engineering domain in EU countries, open to discussions with professionals like me from countries like India. I would like to know your thoughts on this. Currently, I have over 2 years of domain exposure from a service-based company in India.\nmodern data stack is said to be modern but it does not have many functionalities that tools like SAP provide out of the box",
        "content_hash": "6dae7302ec6e5946aeec4a97ae08e888"
    },
    {
        "id": "19bg4jf",
        "title": "I\u2019m releasing a free data engineering boot camp in March",
        "description": "Meeting 2 days per week for an hour each. \n\nRight now I\u2019m thinking: \n\n- one week of SQL\n- one week of Python (focusing on REST APIs too) \n- one week of Snowflake \n- one week of orchestration with Airflow\n- one week of data quality \n- one week of communication and soft skills \n\nWhat other topics should be covered and/or removed? I want to keep it time boxed to 6 weeks. \n\nWhat other things should I consider when launching this? \n\nIf you make a free account at dataexpert.io/signup you can get access once the boot camp launches. \n\nThanks for your feedback in advance!",
        "score": 359,
        "upvotes": 359,
        "downvotes": 0,
        "tag": "Discussion",
        "num_comments": 189,
        "permalink": "/r/dataengineering/comments/19bg4jf/im_releasing_a_free_data_engineering_boot_camp_in/",
        "created": 1705769710.0,
        "comments": "Would data modeling be covered at all?\nSecond this.\nThird this.\nFourth normal form this\nBoyce-Codd this\nSecond normal form this\nIm interested, but an absolute beginner. Can i still Join?\nThats the idea\nI would love to join as well.\nGreat. Im in.\nyeah im in also. sign me up.\nIm inn too.\nimo there are plenty of resources for learning sql and python on yt so why not focus on data engineering aspects like Maybe Working with data from multiple sources/formats Rest API(like you mentioned) Data modelling (like basics concepts one should be aware of)\n++\nGreat...do not we need spark session, when talking about data engineering\nDepends. Ive found teaching spark to be a shit show for people since it involves a lot more setup. Or involves free trials and I hate giving data bricks free press.\nPySpark is the one constant that I encountered when interviewing for DE positions. It's table stakes for a job in the role. Edit: if it means a student has to spend a few bucks on cloud infrastructure to complete the coursework, it's worth it.\n> if it means a student has to spend a few bucks on cloud infrastructure No it is not. Everything that can not be run easily on users hardware puts a barrier in place. Just take a look at hardware suggestions for deep learning, Google colab is much cheaper than a gtx3060 but for some reason people have a mental blockage to go with subscriptions.\nIf the choice is, \"a small barrier to learning what you need to know to get a job in the field\" versus, \"don't teach a skill needed to get a job in the field to avoid putting a barrier in front of the students,\" which choice would you want the instructor to make? Completing a course that omits relevant information has little value. The students who are going to not complete the course due to a small barrier probably won't make very good data engineers anyway.\nNothing against, but you could just set up IntelliJ to run spark dependencies with providers and use it to test any spark commands on scala (for spark porpose only). It runs locally on the machine without extra setups\nYoud be surprised how many students are bad at installing Java, or their laptops dont work.\nyou could use docker to setup all the required dependencies and simply run spark inside docker.\nFor what it's worth, every data engineering interview I had in a recent job search asked me about my PySpark experience. Every single one of them. I don't know what your goals for the course are, but if you are attempting to give your students skills they need to get a job in DE, I just don't see any way you can omit PySpark (and DataBricks) from the course materials. Yes, your students will have to jump through some hoops to set up an environment they can use. Yeah, they might have to whip out a credit card and pay for AWS/Azure/GCP resources to do that. They might have to install and troubleshoot Docker on their local machines. But a student who is unable or unwilling to do these things is probably not someone who's going to be a very good DE (or isn't ready to start that journey yet) anyway. Again depending on your goals, it could be argued that those aren't the students you should be targeting for your course. As I said in a separate comment in this thread, if the choice is, \"a small barrier to learning what you need to know to get a job in the field\" versus, \"don't teach a skill needed to get a job in the field to avoid putting a barrier in front of the students,\" which choice would you want the instructor to make?\nWould you be against making an optional module that would cover that (for those of us that may not be strong in Data engineering but are capable of setting up Java/packages/etc)?\nYou dont want to give databricks free press but youll give snowflake free press? How does that make any sense lol\nits more like snowflake comes with a 300$ or whatever free compute. whereas databricks is free for 14 days but you still end up paying cloud costs if you choose to run anything more than the cheap ass community edition version.\nMaybe because theyre easier to do business with?\nIn what way? Curious to learn\nWhat's your beef with databricks? Vendor lock in?\nI would like to join the Snowflake section.\nFree huh. \nEverybody helped him design his course because they're so desperate to break into DE and think he's actually going to help them. Turns out it isn't free. This sub got farmed. Classic.\nThat's my interpretation too \nIm a bit behind on this. Itll happen\nHi, i made an account long back and still havent received any free boot camp\nGood for you. It got postponed\nI love your content. Any timeframe on when you might launch this.\nI think instead of spark or any data processing tools, it might be beneficial if you can briefly talk about distributed systems.\nVery interested.\nI am in.\nHow can I sign up?\nMake an account on dataexpert.io/signup and Ill be in touch. Theres a bunch of free content already there but Ill be adding an opt into the boot camp in the next few weeks once its finalized\nA beginner.. hope this helps land me a job.   \nIt is Zach Wilson SHIUUUUU. I am following you on LinkedIn bro\nIm interested! \n\\+1 sign me up bro.. thanks\nI'm in\nwould like to help. maybe i can cover a week of distributed storage or computing on aws and/or data infrastructure options\nsomething about warehouseng, lakes and big data workloads\nSounds great, I'd be keen to join\ndata engineering for data science models vs bi/reporting - one needs more flat tables vs data warehousing/modeling concepts. may not need a whole week, but could be covered as part of snowflake. still several power bi/tableau reports are built with flat tables, and its a nightmare to maintain, performance issues.\nInterested too\nUpdateme!\nLooking forward to it\nIm interested haha. Love from vn\nHey Zack, do follow your work on LinkedIn. Though familiar with the topics, will absolutely love to take part in it. And yes, just do what you stated and let beginners follow the basic. We lack people whose basics aren't clear.\nHow can I join your bootcamp?\nMake an account at dataexpert.io/signup\nOoohh I'm interested!\nDamn, I am in!\nIm in\nPlus normalisation please\nThis is great!!\nhow to join\nWhat about dbt ?\nWould really like to sugn up! Im currently a data scientist / researcher that wants to learn better practice and make our ML engineers' lives easier.\nIm interested, will you post here when launching the bootcamp ?\nNo. Join dataexpert.io/signup to stay up to date with\nThis is offtopic from your original ask; I'm a staff full stack engineer and I've been wanting to start a BootCamp but I don't have any kind of following. If you are interested in branching out your bootcamp to include fullstack topics I'd be interested in partnering.\nPlease message me. I'm building a platform for this exact use case\nI dm'd you on linkedin\nI'd like to sign up! I'd like a topics on data modeling and star schemas please.\nI think continues steaming needed like Kafka and cloud technology like one of big three.\nIts free. Theres tons of free content on dataexpert.io already if you sign up\nI am interested\nI'm interested!!\nOi I'm down to trial this ASAP as I need to learn it for work. Willing to be a sounding board if you have any of this together\ndataexpert.io/signup has tons of free content already to learn from\nLooking forward for this\nBrief overview of different cloud services and how DE is utilized within them. (AWS, AZURE, GCP)\nThis would be very high level with links to each cloud providers DE specific certification.\nI would love to join\nThanks for this!\nIm Intrested\nAdding some sort of discussions about building data platform using K8s and Argo would be beneficial as well\nI am very interested in this. I am going to be doing more ETL, and data cube building soon and I come from no experience with SQL. My team is so lovely and they are taking chances with me. I really want to do well, but it honestly is hard for me because I do SQL ETL currently at 20% effort. I think this would be really helpful and I am looking forward to it.\nI would love to be part of this boot camp! It sounds amazing.\nI'm interested about this bootcamp.\nYou the man Zach! I have followed your journey and its amazing how much you have accomplished!\nThanks so much!! I cant wait, Im looking forward to it :)\nI'm definitely in\nInterested.. let me know the details\nThis guy is an influencer, not a DE\nyou don't know what you're talking about\nI did 9 years of data engineering from 2014 to 2023 at companies like Facebook, Netflix and Airbnb\nHe sell courses now \nAbsolute beginner. Im in! Are there any prerequisites?\nPySpark. Databricks. EMR. dbt.\nAm interested and have made a free account\nMeeting two hours a week for an absolute beginner doesnt seem like enough to get much done in six weeks. Edit: whoever mass downvoted this comment section is really cute but yeah. 12 hours isnt enough to cover basic Python concepts past maybe recursion. Certainly not enough to cover the idea of functions and passing arguments, pointers, wildcards, argument expansion, etc. for someone who is unfamiliar with the concepts.\nIt's designed to be part of his sales funnel. Not actually be useful.\nIm asking the community of Reddit. If I can get more community support, Ill make it more comprehensive. So if you want to pitch in, let me know\nThe learning experience simply doesn't matter and you've made it very clear. Let's say what this is - it's a sales funnel. The person I replied to is 100% correct. The amount of time spent on these skills will amount to nothing, so what's really the purpose of this course? No prizes for guessing. Anybody can tell this level of course, even if free, is garbage tier content designed as a way to upsell paid material to their target audience - people desperate to break into DE who are stuck in tutorial hell and completely unaware they are. > If I can get more community support, Ill make it more comprehensive. The community has asked for Spark and data modelling which are completely reasonable asks. Asks which you literally invited. In response, and like every influencer offering courses, it's pretty clear that making this course benefit people isn't very high on your agenda. You have said you are not teaching Spark because [the setup is annoying and you don't want to give free press to Databricks](https://www.reddit.com/r/dataengineering/comments/19bg4jf/im_releasing_a_free_data_engineering_boot_camp_in/kirflqg/). Fair enough, your course, your choice. You'd expect somebody of your alleged caliber could make teaching Spark a bit more simple although that doesn't appear to be the case which, in my opinion, wouldn't bode well for any of your paid content because your material is clearly only aligned with who gives you the most lip service. Case in point: cool with teaching [Snowflake though because they're \"easier to do business with\"](https://www.reddit.com/r/dataengineering/comments/19bg4jf/im_releasing_a_free_data_engineering_boot_camp_in/kisbd9g/?context=3) despite literally no absolute beginner needing to know Snowflake and if they did, they could find a literal 27 part long video playlist for free on Youtube. Data modelling was also requested. In fact, it's the most requested topic on here by the community. Your response? [\"Yall can join my paid boot camp for that\"](https://www.reddit.com/r/dataengineering/comments/19bg4jf/im_releasing_a_free_data_engineering_boot_camp_in/kis0xha/?context=3). That being said, feel free to prove me wrong. Go out of your way to add Spark and the data modelling part of your bootcamp to the free course.\nI will prove you wrong. But please dont join. Your attitude is trash\ni wanna join!\nGreat, Count me in please\nNice, country with me! Also ETL would be great.\nim interested!\nIm interested!\nIm in\nWhat about data warehousing? Also add in a real time streaming project covering the topics you are teaching.\nSkip sql,py as lot of content is already available.skip directly to core topics like orchestration,ETL and more.\ndoes one week of data quality include tests in the pipeline?\nYeah it would\nLooking forward to the course. Thanks!\nAm I late for this?\nIs this still available?? ;-;\nWould 1 week of python be enough for a beginner that only knows how to output hello world?\nHow about something with cloud? AWS, Azure, GCP\ninterested\nInterested\nCan you cover data modeling\nInterested\nI will join!\nI want to join!\nOltp to analytics\nI also suggest the following additional topics: Data Modeling and Architecture Intro to DynamoDB, Kafka\nSecond data modelling and architecture\nYall can join my paid boot camp for that . I cover all of that in my paid boot camp.\nI m interested\nInclude a week about the job search and what interviews are typically like\nI cover all interviews in my blog at blog.dataengineer.io\nThat may be the case but I think you should include it in your lesson plan\nSIGN ME UP\nI'm in.\nIm interested!\nI wanna join!\nInterested!\nIm interested to sign up for it.\nIm in!\nInterested. Lmk how to sign up\nDefinitely interested\nInterested!\nIm interested\ni would recommend this pattern 1 week of each \\-PY \\-SQL \\-Snowflake \\-databricks/spark(prefer spark) \\-kafka \\-airflow \\--cc \\-Modern Data Stack \\-atleast 3 hands-on projects for resume\nI am very much interested in this. As a student, the main resource i find lacking in the internet is a proper cloud based data engineering tutorial/intro. It would be awesome if you could squeeze in that as well.\nNice! Looking forward to it.\nData warehouse design\nI want in!\nPlease add data warehousing and data modeling, I will even pay for a premium account if there is one.\nAlready have 15 hours on data modeling in the premium boot camp\nIs this bootcamp free to join?\nI'd recommend spending time on key concepts. Batch vs streaming, OLTP Vs OLAP, dimensional modelling Vs OBT, the purpose of orchestration, etc. I think from a tech point of view covering SQL and python is great but beyond that diving into Snowflake, Spark, DBT etc may be too specific. Absolutely talk about these specific technologies in terms of basic concepts, what they offer and how they differ, but it's totally possible to be a kick ass DE and use none of them. For a boot camp, fundamental concepts are crucial IMO.\nIm in!\nAdd data ingestion with dlt :) makes it easy for beginners to apply best practices and has a very shallow learning curve\nThis is the biggest botted/shilled post Ive seen in a while, the comment section is filled with random people exclaiming that theyd be joining in the most generic way possible. Its like you cant make this up\nI'm selling a book on \"How to sell books for 300$\" - sign up now only 299$\nThis is free\nLink\ndataexpert.io/signup",
        "content_hash": "a557dac3b85052f6086ac43033d0914f"
    },
    {
        "id": "1hnuomf",
        "title": "Is it too late for me as 32 years old female with completely zero background jump into data engineering?",
        "description": "\nI\u2019ve enrolled in a Python & AI Fundamentals course, even though I have no background in IT. My only experience has been in customer service, and I have a significant gap in my employment history. I\u2019m feeling uncertain about this decision, but I know that starting somewhere is the only way to find out if this path is right for me. I can\u2019t afford to go back to school due to financial constraints and my family responsibilities, so this feels like my best option right now. I\u2019m just hoping I\u2019ll be able to make it work. Anyone can share their experience or any advice? Please helpp, really appreciate it! ",
        "score": 356,
        "upvotes": 356,
        "downvotes": 0,
        "tag": "Help",
        "num_comments": 232,
        "permalink": "/r/dataengineering/comments/1hnuomf/is_it_too_late_for_me_as_32_years_old_female_with/",
        "created": 1735349789.0,
        "comments": "In a flooded entry-level market with an enormous amount of hype, I believe the best way to transition into this field is internally from another role at the same company. That's the nice thing about data: there is a clear on-ramp which doesn't require a degree at the outset, *and* virtually every company has a need for data literacy. You'll eventually be capped on the technical side without a specialized CS education, but six figures is definitely achievable with Python, SQL, cloud knowledge, and sales/consulting knowledge gained through experience. The main problem for someone without experience is demonstrating enough competence in these areas to start and to set them apart from other candidates. The easiest way for you to do this right now is to work with/for people who need people with data skills, gain their trust by building a working relationship with them, and take every opportunity to demonstrate that you have the skills they need \\*plus\\* the knowledge of their business processes. This will make you the no-brainer candidate for any new data role at the company. 1. Do you have a job now? 2. Could you leverage any connections to land a generic office job? Before learning Python & \"\"\"\"AI\"\"\"\" (whatever an online course could actually teach you in that...), my recommendation is the following: A.) Get an office job somewhere, if you don't currently have one. Ideally somewhere that has a tech department, but by no means is this necessary. Best to leverage social connections for this if you have a big gap in your resume B.) Learn Excel & PowerBI (or whichever dashboarding software the company uses). Literally every company needs people who are competent with spreadsheets and making dashboards, and you won't be competing with 1 bazillion CS grads for these positions and 10 bazillion bootcamp/certificate grads. If you like programming, there will inevitably be room to introduce Python both for automating workflows and potentially even ingestion into PowerBI. C.) Get a business analyst/data analyst position at that company. For this, SQL would probably more useful than Python, but Excel + PowerBI would probably be enough on its own. In the US, depending on what part of the country, this would already be a $70k+ salary job. At that point your foot is properly in the door and you can decide if data engineering is really the path for you, or perhaps get into the. business and management side of things. I hope this provides food for thought!\nI'm pretty blown away by what you've described here and the amazing advice you've given. It describes my exact path over the last 2 years that has eventually landed me in a DE role.\nLOL thank you for the kudos but I frankly think this is prevailing wisdom by most people in this subreddit. Glad it's worked for you! I did something roughly analogous to this but was much weirder. I was a de-facto data analyst and research assistant with a different job title, solely using R (common in public health). I decided against grad school but had a gap in my resume since I quit to take community college classes online full-time in preparation for that switch. I did a data engineering bootcamp instead, then got a highly-specific entry-level data engineer job with a company that appreciated the public health research I contributed to. The bootcamp option worked for me, but this was moreso because 1.) I got a job before the entry-level job market tanked entirely (this was early 2023) 2.) I did have some relevant experience already. I'd say the bootcamp path is now entirely dead and most people should just go the route I wrote up above. Although I do appreciate having my whole career be with programming and no no-code/low-code tools, as that's helpful for my eventual goal of being a SWE.\nYeah.. I actually started school about 4 years ago and am finishing my degree in SWE next semester. The schooling helped me build the skills needed to get an analyst role building PBI dashboards where I eventually realized I could do so much more with my SWE skills and pretty much created a role for myself. Next step probably is SWE.\nThere may be others who could have said what you did (i.e. the \"prevailing wisdom\"), but you took the time to empathise and write these things out, so give yourself credit for that :)\nCan you share which DE bootcamp?\nAdditionally, I would have been better served financially. I think if I had just tried to get a data analyst job and then after a year pivot into engineering. That's in a vacuum... Because the market for juniors went really south shortly after the time I got my job. So I don't necessarily regret it for my personal experience, but it was super specific.\nI'd rather not for two reasons: it's small and now defunct so I'd be doxxing myself as what I think was the sole successful engineer from that program; I don't want to encourage people to get into boot camps. That was a very particular circumstance where I was setting myself up for a pause in my career due to grad school, decided against it, and then filled it with a boot camp. So it worked because I had some savings and the timing was right. Sorry if that's too much information but if you want to know truly you can send me a pm\nWow, we seem to have some similarities in our path (I come from a public health background too, strong R background). I've been working in various data analytics goals till I came into DE with the eventual hopes of transitioning into SWE.\nThats what I love about the community of these communities. This would be such a weird thing to come across from my daily Starbucks commute but you hear it here and its fun to know there are others out there who are so alike in what feels like such a niche career path.\nSame.\nListen to this guy, do not listen to anyone on linkedin selling courses. Get domain knowledge, be an analyst, and then switch to DE.\nno stfu OP needs to take my dagster course\nI'll buy your course on how to not buy courses\nExactly this. Leverage whatever industry you're in to use that knowledge to pivot into a role you're interested in.\nPretty much the way I got into DE. came from a Bootcamp for DS, landed a job as Excel/powerbi-monkey in analytics. Now am a DS/DE consultant. That little bit of domain knowledge and business insight as analyst gave me a huge bump in job chances and genuine business competence even though I hated relying nearly entirely on poberbi DAX and SQL.\nBootcamp solidarity. I was fortunate that I completely leapfrogged the lowcode and no code tools and credit that in part to my bootcamp, but also the very idiosyncratic job I got (can't give full details on Reddit, but it was deliberately seeking people who were new to tech industry to build a local market of tech talent).\nHonestly speaking, you are still not a Data Engineer.\nalright. I'll let the team of people building ETL pipelines in Azure, which I lead know, that I'm not a Data engineer. I hope they won't be too shocked.\nThe way you mentioned above, it didn't appeal so. Secondly, I saw someone mention about the software engineering aspects of data that include the system design and ELT or ETL of data is actually Data Engineering. Just using tools on Azure shows your hands-on ability to use the tools, but not the engineering aspect.\nThis is how I did it. I was a real estate agent > capital markets analyst at a mortgage company.. I had access to sql and leaders that gave me enough room to try new things. I looked for the most valuable things I could automate or build better reporting on. Eventually I transitioned to our data engineering / analytics team as a BI analyst. I ended up doing more sql/ back end work, which I enjoy more I agree that SQL and Power BI / excel would be more valuable for breaking in to the industry. I started with a python course and it took a LONG time to fill all of the knowledge gaps.. excel and SQL would be the fastest value adds for your career. Follow this up with power bi and python. This has been a 6-7 year journey for me with the last 3.5 years in an IT role. I would not wait for the role to start learning, keep taking the classes at home. Going back to college may not be a great spend of your money and time anyway. A $20 course or free YouTube courses will teach you everything you need to know. $20 for the convenience of extremely tailored content.\nGreat response! Thanks for sharing.\nLove that you put a bunch of quotes around AI (not sarcasm). I would say, though, that there are some online courses out there that can give you what you need to work with ML/DL models *provided* you have the foundational math knowledge (which I would say is basically just a bit of calculus, multivariable calculus, and linear algebra along with probability theory) to property digest the information and actually use it.\nI've done all the subjects minus the probability theory and did well but would need a major refresher in multi variable. Even that though is far more background than a career changer with none whatsoever. And that's who's taking this courses by and large! Unfortunately hiring managers also know that so the course label is not really going to do much for you, if anything positive at all. Good for someone in an adjacent role to have though. Like a data engineer maybe...hmm.\nYes I agree. I think the only way is if you can show a GitHub/final product that uses ML/DL models. I dont think anyone should be taking the courses to show that they completed them. It should just be to gain knowledge and then demonstrate it somehow.\nThat first sentence is *chef's kiss*. So is the rest, but that one very clearly summarized your entire point.\nThank you. In line with the sales skills I mentioned at some point in that comment, I'd probably remove the \"I believe\" and assert it without reservation.\n1000% agree with everything youve said here! Just want to add I did a write up on the Power BI sub that may be helpful. If you check my profile or the sub (August, 2022~) youll find it.\nBefore learning python get a job. Lol\nIt unironically works like that! Get a job with people who could give you the job you want. That's my actual advice, not \"Chipotle's hiring down the street!\"\nIve been in IT but since I migrated to US its all Chipotle for me. Cannot land a job. What should I do?\nGotta network to the stepping stone desk job, that's my advice. Leverage your existing local network. If you don't have one yet, build it any way you can. I don't recommend cold calling LinkedIn people to get that first job, just build out a group of people you meet and are friends with in person. Edit: how much IT experience do you have? Is it worth going the certified network engineer route? That line of work cares much more about certs to get your foot in the foot.\n> You'll eventually be capped on the technical side without a specialized CS education Any advice some someone at the data analyst stage of this transition? I collected domain knowledge and have worked myself into an analyst role with the goal of DE soon. What gaps do you think are worth addressing from a CS perspective? And how would you recommend doing it?\nWhen I say \"eventually\" I meant itd prevent you from being an engineer at a FAANG tier company, tech startup, or perhaps a senior level engineer at a large enterprise, something like that. For most people I would say that by the time you advance technically to where you are missing the CS fundamentals, you could pivot to management or a sales focused position and make that your progression. Realistically though I don't think a CS degree comes into play for average DE jobs -- most are based primarily on isolated scripts and knowledge of cloud platforms. Your day job doesn't strictly require knowing how computers work on a low level, or complex networking, or large scale software architecture (You could say this for much of web dev, too, tbh). That might piss some people off here but whatever. There are skills which do take time to master (data architectures, data modeling) but that's not directly helped by a CS degree. There are also the SWE tools of the trade (git, Linux, testing, agile dev practices, etc) but again those aren't actual CS. The most useful deep dives to actually know for an average DE are going to be in how DBMS work on a technical level and the fundamentals of distributed compute so you can tune performance. I don't know how much a CS degree goes into these topics. Maybe you'll hit a point where your company won't promote you without a degree but I think that's less reflective of what's required in the role. If you like the space in general or just want the money my advice is don't stress as much about the degree once you get the title. Get into consulting and become an architect (where sales skills plus certifications and cloud knowledge matter much more than a degree).\nFWIW, I think their comment is off-base. I can't from a Poli sci and MBA background, currently doing DE\nIf by off base you mean \"not universal\" then of course! You have a BA and an MBA, presumably you also had a business job to begin with? You're not the target audience of my comment and are in a very different boat than OP. I've told another guy in this thread with experience in he has no reason to just get a generic office job and pivot internally and gave him different advice. Please do not take anything I said as universal, it's specifically for people with little to no applicable experience and a non-elite undergraduate education *in the current job market*. EDIT: Oh if you meant the technical cap comment, if you read my reply to him you'll see what I meant. For the majority of DE jobs no it won't matter because your progression will be about leadership, not some wicked performance tuning stuff or working cross team with other software teams\nI will add another thing, just being willing to learn about data is in itself an important value you are adding today. The number of data-adjecent roles have increased significantly as data roles increased. The number of data-literate people outside of data roles on the other hand seems to have increased much less.\nYes for sure. Becoming a demonstrated data literate person will help your resume if you want a different role after besides analyst or then engineer.\nWow! That's some solid plan. Thanks!\nThis is a great response and deserves a well-earned upvote. Not sure what this means, though: >You'll eventually be capped on the technical side without a specialized CS education, but six figures is definitely achievable with Python, SQL, cloud knowledge, and sales/consulting knowledge gained through experience.\nYou're not going to be a lead de at FAANG without CS education. If there's an exception I'd like to see it. I bet it's rare enough to be fine as a generalization. You can absolutely get a slick gig as a solutions architect making 200k after 8 YOE with cloud and consulting/sales chops. Actual engineer vs salesperson/product consultant.\nThanks for this insight!\nwhat type of office job do you suggest? like what would the position title be?\nAdministrative Assistant Office Coordinator Customer Service Representative Data Entry Clerk Receptionist Executive Assistant Etc. I'm not really even advocating applying online by job title though I'm suggesting people ask friends and family and make new friends + acquaintances in real life. Be completely flexible on what they bring you, the company matters more than the title.\nposition titles are all over the place for most entry level office gigs. Just find a job where you have a desk and computer. Extra points if the job has Data or Analyst somewhere in the title, but thats less important than just learning how corporate america works. (assuming youre a transitioner with no real experience yet) from there, do a good job and meet as many people as you can. Dont be the guy whos only around to network, but also dont be afraid to ask questions. Tech industry moves fast and everyone is always learning something new, so the quicker you get over the fear of looking dumb, the better.\nyes, correct. Market is terrible now.\nFor juniors I would say yes. Otherwise I don't know because there's no clear data and I just have my anecdotes.I'm in the process of interviews and job switching right now. I have not applied to a single online job where I did not get a referral first - that's probably distinct from the firehouse approach people take - and I have 3-4 promising leads. One is clearly very desperate to hire!\nWell very said.\nCan I ask a follow up- I came from a non cs degree and now work on a quant macro modeling team. I learned py,sql,excel vba, power bi, and stats on my own. Heavy use of big data libs and applying ml technique etc. Im 25 and did this over the course of 2.5 years since I came into my role. You mention capped out without a specialized cs degree. Why? (In all seriousness). There are some things I have to learn on the fly and dont understand as quick as others, but is this always the standard? Would it be worthwhile to go get a masters in de/data science if my intent is to work in big data?\nI debated even mentioning that because I think people would take it the wrong way. Average DE probably doesn't need to worry. You and I won't be building the next Apache Datafusion query engine or leading a team at Airbnb. That's the level at which I meant you'd definitely need a CS education. Beyond that I think the barrier is moreso HR and not real knowledge.\nThis is what I did!\nAs someone who did exactly this, this is terrific advice\nThis is excellent advice, you could always pivot to project management or tech PM if you have fundamentals and can talk to (deal with) business and tech peoples. Data Analyst is a strong start.\nThis is the answer\nIf I have a bachelors in data analytics, dont I just search directly for a data position, the other way around sounds bad as fuck \nWell maybe it depends on your local market, how much experience you have, how good your school was. I think you could maybe skip the random job step but dashboard + excel is still likelier to get you an entry level job. I also still suggest networking. If you're fresh out of school, and working a random office job for one year as your first job sounds \"bad as fuck...\" I've got to be real it's really not a huge deal if you're young. You have time.\nI'm 33 years old and have been pursuing other educational paths before this. I have about 5-6 years of work experience. Also, I'm from Denmark, where the job market is significantly better compared to the US, imo. (You sound like you're from the US).\nI am from the US yes. My post is more for people without relevant experience or a specific college degree in the us market. the advice to focus on networking still applies to you but idk about the rest\nI would certainly recommend also using libgen to access some 0r4illy books on the matter. Thats free quality education my friend!\nI know this is a bit late but this is the exact path I took! (landed a role 6 months ago). It was a long road but definitely a viable one.\nI started as a Data Analyst at 42 but that was after a few years in GIS and no degree. Then after many years became a Data Engineer. That was after DE was actually a thing. Just get a foothold in a business in IT as whatever and angle your way towards DE work if that's what you want to do. We hired a female with a community college degree as a DE and we're completely dependent on her now. She came with a little work experience but younger than you.\nNice! Im in GIS making the transition right now (or trying to).\nI'm a pretty senior figure in GIS and some related areas in my part of the country, and I'm looking at how to increase DE in our GIS operations. Our organisation is quite far behind, and despite senior leadership wanting change, massively behind due to their own policies.\nGotcha. Seems like Ive shown my leadership the light and Im getting promoted to GIS Developer. Ill have free reign and I plan to just make it a DE role.\nKeep at it. Another colleague and I are a year or so away from retirement and that protege will become top dog with top pay way sooner than we did.\nThats great to hear.\nNice, I like GIS focused analysis (transport studies etc) and worked in it briefly. Its fun for me. But it pays kind of crap with not much opportunity where I live. So I work in data engineering and architecture in a bank. Which is boring but it pays the bills.\nI was glad to move into Data Analyst. I didn't see a lot of up side in GIS either. DE pulls you in many directions. I miss the days of mastering one or two applications rather than having to constantly learn and support new ones for every little task.\nI have kind of the opposite problem at the moment. Getting funding approval for anything is like getting blood from a stone. So I have to abuse open source tools and basically code all of my own tools for everything else then write hundreds of pages of documentation no one will ever read. Bit of a waste of time really.\nOuch. You're probably much more resourceful than I am. I actually don't mind documenting. I know it's rarely read but it's really annoying when people don't document and I have to fix their process.\nThe realistic answer: While it's not too late to get into Data Engineering it is very likely that you won't be getting any entry level jobs at ALL by just doing an online courses. There is something people don't seem to understand. Just because a career has entry level positions does NOT mean the job is \"entry level\" at all (hope that makes sense). My recommendation: Go for Data Analytics instead. If you're dedicated enough in 3-6 months you should be job ready. To get you started watch this [Free Bootcamp](https://www.youtube.com/watch?v=wQQR60KtnFY). After you've landed a job you can work on upskilling to become a data engineer. Plenty of Data Engineers started as Analysts first.\nYeah especially with how the tech market is right now. It really couldnt be a worse time to try and jump on the ship. If your requirements dont match perfectly you dont even have a chance to be part of the lottery slot machine of even trying to get your resume in the hands of the hiring managers. Its a bloodbath right now.\nWith a degree in Engineering(ICT), diploma in Data Science and projects, what would be your recommendation?\nI personally don't think it's too late for anyone, with any employment history. That's not to say it won't be difficult! But I've found that working in a field you like is awesome, so if you like it, go for it.\nGood point! This is probably the most important thing you need to sell to future employers: are you pursuing this career because you heard of the great chances and jumped the bandwagon, or are you here because you're genuinely enjoying this kind of work? People who come into the data field solely for the money won't be happy and will have a hard time fooling an employer into hiring them. I've rejected too many CVs of people who don't even try hiding the fact that they did the absolute bare minimum requirement to call themselves \"data-anything\" before applying. If they ever find themselves in interviews, they often fail to give a single reason as to what they are passionate about in this line of work. You're competing with people who genuinely like their jobs. Bear that in mind.\nI dont agree with the majority of these comments. A lot of people are setting you up for false hope here. The best data engineering roles are essentially a subset of software engineering, the way that DevOps has essentially become as well. If you follow anything about our industry, software engineers are in far more supply than demand at the moment. Several hundred to thousands are looking for work, including desperate H1B visa holders with lower standards who are desperate for employment sponsors because otherwise they will have to go back to India Dont let positive Reddit vibes fool you; there is a very huge brick wall youre up against if starting from absolutely 0 technical skills or experience, it means you not only dont understand data engineering but no IT experience means to me you dont know anything about data, distributed systems, kubernetes, how data or software in general is used in the business, how data has transformed into real time insights for decision makers, how the cloud works, ETL or ELT-and 100 other things not related to the software engineering / Java / scala/ Python portion of data engineering itself with Spark or Databricks snowflake etc etc What Ive described here is whats necessary to be proficient and well compensated, if those are your goals Maybe look into roles with a lower barrier to entry like data analyst, Helpdesk, cloud support engineer, etc - youre inquiring about brain surgery and havent entered med school yet if youd like an analogy, and no amount of Reddit fluff will change that\nThis !  100% This. I find it very surprising that, these days, so many experienced mid-level data scientists and data engineers *still* think that their roles are completely separate from software engineering! Much of what Im saying here are generalisations of course, but its high time that data professionals, as I prefer to describe myself and others in similar roles, realise that they ARE software engineers!! We are designing and building software applications. Its just that our applications are data-oriented. Whether it be data architecture and data modelling, a multitude of ETL/ELT pipelines, designing (and using) data lakes and data warehouses, building and deploying machine learning models, and setting up (and using) the necessary monitoring tools, these are all software products and so the tools and best practices of software engineering - design patterns and source control, proper peer code reviews for example- should be adopted. Other related skills include those that could be described as DevOps - such as provisioning cloud infrastructure using infrastructure as code. Obviously Im not saying that ALL these skills need to be learned, and different companies will have different ways of dealing with security issues, keeping developers and data scientists away from production etc, and those working in or close to the data science and analytics work will need maths and stats skills more than pipeline building, for example. So back to the OPs situation, becoming a data engineer with no tech background, I say: - Go for it !! But in order to manage expectations, I would highly recommend taking software engineering courses along with data engineering ones. - Learn about principles of SE such as SOLID design principles, the MERN and PERN stack, creating and using APIs, object-oriented design, functional programming, software architectures such as micro services. - Build some of your own projects. A good place to start IMO is with a personal website. Start simple- a static site hosted in the cloud (eg AWS S3 which will basically be free). Then make another version- with a backend database such as MySQL or MongoDB. Then another with NodeJS and a popular web framework like React. If you are rubbish at graphic design, like me, dont worry there are loads of free tools that can help with the basic UI/UX. - Host your work on GitHub where people can see what you are capable of, and adopt modern SE workflows such as CI/CD using GitHub Actions. Go beyond the basics of git/github and adopt the practices used by teams working on the same project, such as branching and tagging, and doing pull requests, code reviews of your own code and merging.\nTruth. Never seen a junior nor entry level DE\nIt's not too late! Just be aware that it will be a long path of learning, and you will likely face struggles getting your first job (any tech job). Regardless, the skills you learn today will really set yourself up for future success. You only benefit by doing this.\nAgree!\nNot late, a data analytics degree would be the faster way to get into the data world. Then you can jump into DE\nIts not too late at all. Your starting point is rough, but your age isnt an issue. Development work as a whole is going to be really hard without a degree or experience right now, and there's no real hope for it to change back any time soon. Learning coding by itself is a great skill and can be used to help automate and accelerate any desk job, so learning it isn't a waste at all. Data Engineering itself is generally not a junior role, but there are some exceptions. Those exceptions are going to be for CS grads. So, going directly from your current situation to data engineer is wildly unlikely when most roles aren't even available to new CS grads. My personal suggestion to you is to figure out a way to finish undergrad. WGU is a cost-effective way to get a CS degree, so that could be an option you haven't considered yet. They have a sub for the program even if you search for it. There's a ton of people who make that program as cheap as they possibly can. Don't forget to look into scholarships as well.\nThank you for your insight & suggestions!\nNot too late. It will probably be hard to go straight into a DE role without experience but you may have an easier time going into a Data Analyst or Business Intelligence as a stepping stone\nno, I did it at 40.\nOhhh thank you!\nIm in the same boat. Im starting out at 40 and thought I was too late. Thanks for this.\nIt's less about age or gender than timing. It's a bad time for anyone in IT unless you came from a prestigious school and bagged multiple good interns. I think the best course is to find as many connections as you can, beg them as much as you can, and jump into something remotely related to the field. IT helpdesk, Data analyst, Business analyst, whatever, anything that can lead to DE in a few hops is good.\nYou are not late. The only question I have is how much stamina you got. Here is probably what you will need to learn 1. Python OOPS, Solid Principles 2. Design patterns in python 3. ISLR 4. Big Data course with Hadoop, Spark 5. AWS developer course 6. Airflow course 7. Git 8. Docker and Kubernetes 9. CI CD 10. ML Engineering Deployment Failures I know are people who do very little. Its about showing up daily for a couple of years till you are good in everything thats needed. I know list of things is long but it is what it is. All the best!\nWhat about SQL? For what it's worth, and to make sure people aren't misguided... The associate level (not junior, but one step above) data engineers at my last two companies didn't know any of these 10 things when they started on our teams.\nVery important!\n> 8. Docker and Kubernetes What exactly does \"know Kubernetes\" mean in this context? K8s is a monster, but how or what do you need to know for it to be enough in DE?\nYes, it is a monster. One would start with pods, services and deployments. Then work their way up to monitoring, batch jobs, permissions, etc.\nNope. Don't listen to the negative gatekeeping here.\nAside from any specific career advice. If you're 32, and the average retirement is 62, you have just under how long you've been on this earth to keep honing your skills and experience. I think you'll manage just fine if you start now.\nI hadn't seen it this way!\nDefinitely too late. Human life expectancy is 33. You should be preparing your funeral arrangements, not learning new skills. Seriously though, you can retrain in pretty much anything at any time in your life. Only you know whether it's worth investing the time in a new skill set. If you enjoy it and can find the time to study and practice, work towards your first data engineering role. You likely have another 30 working years left in you. You could change careers again in 10 years if you really wanted.\nWhile formal training has its merits, demonstrating how you identified a problem, taught yourself the necessary data engineering skills, and successfully executed a solution will often hold more weight than a bootcamp. Even if this doesn't directly lead to a new opportunity within your current organization, many other employers will be impressed by your initiative and commitment to learning. Your ability to self-teach and persevere, along with overcoming failures on your path to success, will speaks volumes about your dedication and passion for continued growth. That difference between just be training *entry-level*, and having a real experience. Even if you don't have the title, can be a compelling selling point.\nI dont see how you would get hired, sorry. Also you need SQL. There are so many desperate f1 masters out there rn.\nLets wait and see\nThis journey will take years, not weeks or months, if you want to be a bonafide data engineer. Entry level competition is ridiculous at the moment, but not impossible. You can do it if youre willing to put in the substantial amount of time. If you cant commit that time, start by aiming lower. QA roles, data annotation, etc., to get a foot in the door.\nCan I dm you?\nnot too late, but you'll need to start somewhere that isn't data engineering. I came from 10+ years of software and it still took years and a complete reskill to even get my first interview. data analysis/business intelligence stuff is the closest thing to a direct entry point. https://roadmap.sh/data-analyst\nDo you really want to hear the truth?\nYes\nIf you have the personality for it I would backdoor your way into a company where you can make a real impact pitching them on a project where you can essentially get paid to learn independently. Lots of companies are trying to figure out what 2025 efforts theyll have related to gen ai and typically rag. You could pilot a project, youd be a low cost resource for them, its have to be a good fit. And I would lean hard on cursor or windsurf for writing python and skip learning syntax or coding altogether and instead cut a straight line to programming foundation theory, prompt engineering techniques, data preprocessing pipelines. Otherwise, get a degree in DS and knock on the front door.\nI have been practicing mock sql interviews with random people online. I came across this one lady who just nailed almost all sql questions in 30mins from easy to hard. I was impressed and honestly inspired when she told me that she is just started preparing recently. She was 30ish. And absolutely yes u should not think if its early or late. Work hard and reach out to people who can provide you with opportunities! Yes there are many leaders and managers who are willing to bet on you!\nThank you for your words though! Its gonna be a rough journey but better than not trying.\nAlso I see a lot of negative bs on your post. Please ignore them. Yes your profile cannot be great for a recruiter since u dont have a tech background. But hey, if u r proactive and sincere you will get it. Also u will find ways to get a job. Build projects, network with people, show your journey on LinkedIn etc..: go to GHC if you can afford. In the present economic conditions an application and recruiter screwing wont help you. Only network and also your network should trust your skills, so u should start and show your work and determination. Although you may or may not get the exact job u wanted but you will start somewhere. Just appreciate the approach and keep working hard! Also dont pay for any course everything is free online, people are ready to help. Your post here itself inspired me to write all these. Keep fighting, the road is hard but dont give up. Think about people like Elon who have achieved impossible things.\nWhat is GHC?\nGrace hopper conference\nIt's definitely not too late! Hell, I went from a Psychology PhD program to data scientist in my late 20s. There is some great advice in this thread, so I'll just say, you've got this!\n...\nIts going to be hard - but hard is often the only good way to do something worth your time. Dont study curriculums. Study job ads - what tools do people use. Learn those tools. Ask a friend if you can shadow them at work - especially easy when people are working remote. If you are interested in shadowing me at work, you can PM me. Im in cet timezone\nCommenting as someone that ended up with an ERP consulting job (data analysis/database management) with no experience: I got very lucky with contacts I made in my life who realized I could handle it. Six months into the job and still getting my ass kicked everyday by the sheer amount of stuff I don't know. Not a single course in the world could honestly prepare me for the job. On the job training turns your \"unknown unknowns\" into your \"known unknowns.\" Network, impress people, and weasel your way into any gig you can. TLDR: Taking a course is probably a great way to show you're interested but an actual job in the space is the best way to learn.\nNever too late. I (47m) was in music, which is what I went to school for. I learned computers on the side by myself. The biggest skills needed aren't even necessarily technical, it's all problem solving and the ability to learn new things.\nIm on the software side but I started at 36 with a background in non-tech non-business all together. Have a rather unusual very non-traditional background. Anyways the market is very different from where I started but its doable. My best luck was working for a consulting firm. I started at a local firm that was eventually bought by Accenture. It was unsexy and boring but it got my foot in the door and that was all I needed. I moved from there to a startup and kept moving up. Im now a staff engineer at a fintech company you probably use if youre issued equity by your employer. For now, Ive secured my place in this industry.\nIf you're also interested in this as a hobby, then by all means go for it. If this is solely to transition careers, it's going to be a steep uphill battle. Many workers with bachelor's and master's degrees in this field are struggling to find jobs currently and the future prospects for the time being are not looking great\nThe tech job market is difficult right now even for those with formal degrees. If you are in the US check out this link for free technology training https://perscholas.org\nIt's possible but it will be difficult. Your age is less of an issue than your lack of experience. Data engineering is not an entry-level job. Everyone on my team has at least a few years of experience with most having more. And the ones with less experience tend to have master's degrees. Also understand that some people were able to luck into data engineering jobs. They were hired during a very hot job market (e.g. in 2021) when companies could not hire workers fast enough. Those market conditions are unlikely to repeat themselves anytime soon. If you're good at networking that would be your best bet. You can try to find women focused IT jobs / groups, see if you can get on good terms with people in the group, and perhaps get offered a job by someone in the group.\nRather than learning and fortifying your fundamentals in AI. Learn Python. Master your SQL. Learn version control. Learn how data warehouses work. Learn what data modeling is.\nI was 36 when I transitioned into DE. It was for an internal project so I got promoted into it. After 2 years I went to a different company and kept learning and moving companies. I'm 50 now. So yeah it's possible to do, you're still young\nyour infos are a huge red flag for the recruiters tbh, maybe try something else like data analyst\nYou are competing with tens of thousands of Masters Graduates\nNo just do it. I knew a person who started a math PhD at 38 and did very well in data science following that\nDid you say female?\nNot too late at all! The tech industry actually values career changers because they bring unique perspectives and soft skills - your customer service background is a huge asset since data engineering often involves collaborating with stakeholders and understanding business needs. Start with Python basics, then gradually move into SQL, data structures, and ETL concepts. There are tons of free resources like freeCodeCamp, DataCamp (first chapters free), and YouTube channels like Alex The Analyst that can supplement your current course. For hands-on practice without spending money, grab some public datasets from Kaggle and try building small data pipelines. Once you've got the fundamentals down, consider picking up some cloud basics (AWS/Azure free tiers) and containerization (Docker). Build a portfolio on GitHub showcasing your progress - even simple projects count! The learning curve might feel steep at first, but breaking it down into small, manageable chunks makes it less overwhelming. Your age is actually a strength - many companies appreciate professionals who bring real-world experience and proven work ethic. Focus on learning the concepts thoroughly rather than racing through tutorials, and you'll be surprised how much you can accomplish in 6-12 months of consistent effort.\nNot at all. My former Manager started going to college for computer science in her early 40s.\nI think it will be hard if you are going into operations type of job. Said it might take you 2 years to finish the masters degree, you should focus additional work AI agent cannot do. Focus on security governance and accuracy framework on data analytics or AI agents. In 2025 and beyond, you will see more hallucinating outputs which will in turn for businesses wanting to put the guardrails in practices. So, do your degree but dont focus on the ops only. Find and select classes that has supervision function. And lastly this has nothing to do the age and female/male at all. It is never too late. Its not a DEI hire.\nIts a spectrum Its not a binary answer Is 32 old? Not one bit Depending on where youre currently at, will you have to take a slight pay cut? Potentially . Up to you if thats something you and you fam can do Ive seen 45 full transitioners get the job done Ive seen 22 year olds that couldnt handle the rejections and quit after 100 applications I would say you still have 70% chance of succeeding and breaking into a data role So not bad\nNo.\nLots of great advice....and a few sprinkles of doubt and negativity......but enough to give a a real view of the possibilities. Personally....SQL and Python are your must learn tools. SQL is the bread and butter, but Python is that one thing that could give you an advantage, especially when you can use it to solve complex issues you most grads would have never seen in school.\nIdk about the current market, but a couple years ago we hired a 55 year old man with zero professional experience in the field and he's been excellent\nIt's much easier to get a job in DA and then internally transfer to DE then to just get a DE job with no background. Of course this advice can be thrown out the window if you have some inside connection who is willing to take a chance on you\nIt would be, but since u are asking, that means this experience seems for u something new to your life. And this question is about do I need to bring something new to my life? - Definitely yes There no such thing as complicated, life in general is more complicated then any field you might step in. And since u are 32, with your interest, believe me or not- u can beat everybody.\nIf you feel that being female matters then yes its too late.\nMarket is oversaturated unfortunately\nI went back to school at 30. You have plenty of time to do whatever you want. Re: data analyst A subject matter expert (construction, baking, manufacturing, customer service, education etc.) trained to use data, will make a better analyst than a data person learning a market from scratch.\ni made a career switch from teaching to tech when i was 35. started as a support engineer, transitioned to an analyst role, and am now more heavily involved in the data engineering side of things--albeit within the same company. while i constantly feel like i don't know what i'm doing, the move is possible.\nIt is never too late\nNever to late but like others have said you got find a side way in to transition in and get experience\nHell no.\nIt's never too late! Good luck!\nIts never too late.\nNo.\nNope. You got this girl! Go for it!\nNah, never too late. Im 45, Im still learning.\nWhat kind of customer experience do you have? I would leverage that. One thing that is a valuable is understanding the customers needs, and also comfortable speaking to them. Dont know exactly what you'll land in data engineering, you might not be very client facing but it helps if you're capable. Enrolling in online courses is a start. If you can show any employer you're interested and driven to learn more, it helps. On interviews, I'd ask if they offer some form of educational assistance programs (might sound like: you stay w the company for about a year, you go back to school or enroll in some training that pertains to your job, they reimburse you after passing) Think of your value in terms of skill - what skills you have / what are you good at doing. Build upon them. The industry knowledge will come naturally. I started out in customer/help desk support. Before that, I had customer service experience from retail and restaurant jobs. I started applying at any and every job opening. I didn't land the position I wanted but it was a \"business anaylst\" that got my foot in the door. Even though it feels like the chances are slim, you want to find the ones looking for a you-type candidate\nHow many years of experience do you need to land a job in data engineering?\nI am 32 too. Being a 2022 A/C and Finance graduate, I made an equally painful and scary decision to jump to C Science,whilst having newborn twins at 30. Mind you, I was fresh out of college then and dimeless. What I encourage you is,it all goes down to what you really want. Whereas there are easier ways of getting into it, like assimilating your ambition with what you currently hold, a complete overhaul wouldn't be over reaching. When I made the decision, everyone told me what I was opting for was already flooded. Like the Bitcoin Story, 2018, the narrative was Run. 6 years later here we are. Take a leap of faith and jump ships ONLY if you feel and know you have a lot more to offer in DE than what's currently available. Salary ups should come secondary. My Primary motivation to change was I saw an underlying problem in the community I live in...2 years later, I have solved it for them..I also have my bills covered a year ahead. Remember i am halfway through. Delayed Gratification is a thing. Promise yourself that if you want to get your feet on DE, make sure you come out as the best. There's something reassuring about holding all your cards. You won't be a pushover anywhere you step. 32 is still young enough. Forbes lauds 40 under 40's if timelines bothers you that much. Mentally prepare yourself to walk on knife edges. A degree will build you wholesome if you are someone without network. If you are well connected, a boot camp certification can work for you.\nI think its a dangerous move to jump immediately to data engineering, baby steps, start with data analysis and work your way up\nNo, it is never too late\nIm a practice lead of a data consulting firm where I have about 250 data scientists, analysts, and data engineers on my team. I think the purist idea of data engineering will soon be dying and (mostly non-tech first) companies will be looking for a host of different data engineering type roles - and many of the people who will succeed will ironically be very very different from the way we look at talent (mostly through a technical lens) with traditional data engineering roles. So my advice is to go for it if youre truly interested, but dont try to play the same game as the kids out of school. Go about it your own way and chart your own path as you go. I think the industry will be evolving in your favor. Also stay flexible however. It sounds like you still dont fully understand the data space so explore the different roles before you jump right into data engineering.\nSummarized the whole thing in a Medium post. [https://medium.com/@aa.khan.9093/think-its-too-late-to-become-a-data-engineer-at-32-think-again-ed7c542a2ecf](https://medium.com/@aa.khan.9093/think-its-too-late-to-become-a-data-engineer-at-32-think-again-ed7c542a2ecf)\nIt may work. But as a backup option consider QA position. Data engineering course may be helpful for that too. Another option is Data Analyst role, it maybe sounds scarier than data engineer, but it actually requires less programming skills and more focused on data visualization - think as more advanced Excel.\nGo to college get a degree and get ur masters. Anyone telling you you dont need a degree is lying. The market is extremely competitive. Yes you do need experience. Yes you do need to know how to code but the college isnt required posts are annoying. Get a degree in economics math stats business MIS or engineering . These degrees allow you to think outside the box. Data engineering isnt just about coding. You need to understand the math behind it to be successful in the field. Why do you think these jobs pay a lot? They want good talent who not only know how to code but truly understand the math. Please dont go the easy way out itll only hurt you in the long run. You are able to work for the government with a masters degree and most retail corporate jobs require a masters degree in finance economics DS MIS stats etc..\nNah\nits never too late!\nJust jump, my brother was 50\nI wont Say , It is too late There isnt age for new learning I have changed my career completely in data engineering from robotics program. I would suggest to pick a good course and study in a complete sequence. Prepare end to end project and show it in GitHub Whichever topic you study , study in complete depth so you can tackle interview questions.\nNope. Julia Childs didn't learn to cook until she was 40. My suggestion; find free data (which is actually kind of plentiful) and free tools (which are often the best) and start doing something - something that interests you. Great free tools: ObservableHQ for visualization Python Python and Pandas Python and Polars VSCode\nNo, and lets be honest, in this market and sector you really have being a female helping you. Reality is that a lot of companies hire women with interests in IT because they need to hit quotas for their DEI scores and social optics. Its an awful thing, but with this saturated market at the entry level, you at least have that as a +1 above all entry male applicants. Sucks, but play the game \nWhat about being hispanic from a minority underrepresented. It could help?\nIt could help you get a job, yes. However its not something to be proud of or happy with since you know, as an entry level person, that you were most likely hired to be a token DEI statistic. I personally would find it incredibly dehumanising, but theres not much you can do about it.\nI believe the same, I think you should be hired for your skills and capabilities, regardless of your race, skin color or origins.\n55+ years old here. I've switched into professional administration (systems and networks + some software dev) at 28 years old. Yes it was different time then compared to today, so I'll give you just one tip: you should try to figure out what really interests you and try to develop into that direction. But be very careful! Many of current available IT jobs will die soon due to AI development. E.g. I don't expect that DBAs will disappear soon, it may change, but companies will need DBAs for years to come.\nIt's never too late to pursue your aspirations. Recently, at the age of 33, I transitioned from a career in Mechanical Engineering to the field of Artificial Intelligence and Machine Learning. The journey has been challenging and continues to demand effort, but it is steadily progressing. I am now employed at an AI-based startup, reinforcing my belief that age is not a barrier. What truly matters is unwavering dedication and consistent learning. So, take the leap and move forward with confidence.\nTheres no such thing as too late to learn new things, u go girl  u cant know until u try, good luck \n32??????????? Come on that is no age at all. Neil Armstrong didnt become an astronaut until he was in his late fifties ( or something )\nDont want to demotivate but thats not how it works. People see the happy side of IT where one earning huge but dont see rest portion on unemployment and layoffs recently. Not to add the level of entry level freshers offering more time, enthu and ready to relocate easily on projects needs with little to no strings attached to look after a family ( meaning can work worry free). Unfortunately, if you didnt look for your own career at right time, unfortunately you would need tremendous support from your immediate family members and ready to share responsibilities, which i have seen very poor in india unlike other countries. Thats why i always promote for every woman to be financially independent as early as possible. Now since you have reached out, acceptance of non IT people in IT industry is very poor. Take a comparison- i am hiring manager, i am getting young 22 year old IT candidate, very good in academics, and willing to do anything to get a paid job in good company, and another is you who is say 33, unemployed, no past experience, and no IT knowledge, plus has strings attached related to taking care for family, cannot relocate or flexible working at project hours, answer is no brainer. However did run few initiatives under CSR, where we train people for IT skills free of cost and land them to job in MNC, entire process is end to end free, all you need to invest was time to learn that too at your own pace, but unfortunately i got hardly 2 to 3 candidates only and rest women were just enjoying comfort of their family wealth showed no willingness to invest time in learning. Last is, if you learn the course still you would be treated as fresher and no crash course would compensate your knowledge for a full 4 year degree. However if you have some connections, it would help you land job else without connections it would be waste of money.\nCan I dm you pls?\nPlz share your expectations and intent for connect.\nI am in networking support, but want to shift to Data Science/Engineering, what advice could you give me?\nYeah too late\nMy best advice is to get familiar with the roles and then you can make a more educated decision what you really want to do. In short: (1) Data analysts: business people with some basic analytical skills. Mostly creating reports. Required skills: business education and deep domain expertise, some analytical skills. Maybe a Google Data Analyst Prof. Certificate on Coursera. (2) Data scientists: who convert business problems into data problems, develop solutions. Best education: depending on which domain you are targeting, some quantitative (preferably domain-specific) undergrad (really anything but e.g. quantitative social sciences like economics, sociology are also good here); and then a statistics or data analytics master's. If you have a business undergrad then a statistics or data analytics masters. (Probably this is the best if you are migrating from a business field, because you can use your domain expertise.) (3) Machine Learning Engineer: someone like a data scientist but more focusing on software development and putting the models into production. I guess, because here in Europe we data scientists do this, too in most cases. (There are not that many MLE roles here.) This is different in the US. (4) Data Engineers. I am personally genuinely confused here, because in our company (huge telecommunications corporation) Data Engineers are basically infrastructure people (cloud engineers, MLOps, etc.). They are working with OpenShift / AWS / GCP / AWS services, setting up environments, working with Docker, Helm, Kubernetes etc. -- but in other companies Data Engineers are basically database engineers who are maintaining databases, writing data ingestion pipelines etc. -- So regardless of which role do you go with, the best is to have a CS degree here, because these are system architects, cloud engineers & stuff. Now honestly the Data Engineer role is overhyped nowadays, but it is not for all personalities. E.g. I myself find it boring and I wouldn't want to do it. On the other hand I love data science, love to come up with superior modeling solutions, love to play with prototypes, develop outstanding user experiences etc. (Me personally came from business, upskilled myself to data science with a relevant master's and lots of learning.)\nNope - go for it get yourself learning materials a training environment and go for it.\nI entered the field at a similar age, with some experience in databases and a bit of programming in R. The job opportunity came unexpectedly, and I was hesitant to take it, but I disliked my previous job so much that I accepted. Initially, it was very challenging due to the steep learning curve. However, over time, things started to make sense, and now, five years later, I enjoy it. Don't get too focused on specific tools, frameworks, languages, etc as they constantly evolve and there's always something new. Before jumping into this field, ask yourself if you enjoy solving problems and if you have the drive to tackle issues independently. If you lack the urge to tinker and figure things out on your own, this field is not for you, and you will burn out quickly. Many people enter this field for the money and end up burning out within a year.\nNo.\nYoull need to become solid with SQL. Also, its hyper saturated with desperate visa applications willing to work for free. Discrimination in this space is really bad atm due to current events. Gov is a decent place to look if you are really interested. But most gov entry to senior are excel and access based.\nNah, never too late but it aint gonna be super easy. I also come from a non-tech background and am a late career transitioner (I think I was 42 or so, 45 now). The market is looking pretty saturated for lower lvl des (which I count myself among). Main problem with your plan is the course you are doing isnt likely to be sufficient to prepare you for DE work just by itself. I dont know whats in the course, but if I were you, I dont think AI fundamentals will help too much and would focus on learning Python, SQL and learning more about the fundamentals of what DE work generally entails, learn about data modeling, ETL.. Im sure more experienced folks can give better advice. Problem is every company seems to use a different tech stack so you want to learn more about the fundamentals that will more universally apply, and then pick a few tools to specialize in. I def think you can do this if you have the time and patience to learn enough. Hopefully the job market gets better by the time youre ready to start applying. Wish you the best!\nTo get your first entry level DE job, your experience with customer service will take you so far! It is generally perceived that people with a formal computer science background dont have great bedside manner working with clients and business side employees. Train up and then as you go into interviews have a strong repo that shows you have learned into code and lean into your people skills in the interview. Side note - never too late to make a change\nOh I never see it this way in terms of the experience as customer service. Thank you for your words of encouragement! This mean a lot..\nThis is also a good reason to start in something like analytics and transition to engineering. Analysts will often get a fair bit of face time with clients, especially if you do something like management consulting. If you find analytics processes are inefficient (e.g. data prep, storage, access, change control, security etc) then you can introduce engineering principles and get experience that way. This is how I started, at the age of 35. I am now 43 and the lead data engineer and data architect at a small bank. (For context I am not in the USA so your mileage may vary but the principle should be the same).\nYes it probably is\nIt's never too late, do it! I was a construction guy who got hooked on Excel and got with the right company to help push me to grow. Do what you like to do, you will do it well. (Most likely, it just takes a bit.)\nNo, its completely fine. You wont be a FAANG engineer but there are lots of opportunities. Just dive head first and work really hard. You need projects and contributions of open source programs.\nWhy not a FAANG?\nHmm.. Im not in data engineering, but Ive taken courses. Im in cyber. I think something important about this industry overall, is having a mental roadmap of how youd get there. Look at the building block roles that lead to where you want to go. Its certainly not too late, based off people Ive met with similar situations. However, its a journey that takes a commitment. Breaking down the pathway into more digestible pieces can make a big difference. Also, networking and getting involved online and offline. There are so many people a projects out there. Theres so much support. Its competitive, but there are many avenues that can build experience and a network.\nNo, but itll be tough\nTry creating a full end to end cloud DE project on your own, with help from chatgpt or Claude or any other AI tool. The data can be a dataset from kaggle or if you feel bold, any public access API like the weather API. I'd say a basic ETL with AWS' free tier should suffice. Pull in data through AWS glue to s3 or redshift, then transform it somehow using dbt core or dbt cloud, then show transformation steps to a simple visualization tool like tableau public or metabase (open source). You'll gain hands on experience with DE and devops skills and can showcase this on your GitHub.\nNo but it feels weird to have peers and seniors that are younger. Im 38 and am a lead but I have peers that are a decade younger than me cause I started DE at around 31. Dont let that hurt your ego and youll be fine.\nNever to late for anything\nWent back to school for CPSC and lots of people in even in their late 30s. Others were doing their masters in data science trying to break into tech. Its been great career pivot and tech has given me the highest income and best work/life balance I could ask for. Good luck!!\nI just started my Masters of Data Science at 35 and transitioned into DE from GIS, internally within gov. I had been working senior roles in GIS for over 10 years and even though working various technical roles I'm finding the transition to be a challenge. You need to proficient in SQL, DevOps, version control, ETL/ELT pipelines, cloud. I'm finding free Microsoft courses to be an extremely useful resource in between my masters.\nYou're not too old, start with sql and data modeling and you will be fine. Also learn a bi tool and specialized yourself in one functional domain to start with (hr, finance, etc...)\nGet a job first. And to start with explore data analysis and the related tools. This may ease you into the data side and this learning can fetch a job with much easier/smaller skill set . Once you are comfortable with data analysis, you can then expand by looking at data engineering. Without IT and data background, jumping directly into data engineering can be overwhelming.\n> and I have a significant gap in my employment history \"I've signed an NDA\" - boom. fixed. Anyway, I went back to school at 28 for software engineering, so I was on the late side as well. However, I already had an IT background, which helped somewhat have it easier at school. Sadly, I have no idea how the market is ATM, because I joined my first company some 3 years ago and haven't switched yet.  That being said: Teach yourself debugging. [vscode debugging](https://code.visualstudio.com/docs/editor/debugging) - this should make it easier to figure out WHERE the application breaks, or why a value isn't what you've expected. And second (and this is harder without help): teach yourself to write tests. Ask Claude.ai for examples (its output is of a higher quality than even ChatGPT's o1, IMO), or slap your code into an LLM and ask for tests. I would recommend using `pytest` to run the tests, since `unittest` feels a little bit archaic, though using `unittest` is better than nothing at all!!! Learn unit testing simply because you should do it at work, which fill feel futile, until you have to _change_ something, and all of a sudden a test breaks that you did not expect to break, and now your tests have saved your bacon by not introducing a new bug into your code. TL;DR: Learn **debugging** and **unit testing**. They are massively valueable tools, but school and courses will likely not teach you that.\nYou only have 33 years until retirement. You might want to just give up.\nNo, you are fine. Absolutely fine. There are some questions I have though. A) Do you enjoy it so far? Is it kind of addicting? Has it replaced all in your life all of a sudden? B) What is your dream job? Why?\nThe problem is not your age, but you do have two very serious problems. The first is experience: assuming you don't have a STEM background, or any specialized experience that can be leveraged into an analytics role, this is akin to asking, \"I have never played a sport in my life, could I make a career out of hockey?\" I mean it's not impossible, but to me it does not sound like a high probability play. The second is timing. If this were 2018 during the DS boom, there would be tons of companies just realizing for the first time they need data engineers and nobody looking for the role. I would be able to give you a cookbook. Now it's an open secret and the market is saturated with job seekers (who have all done the exact same learning path as any I could give to you). Worse, with the recent layoffs the market is also full of job seekers *with experience* who will be competing with you. Even good advice from as of 5 years ago is imo not tenable in today's market.\nNo it's not too late. Dm me if you wanna talk about it\nnot at all!\nWelcome! Kick ass and have fun!\nI'm a bit confused why you enrolled in \"Python and AI fundamentals\" if you want to become data engineer. From my experience many companies are still stuck in century old data warehouses with a shitload of SQL stored procedures that no one understands anymore because the original data engineers are long gone. These run on on premise SQL servers that are at their limit of hardware capacities. All this leads to cubes that take half a day to refresh and cause severe problems for BI departments. These companies are in desperate need of data engineers who both understand that old world and also have experience in cloud environments, data lake house or data mesh architecture and how to upgrade a shitty dwh to that.\nIf it takes you 2 years to study, you'll be 34 by the time you get to go job hunting in that field. If you don't study, you'll still be 34 in 2 years\nStart with manual QA instead. You'll have more job opportunities. You can self study and be ready in a few months.\nIt's not too late but there aren't many data engineering roles that require just python and ''AI'' - and when they are available, they require a decent amount of experience. The jobs not just about turning a CSV into a dashboard, it's about capturing errors, and understanding the difference in data types between various languages and operating systems in order to make sure the data is where it's meant to be, in the right format and on time! If you put the hours in and have the passion, you'll succeed. I would recommend SQL above everything else.\nMaybe not faang but you can find a way in with enough perseverance. Got in when it was new but have a math and cs background. Back then, 2013, I was a data manager then a backend data dev in 2018 and finally a de in 2022. Maybe find non-profits that need help and start a consultancy.\nIt's never too late to start learning something new. You just need to understand the basics of the field you're going into, practice those basics until you get good at them and build a portfolio using the tools you'll probably end up using at your role. For a data engineer, I would suggest learning Python & SQL. For data visualisation and creating dashboards, PowerBI or Grafana would be great. Learn basic data analytics principles as well. They'll be handy. All the best\nNope. It's never too late to do anything. important thing is to find if you like it before dedicating to it; if you don't actually like CS type work, you'll burn out in a few years, or be miserable for more. Here are some things I think are good precursors to do before dedicating heavily into CS: Learn some python to start. See if you like the process of going from little-to-no knowledge to usable knowledge. Try some codewars and leetcode to see if you like solving cosing problems. Learn and setup a Linux machine (on a cheapo refurbished $150 lenovo if need be) to see how you handle that process. Setup your own local postgres and practice making a database with sample tables and data. Learn to pull it in python and manipulate with pandas. Try and grab data from a free API somewhere and clean + ingest it into your db on a timer with crontab. Make a small local site with node or next to interact with the db (either read and present, or also full CRUD). See if you can host a small ec2 on AWS with the 1yr free tier. Doing these will give you a sense of the kinda work you can expect, though you will likely end up doing ETL more than anything else. I think all of the above can be done in a few months if you work at it. If you want to go for it, it's definitely helpful to get a BS degree. The job market is tough right now, but it's possible with persistence. Best of luck!\nI was 35 when I landed my first DE role! But my transition was quite smooth. I was feeding data into analysis tools at a marketing department, found out I had a knack for data, and I grew up with computers and have a bit of programming background as well. When people at that org were asking about Hadoop I jumped right on it, and managed to get a cluster up and running using Hortonworks. Thats when my career really took off! Hadoop is a thing of the past now and the DE role has changed a bit as well, but I think it should be easier now to transition into.\nNope not at all - 53 year old male who started as a de two years ago (was previously a bi dev)\nStarted as a data analyst at 32 and position became more of a data engineer about 2 years ago. Its never too late to update your skills and get a new job. Just be ready to learn a lot, and learn from your failures\nNah DEI initiatives are still a thing\nAre you single?\nIts a thankless and mentally grueling industry. I recommend finding something more entrepreneurial. Start a company and hire the people with these skills. Or, if you haven't already, get married and have children to keep this country going in the face of population collapse.\nWhy data engineering?\nIts never too late, but probability of you never landing a job in the field is likely high. Starting from scratch with courses will likely leave you ill equipped to break into the club from the outside. I think you have much better chance trying to get in from the inside by starting at some large companies data entry team, data labeling team, manual QA team, etc. and even that route is dubious given the state of the industry, but at least youd be getting a wage for your endeavors",
        "content_hash": "e26a014fa05db303ab3e0b2ff1029704"
    }
]